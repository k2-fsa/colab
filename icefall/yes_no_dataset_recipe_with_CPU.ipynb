{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP07LJLBigd4xMacabp9YJx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/k2-fsa//colab/blob/master/icefall/yes_no_dataset_recipe_with_CPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cC0w8FMdN1j1"
      },
      "source": [
        "# Yesno recipe in icefall\n",
        "\n",
        "This notebook shows you how to setup the environment to use [icefall][icefall] for training and decoding.\n",
        "It also describes how to use a per-trained model to decode waves.\n",
        "\n",
        "\n",
        "We use the [yesno] dataset as an example.\n",
        "\n",
        "[icefall]: https://github.com/k2-fsa/icefall\n",
        "[yesno]: https://www.openslr.org/1/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8jMkYeDQUeY"
      },
      "source": [
        "## Environment setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Etfavk8FQXRQ"
      },
      "source": [
        "### Install PyTorch and torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AliAaueDNteG",
        "outputId": "9df8fb4c-3209-40dc-d2ec-c33ab317760a"
      },
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.1+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXyznh4VaZVj"
      },
      "source": [
        "Colab pre-installs PyTorch, so we don't need to install it here.\n",
        "\n",
        "From https://pytorch.org/audio/main/installation.html#compatibility-matrix, we need to install torchaudio==2.0.2 as the current PyTorch version is 2.0.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJzBhtJAQbD6",
        "outputId": "489c4f46-b095-4ae0-ccc1-0a83915d15b7"
      },
      "source": [
        "! pip install torchaudio==2.0.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchaudio==2.0.2 in /usr/local/lib/python3.10/dist-packages (2.0.2+cu118)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchaudio==2.0.2) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchaudio==2.0.2) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchaudio==2.0.2) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchaudio==2.0.2) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchaudio==2.0.2) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchaudio==2.0.2) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchaudio==2.0.2) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchaudio==2.0.2) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchaudio==2.0.2) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchaudio==2.0.2) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchaudio==2.0.2) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0HY88GiR7_J"
      },
      "source": [
        "### Install k2\n",
        "\n",
        "We are going to install k2 by following https://k2-fsa.github.io/k2/installation/from_wheels.html.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JccZFPBQ7vJ",
        "outputId": "d5662f9c-e45b-4094-e946-03c05526638b"
      },
      "source": [
        "! pip install k2==1.24.3.dev20230718+cuda11.8.torch2.0.1 -f https://k2-fsa.github.io/k2/cuda.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://k2-fsa.github.io/k2/cuda.html\n",
            "Collecting k2==1.24.3.dev20230718+cuda11.8.torch2.0.1\n",
            "  Downloading https://huggingface.co/csukuangfj/k2/resolve/main/cuda/k2-1.24.3.dev20230718%2Bcuda11.8.torch2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.9/117.9 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (2.0.1+cu118)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (0.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (1.3.0)\n",
            "Installing collected packages: k2\n",
            "Successfully installed k2-1.24.3.dev20230718+cuda11.8.torch2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Jl3lHExSTSI"
      },
      "source": [
        "Check that k2 was installed successfully:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYnIIeh6SPz9",
        "outputId": "48ac89d1-9011-4a30-edbb-9aeb3c87add3"
      },
      "source": [
        "! python3 -m k2.version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting environment information...\n",
            "\n",
            "k2 version: 1.24.3\n",
            "Build type: Release\n",
            "Git SHA1: e400fa3b456faf8afe0ee5bfe572946b4921a3db\n",
            "Git date: Sat Jul 15 04:21:50 2023\n",
            "Cuda used to build k2: 11.8\n",
            "cuDNN used to build k2: \n",
            "Python version used to build k2: 3.10\n",
            "OS used to build k2: CentOS Linux release 7.9.2009 (Core)\n",
            "CMake version: 3.26.4\n",
            "GCC version: 9.3.1\n",
            "CMAKE_CUDA_FLAGS:  -Wno-deprecated-gpu-targets   -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w  --expt-extended-lambda -gencode arch=compute_35,code=sm_35  -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w  --expt-extended-lambda -gencode arch=compute_50,code=sm_50  -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w  --expt-extended-lambda -gencode arch=compute_60,code=sm_60  -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w  --expt-extended-lambda -gencode arch=compute_61,code=sm_61  -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w  --expt-extended-lambda -gencode arch=compute_70,code=sm_70  -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w  --expt-extended-lambda -gencode arch=compute_75,code=sm_75  -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w  --expt-extended-lambda -gencode arch=compute_80,code=sm_80  -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w  --expt-extended-lambda -gencode arch=compute_86,code=sm_86 -DONNX_NAMESPACE=onnx_c2 -gencode arch=compute_35,code=sm_35 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_89,code=compute_89 -gencode arch=compute_90,code=compute_90 -Xcudafe --diag_suppress=cc_clobber_ignored,--diag_suppress=integer_sign_change,--diag_suppress=useless_using_declaration,--diag_suppress=set_but_not_used,--diag_suppress=field_without_dll_interface,--diag_suppress=base_class_has_different_dll_interface,--diag_suppress=dll_interface_conflict_none_assumed,--diag_suppress=dll_interface_conflict_dllexport_assumed,--diag_suppress=implicit_return_from_non_void_function,--diag_suppress=unsigned_compare_with_zero,--diag_suppress=declared_but_not_referenced,--diag_suppress=bad_friend_decl --expt-relaxed-constexpr --expt-extended-lambda -D_GLIBCXX_USE_CXX11_ABI=0 --compiler-options -Wall  --compiler-options -Wno-strict-overflow  --compiler-options -Wno-unknown-pragmas \n",
            "CMAKE_CXX_FLAGS:  -D_GLIBCXX_USE_CXX11_ABI=0 -Wno-unused-variable  -Wno-strict-overflow \n",
            "PyTorch version used to build k2: 2.0.1+cu118\n",
            "PyTorch is using Cuda: 11.8\n",
            "NVTX enabled: True\n",
            "With CUDA: True\n",
            "Disable debug: True\n",
            "Sync kernels : False\n",
            "Disable checks: False\n",
            "Max cpu memory allocate: 214748364800 bytes (or 200.0 GB)\n",
            "k2 abort: False\n",
            "__file__: /usr/local/lib/python3.10/dist-packages/k2/version/version.py\n",
            "_k2.__file__: /usr/local/lib/python3.10/dist-packages/_k2.cpython-310-x86_64-linux-gnu.so\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biN4HMqFSdJ5"
      },
      "source": [
        "### Install lhotse\n",
        "[lhotse][lhotse] is used for data preparation.\n",
        "\n",
        "[lhotse]: https://github.com/lhotse-speech/lhotse\n",
        "\n",
        "Normally, we would use `pip install lhotse`. However, the yesno recipe is added recently and has not been released to PyPI yet, so we install the latest unreleased version here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6SsFZwCESWz_",
        "outputId": "e4a6c1fa-5b4b-41d0-b502-7afaebe93f83"
      },
      "source": [
        "# ! pip install lhotse\n",
        "! pip install git+https://github.com/lhotse-speech/lhotse"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/lhotse-speech/lhotse\n",
            "  Cloning https://github.com/lhotse-speech/lhotse to /tmp/pip-req-build-d93hocrb\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/lhotse-speech/lhotse /tmp/pip-req-build-d93hocrb\n",
            "  Resolved https://github.com/lhotse-speech/lhotse to commit 7640d663469b22cd0b36f3246ee9b849cd25e3b7\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from lhotse==1.16.0.dev0+git.7640d66.clean) (3.0.0)\n",
            "Requirement already satisfied: SoundFile>=0.10 in /usr/local/lib/python3.10/dist-packages (from lhotse==1.16.0.dev0+git.7640d66.clean) (0.12.1)\n",
            "Requirement already satisfied: click>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from lhotse==1.16.0.dev0+git.7640d66.clean) (8.1.6)\n",
            "Collecting cytoolz>=0.10.1 (from lhotse==1.16.0.dev0+git.7640d66.clean)\n",
            "  Downloading cytoolz-0.12.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dataclasses (from lhotse==1.16.0.dev0+git.7640d66.clean)\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Collecting intervaltree>=3.1.0 (from lhotse==1.16.0.dev0+git.7640d66.clean)\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from lhotse==1.16.0.dev0+git.7640d66.clean) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lhotse==1.16.0.dev0+git.7640d66.clean) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from lhotse==1.16.0.dev0+git.7640d66.clean) (6.0.1)\n",
            "Requirement already satisfied: tabulate>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from lhotse==1.16.0.dev0+git.7640d66.clean) (0.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from lhotse==1.16.0.dev0+git.7640d66.clean) (4.65.0)\n",
            "Collecting lilcom>=1.1.0 (from lhotse==1.16.0.dev0+git.7640d66.clean)\n",
            "  Downloading lilcom-1.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.1/87.1 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from lhotse==1.16.0.dev0+git.7640d66.clean) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from lhotse==1.16.0.dev0+git.7640d66.clean) (2.0.2+cu118)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from cytoolz>=0.10.1->lhotse==1.16.0.dev0+git.7640d66.clean) (0.12.0)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from intervaltree>=3.1.0->lhotse==1.16.0.dev0+git.7640d66.clean) (2.4.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from SoundFile>=0.10->lhotse==1.16.0.dev0+git.7640d66.clean) (1.15.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->lhotse==1.16.0.dev0+git.7640d66.clean) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->lhotse==1.16.0.dev0+git.7640d66.clean) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->lhotse==1.16.0.dev0+git.7640d66.clean) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->lhotse==1.16.0.dev0+git.7640d66.clean) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->lhotse==1.16.0.dev0+git.7640d66.clean) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->lhotse==1.16.0.dev0+git.7640d66.clean) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->lhotse==1.16.0.dev0+git.7640d66.clean) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->lhotse==1.16.0.dev0+git.7640d66.clean) (16.0.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->SoundFile>=0.10->lhotse==1.16.0.dev0+git.7640d66.clean) (2.21)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->lhotse==1.16.0.dev0+git.7640d66.clean) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->lhotse==1.16.0.dev0+git.7640d66.clean) (1.3.0)\n",
            "Building wheels for collected packages: lhotse, intervaltree\n",
            "  Building wheel for lhotse (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lhotse: filename=lhotse-1.16.0.dev0+git.7640d66.clean-py3-none-any.whl size=687627 sha256=bf4684887746852ffe10d0b4e216bf2e68f10fa31f7ea3d2d59b423759088f52\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-3bg_hvap/wheels/df/b0/ff/cce0f16868fcdbee2088f3acf9f249dc90117d5f5dd9b6f69d\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26099 sha256=f313bdf57e8924c0a98f481e745a18e4c2db02e83645e536253cbc487c61eaf6\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/80/8c/43488a924a046b733b64de3fac99252674c892a4c3801c0a61\n",
            "Successfully built lhotse intervaltree\n",
            "Installing collected packages: dataclasses, lilcom, intervaltree, cytoolz, lhotse\n",
            "Successfully installed cytoolz-0.12.2 dataclasses-0.6 intervaltree-3.1.0 lhotse-1.16.0.dev0+git.7640d66.clean lilcom-1.7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dataclasses"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jwq8uAGpSyzu"
      },
      "source": [
        "### Install icefall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ivIXM8FS0Yy"
      },
      "source": [
        "[icefall][icefall] is a collection of Python scripts.\n",
        "You don't need to install it. What you need to do is\n",
        "to get its source code, install its dependencies, and\n",
        "set the `PYTHONPATH` pointing to it.\n",
        "\n",
        "[icefall]: https://github.com/k2-fsa/icefall"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aflfLSytSnFe",
        "outputId": "2735b0d5-80c1-43b5-e385-77e19e5ef0f5"
      },
      "source": [
        "! pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KP4RZ31xTKzL",
        "outputId": "641c7885-9e47-4ff2-f2bd-563e751eb54a"
      },
      "source": [
        "! git clone https://github.com/k2-fsa/icefall"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'icefall'...\n",
            "remote: Enumerating objects: 12942, done.\u001b[K\n",
            "remote: Counting objects: 100% (67/67), done.\u001b[K\n",
            "remote: Compressing objects: 100% (56/56), done.\u001b[K\n",
            "remote: Total 12942 (delta 17), reused 34 (delta 6), pack-reused 12875\u001b[K\n",
            "Receiving objects: 100% (12942/12942), 14.81 MiB | 23.04 MiB/s, done.\n",
            "Resolving deltas: 100% (8841/8841), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1c5zESwTtii"
      },
      "source": [
        "Now install dependencies of `icefall`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYOwkp6FTrdH",
        "outputId": "cbcc3b61-6f27-41b8-97e4-6101c97cf0ed"
      },
      "source": [
        "! cd icefall && \\\n",
        "  pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kaldifst (from -r requirements.txt (line 1))\n",
            "  Downloading kaldifst-1.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kaldilm (from -r requirements.txt (line 2))\n",
            "  Downloading kaldilm-1.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kaldialign (from -r requirements.txt (line 3))\n",
            "  Downloading kaldialign-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.5/82.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece>=0.1.96 (from -r requirements.txt (line 4))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (2.12.3)\n",
            "Collecting typeguard (from -r requirements.txt (line 6))\n",
            "  Downloading typeguard-4.0.0-py3-none-any.whl (33 kB)\n",
            "Collecting dill (from -r requirements.txt (line 7))\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (1.56.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (3.4.3)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (1.22.4)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (2.27.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (2.3.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (0.41.0)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from typeguard->-r requirements.txt (line 6)) (4.7.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 5)) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 5)) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 5)) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 5)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 5)) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 5)) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 5)) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 5)) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 5)) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 5)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 5)) (3.2.2)\n",
            "Installing collected packages: sentencepiece, kaldilm, kaldifst, kaldialign, typeguard, dill\n",
            "Successfully installed dill-0.3.7 kaldialign-0.7.1 kaldifst-1.6 kaldilm-1.15 sentencepiece-0.1.99 typeguard-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeFyXWEOT4Mi"
      },
      "source": [
        "## Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqMH7ZLKT8jM"
      },
      "source": [
        "We have set up the environment. Now it is the time to prepare data for training and decoding.\n",
        "\n",
        "As we just said, `icefall` is a collection of Python scripts and we have to set up the `PYTHONPATH` variable to use it. Remember that `icefall` was downloaded to\n",
        "`/content/icefall`, so we use\n",
        "\n",
        "```\n",
        "export PYTHONPATH=/content/icefall:$PYTHONPATH\n",
        "```\n",
        "\n",
        "**HINT**: You can have several versions of `icefall` in your virtual environemnt. To switch to a specific version of `icefall`, just change the `PYTHONPATH` environment variable."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To remove the following warning message\n",
        "# 2023-07-27 05:03:07.156920: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
        "! pip uninstall -y tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YQUEXWruxFu",
        "outputId": "49ed9f47-3d65-4b65-ec74-ca5a2aa24105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.12.0\n",
            "Uninstalling tensorflow-2.12.0:\n",
            "  Successfully uninstalled tensorflow-2.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vzyw8VyfUjUB",
        "outputId": "33638ea9-c9b9-419c-85e5-a3d18c5fb979"
      },
      "source": [
        "! export PYTHONPATH=/content/icefall:$PYTHONPATH && \\\n",
        "  cd /content/icefall/egs/yesno/ASR && \\\n",
        "  rm -rf data && \\\n",
        "  ./prepare.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-27 05:10:31 (prepare.sh:27:main) dl_dir: /content/icefall/egs/yesno/ASR/download\n",
            "2023-07-27 05:10:31 (prepare.sh:30:main) Stage 0: Download data\n",
            "/content/icefall/egs/yesno/ASR/download/waves_yesno.tar.gz: 100% 4.70M/4.70M [00:00<00:00, 5.34MB/s]\n",
            "2023-07-27 05:10:36 (prepare.sh:39:main) Stage 1: Prepare yesno manifest\n",
            "2023-07-27 05:10:41 (prepare.sh:45:main) Stage 2: Compute fbank for yesno\n",
            "2023-07-27 05:10:45,340 INFO [compute_fbank_yesno.py:65] Processing train\n",
            "Extracting and storing features: 100% 90/90 [00:00<00:00, 163.47it/s]\n",
            "2023-07-27 05:10:45,911 INFO [compute_fbank_yesno.py:65] Processing test\n",
            "Extracting and storing features: 100% 30/30 [00:00<00:00, 306.05it/s]\n",
            "2023-07-27 05:10:46 (prepare.sh:51:main) Stage 3: Prepare lang\n",
            "2023-07-27 05:10:48 (prepare.sh:66:main) Stage 4: Prepare G\n",
            "/project/kaldilm/csrc/arpa_file_parser.cc:void kaldilm::ArpaFileParser::Read(std::istream&):79\n",
            "[I] Reading \\data\\ section.\n",
            "/project/kaldilm/csrc/arpa_file_parser.cc:void kaldilm::ArpaFileParser::Read(std::istream&):140\n",
            "[I] Reading \\1-grams: section.\n",
            "2023-07-27 05:10:48 (prepare.sh:92:main) Stage 5: Compile HLG\n",
            "2023-07-27 05:10:51,071 INFO [compile_hlg.py:124] Processing data/lang_phone\n",
            "2023-07-27 05:10:51,071 INFO [lexicon.py:171] Converting L.pt to Linv.pt\n",
            "2023-07-27 05:10:51,073 INFO [compile_hlg.py:48] Building ctc_topo. max_token_id: 3\n",
            "2023-07-27 05:10:51,074 INFO [compile_hlg.py:52] Loading G.fst.txt\n",
            "2023-07-27 05:10:51,074 INFO [compile_hlg.py:62] Intersecting L and G\n",
            "2023-07-27 05:10:51,075 INFO [compile_hlg.py:64] LG shape: (4, None)\n",
            "2023-07-27 05:10:51,075 INFO [compile_hlg.py:66] Connecting LG\n",
            "2023-07-27 05:10:51,075 INFO [compile_hlg.py:68] LG shape after k2.connect: (4, None)\n",
            "2023-07-27 05:10:51,075 INFO [compile_hlg.py:70] <class 'torch.Tensor'>\n",
            "2023-07-27 05:10:51,075 INFO [compile_hlg.py:71] Determinizing LG\n",
            "2023-07-27 05:10:51,077 INFO [compile_hlg.py:74] <class '_k2.ragged.RaggedTensor'>\n",
            "2023-07-27 05:10:51,077 INFO [compile_hlg.py:76] Connecting LG after k2.determinize\n",
            "2023-07-27 05:10:51,077 INFO [compile_hlg.py:79] Removing disambiguation symbols on LG\n",
            "2023-07-27 05:10:51,078 INFO [compile_hlg.py:91] LG shape after k2.remove_epsilon: (6, None)\n",
            "2023-07-27 05:10:51,079 INFO [compile_hlg.py:96] Arc sorting LG\n",
            "2023-07-27 05:10:51,079 INFO [compile_hlg.py:99] Composing H and LG\n",
            "2023-07-27 05:10:51,079 INFO [compile_hlg.py:106] Connecting LG\n",
            "2023-07-27 05:10:51,080 INFO [compile_hlg.py:109] Arc sorting LG\n",
            "2023-07-27 05:10:51,080 INFO [compile_hlg.py:111] HLG.shape: (8, None)\n",
            "2023-07-27 05:10:51,080 INFO [compile_hlg.py:127] Saving HLG.pt to data/lang_phone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RAnFhhqZgPo"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1phegInRZkbl",
        "outputId": "b625f969-fd78-465d-d022-16996c9efed1"
      },
      "source": [
        "! export PYTHONPATH=/content/icefall:$PYTHONPATH && \\\n",
        "  cd /content/icefall/egs/yesno/ASR && \\\n",
        "  ./tdnn/train.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-27 05:10:53,838 INFO [train.py:481] Training started\n",
            "2023-07-27 05:10:53,838 INFO [train.py:482] {'exp_dir': PosixPath('tdnn/exp'), 'lang_dir': PosixPath('data/lang_phone'), 'lr': 0.01, 'feature_dim': 23, 'weight_decay': 1e-06, 'start_epoch': 0, 'best_train_loss': inf, 'best_valid_loss': inf, 'best_train_epoch': -1, 'best_valid_epoch': -1, 'batch_idx_train': 0, 'log_interval': 10, 'reset_interval': 20, 'valid_interval': 10, 'beam_size': 10, 'reduction': 'sum', 'use_double_scores': True, 'world_size': 1, 'master_port': 12354, 'tensorboard': True, 'num_epochs': 15, 'seed': 42, 'feature_dir': PosixPath('data/fbank'), 'max_duration': 30.0, 'bucketing_sampler': False, 'num_buckets': 10, 'concatenate_cuts': False, 'duration_factor': 1.0, 'gap': 1.0, 'on_the_fly_feats': False, 'shuffle': False, 'return_cuts': True, 'num_workers': 2, 'env_info': {'k2-version': '1.24.3', 'k2-build-type': 'Release', 'k2-with-cuda': True, 'k2-git-sha1': 'e400fa3b456faf8afe0ee5bfe572946b4921a3db', 'k2-git-date': 'Sat Jul 15 04:21:50 2023', 'lhotse-version': '1.16.0.dev+git.7640d66.clean', 'torch-version': '2.0.1+cu118', 'torch-cuda-available': True, 'torch-cuda-version': '11.8', 'python-version': '3.1', 'icefall-git-branch': 'master', 'icefall-git-sha1': '3fb0a43-clean', 'icefall-git-date': 'Thu Jul 27 04:36:05 2023', 'icefall-path': '/content/icefall', 'k2-path': '/usr/local/lib/python3.10/dist-packages/k2/__init__.py', 'lhotse-path': '/usr/local/lib/python3.10/dist-packages/lhotse/__init__.py', 'hostname': '09e0805df543', 'IP address': '172.28.0.12'}}\n",
            "2023-07-27 05:10:53,839 INFO [lexicon.py:168] Loading pre-compiled data/lang_phone/Linv.pt\n",
            "2023-07-27 05:10:53,840 INFO [train.py:495] device: cuda:0\n",
            "2023-07-27 05:11:00,327 INFO [asr_datamodule.py:146] About to get train cuts\n",
            "2023-07-27 05:11:00,327 INFO [asr_datamodule.py:244] About to get train cuts\n",
            "2023-07-27 05:11:00,882 INFO [asr_datamodule.py:149] About to create train dataset\n",
            "2023-07-27 05:11:00,882 INFO [asr_datamodule.py:199] Using SingleCutSampler.\n",
            "2023-07-27 05:11:00,882 INFO [asr_datamodule.py:205] About to create train dataloader\n",
            "2023-07-27 05:11:00,882 INFO [asr_datamodule.py:218] About to get test cuts\n",
            "2023-07-27 05:11:00,883 INFO [asr_datamodule.py:252] About to get test cuts\n",
            "2023-07-27 05:11:08,662 INFO [train.py:422] Epoch 0, batch 0, loss[loss=1.065, over 2436.00 frames. ], tot_loss[loss=1.065, over 2436.00 frames. ], batch size: 4\n",
            "2023-07-27 05:11:09,303 INFO [train.py:422] Epoch 0, batch 10, loss[loss=0.4572, over 2828.00 frames. ], tot_loss[loss=0.7084, over 22192.90 frames. ], batch size: 4\n",
            "2023-07-27 05:11:09,983 INFO [train.py:444] Epoch 0, validation loss=0.8837, over 18067.00 frames. \n",
            "2023-07-27 05:11:10,695 INFO [train.py:422] Epoch 0, batch 20, loss[loss=0.2522, over 2695.00 frames. ], tot_loss[loss=0.4876, over 34971.47 frames. ], batch size: 5\n",
            "2023-07-27 05:11:11,451 INFO [train.py:444] Epoch 0, validation loss=0.4522, over 18067.00 frames. \n",
            "2023-07-27 05:11:11,586 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-0.pt\n",
            "2023-07-27 05:11:11,798 INFO [train.py:422] Epoch 1, batch 0, loss[loss=0.2563, over 2436.00 frames. ], tot_loss[loss=0.2563, over 2436.00 frames. ], batch size: 4\n",
            "2023-07-27 05:11:12,499 INFO [train.py:422] Epoch 1, batch 10, loss[loss=0.124, over 2828.00 frames. ], tot_loss[loss=0.1651, over 22192.90 frames. ], batch size: 4\n",
            "2023-07-27 05:11:13,206 INFO [train.py:444] Epoch 1, validation loss=0.1746, over 18067.00 frames. \n",
            "2023-07-27 05:11:13,857 INFO [train.py:422] Epoch 1, batch 20, loss[loss=0.07586, over 2695.00 frames. ], tot_loss[loss=0.1217, over 34971.47 frames. ], batch size: 5\n",
            "2023-07-27 05:11:14,488 INFO [train.py:444] Epoch 1, validation loss=0.06803, over 18067.00 frames. \n",
            "2023-07-27 05:11:14,600 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-1.pt\n",
            "2023-07-27 05:11:14,749 INFO [train.py:422] Epoch 2, batch 0, loss[loss=0.07881, over 2436.00 frames. ], tot_loss[loss=0.07881, over 2436.00 frames. ], batch size: 4\n",
            "2023-07-27 05:11:15,221 INFO [train.py:422] Epoch 2, batch 10, loss[loss=0.0443, over 2828.00 frames. ], tot_loss[loss=0.05535, over 22192.90 frames. ], batch size: 4\n",
            "2023-07-27 05:11:15,702 INFO [train.py:444] Epoch 2, validation loss=0.04124, over 18067.00 frames. \n",
            "2023-07-27 05:11:16,136 INFO [train.py:422] Epoch 2, batch 20, loss[loss=0.03629, over 2695.00 frames. ], tot_loss[loss=0.04799, over 34971.47 frames. ], batch size: 5\n",
            "2023-07-27 05:11:16,635 INFO [train.py:444] Epoch 2, validation loss=0.03555, over 18067.00 frames. \n",
            "2023-07-27 05:11:16,743 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-2.pt\n",
            "2023-07-27 05:11:16,895 INFO [train.py:422] Epoch 3, batch 0, loss[loss=0.03689, over 2436.00 frames. ], tot_loss[loss=0.03689, over 2436.00 frames. ], batch size: 4\n",
            "2023-07-27 05:11:17,361 INFO [train.py:422] Epoch 3, batch 10, loss[loss=0.02451, over 2828.00 frames. ], tot_loss[loss=0.02913, over 22192.90 frames. ], batch size: 4\n",
            "2023-07-27 05:11:17,848 INFO [train.py:444] Epoch 3, validation loss=0.02767, over 18067.00 frames. \n",
            "2023-07-27 05:11:18,286 INFO [train.py:422] Epoch 3, batch 20, loss[loss=0.02245, over 2695.00 frames. ], tot_loss[loss=0.02801, over 34971.47 frames. ], batch size: 5\n",
            "2023-07-27 05:11:18,765 INFO [train.py:444] Epoch 3, validation loss=0.0237, over 18067.00 frames. \n",
            "2023-07-27 05:11:18,870 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-3.pt\n",
            "2023-07-27 05:11:19,016 INFO [train.py:422] Epoch 4, batch 0, loss[loss=0.02271, over 2436.00 frames. ], tot_loss[loss=0.02271, over 2436.00 frames. ], batch size: 4\n",
            "2023-07-27 05:11:19,470 INFO [train.py:422] Epoch 4, batch 10, loss[loss=0.01668, over 2828.00 frames. ], tot_loss[loss=0.01941, over 22192.90 frames. ], batch size: 4\n",
            "2023-07-27 05:11:19,959 INFO [train.py:444] Epoch 4, validation loss=0.02165, over 18067.00 frames. \n",
            "2023-07-27 05:11:20,390 INFO [train.py:422] Epoch 4, batch 20, loss[loss=0.01678, over 2695.00 frames. ], tot_loss[loss=0.01967, over 34971.47 frames. ], batch size: 5\n",
            "2023-07-27 05:11:20,882 INFO [train.py:444] Epoch 4, validation loss=0.01643, over 18067.00 frames. \n",
            "2023-07-27 05:11:20,994 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-4.pt\n",
            "2023-07-27 05:11:21,142 INFO [train.py:422] Epoch 5, batch 0, loss[loss=0.01689, over 2436.00 frames. ], tot_loss[loss=0.01689, over 2436.00 frames. ], batch size: 4\n",
            "2023-07-27 05:11:21,601 INFO [train.py:422] Epoch 5, batch 10, loss[loss=0.0128, over 2828.00 frames. ], tot_loss[loss=0.01451, over 22192.90 frames. ], batch size: 4\n",
            "2023-07-27 05:11:22,077 INFO [train.py:444] Epoch 5, validation loss=0.0159, over 18067.00 frames. \n",
            "2023-07-27 05:11:22,517 INFO [train.py:422] Epoch 5, batch 20, loss[loss=0.01521, over 2695.00 frames. ], tot_loss[loss=0.01559, over 34971.47 frames. ], batch size: 5\n",
            "2023-07-27 05:11:23,003 INFO [train.py:444] Epoch 5, validation loss=0.01374, over 18067.00 frames. \n",
            "2023-07-27 05:11:23,108 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-5.pt\n",
            "2023-07-27 05:11:23,263 INFO [train.py:422] Epoch 6, batch 0, loss[loss=0.01399, over 2436.00 frames. ], tot_loss[loss=0.01399, over 2436.00 frames. ], batch size: 4\n",
            "2023-07-27 05:11:23,717 INFO [train.py:422] Epoch 6, batch 10, loss[loss=0.01115, over 2828.00 frames. ], tot_loss[loss=0.01277, over 22192.90 frames. ], batch size: 4\n",
            "2023-07-27 05:11:24,209 INFO [train.py:444] Epoch 6, validation loss=0.01416, over 18067.00 frames. \n",
            "2023-07-27 05:11:24,835 INFO [train.py:422] Epoch 6, batch 20, loss[loss=0.01344, over 2695.00 frames. ], tot_loss[loss=0.01407, over 34971.47 frames. ], batch size: 5\n",
            "2023-07-27 05:11:25,547 INFO [train.py:444] Epoch 6, validation loss=0.01253, over 18067.00 frames. \n",
            "2023-07-27 05:11:25,719 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-6.pt\n",
            "2023-07-27 05:11:25,940 INFO [train.py:422] Epoch 7, batch 0, loss[loss=0.01268, over 2436.00 frames. ], tot_loss[loss=0.01268, over 2436.00 frames. ], batch size: 4\n",
            "2023-07-27 05:11:26,633 INFO [train.py:422] Epoch 7, batch 10, loss[loss=0.01031, over 2828.00 frames. ], tot_loss[loss=0.01182, over 22192.90 frames. ], batch size: 4\n",
            "2023-07-27 05:11:27,351 INFO [train.py:444] Epoch 7, validation loss=0.01295, over 18067.00 frames. \n",
            "2023-07-27 05:11:27,979 INFO [train.py:422] Epoch 7, batch 20, loss[loss=0.01277, over 2695.00 frames. ], tot_loss[loss=0.01307, over 34971.47 frames. ], batch size: 5\n",
            "2023-07-27 05:11:28,680 INFO [train.py:444] Epoch 7, validation loss=0.01197, over 18067.00 frames. \n",
            "2023-07-27 05:11:28,783 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-7.pt\n",
            "2023-07-27 05:11:28,932 INFO [train.py:422] Epoch 8, batch 0, loss[loss=0.01165, over 2436.00 frames. ], tot_loss[loss=0.01165, over 2436.00 frames. ], batch size: 4\n",
            "2023-07-27 05:11:29,404 INFO [train.py:422] Epoch 8, batch 10, loss[loss=0.009802, over 2828.00 frames. ], tot_loss[loss=0.011, over 22192.90 frames. ], batch size: 4\n",
            "2023-07-27 05:11:29,880 INFO [train.py:444] Epoch 8, validation loss=0.0119, over 18067.00 frames. \n",
            "2023-07-27 05:11:30,329 INFO [train.py:422] Epoch 8, batch 20, loss[loss=0.01234, over 2695.00 frames. ], tot_loss[loss=0.01201, over 34971.47 frames. ], batch size: 5\n",
            "2023-07-27 05:11:30,804 INFO [train.py:444] Epoch 8, validation loss=0.0118, over 18067.00 frames. \n",
            "2023-07-27 05:11:30,910 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-8.pt\n",
            "2023-07-27 05:11:31,076 INFO [train.py:422] Epoch 9, batch 0, loss[loss=0.01131, over 2436.00 frames. ], tot_loss[loss=0.01131, over 2436.00 frames. ], batch size: 4\n",
            "2023-07-27 05:11:31,535 INFO [train.py:422] Epoch 9, batch 10, loss[loss=0.009501, over 2828.00 frames. ], tot_loss[loss=0.0107, over 22192.90 frames. ], batch size: 4\n",
            "2023-07-27 05:11:32,014 INFO [train.py:444] Epoch 9, validation loss=0.01136, over 18067.00 frames. \n",
            "2023-07-27 05:11:32,455 INFO [train.py:422] Epoch 9, batch 20, loss[loss=0.01234, over 2695.00 frames. ], tot_loss[loss=0.01157, over 34971.47 frames. ], batch size: 5\n",
            "2023-07-27 05:11:32,930 INFO [train.py:444] Epoch 9, validation loss=0.01129, over 18067.00 frames. \n",
            "2023-07-27 05:11:33,033 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-9.pt\n",
            "2023-07-27 05:11:33,185 INFO [train.py:422] Epoch 10, batch 0, loss[loss=0.01097, over 2436.00 frames. ], tot_loss[loss=0.01097, over 2436.00 frames. ], batch size: 4\n",
            "2023-07-27 05:11:33,642 INFO [train.py:422] Epoch 10, batch 10, loss[loss=0.009282, over 2828.00 frames. ], tot_loss[loss=0.01036, over 22192.90 frames. ], batch size: 4\n",
            "2023-07-27 05:11:34,128 INFO [train.py:444] Epoch 10, validation loss=0.01112, over 18067.00 frames. \n",
            "2023-07-27 05:11:34,567 INFO [train.py:422] Epoch 10, batch 20, loss[loss=0.01195, over 2695.00 frames. ], tot_loss[loss=0.01104, over 34971.47 frames. ], batch size: 5\n",
            "2023-07-27 05:11:35,063 INFO [train.py:444] Epoch 10, validation loss=0.01107, over 18067.00 frames. \n",
            "2023-07-27 05:11:35,176 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-10.pt\n",
            "2023-07-27 05:11:35,326 INFO [train.py:422] Epoch 11, batch 0, loss[loss=0.01081, over 2436.00 frames. ], tot_loss[loss=0.01081, over 2436.00 frames. ], batch size: 4\n",
            "2023-07-27 05:11:35,801 INFO [train.py:422] Epoch 11, batch 10, loss[loss=0.009127, over 2828.00 frames. ], tot_loss[loss=0.01011, over 22192.90 frames. ], batch size: 4\n",
            "2023-07-27 05:11:36,287 INFO [train.py:444] Epoch 11, validation loss=0.01102, over 18067.00 frames. \n",
            "2023-07-27 05:11:36,725 INFO [train.py:422] Epoch 11, batch 20, loss[loss=0.01189, over 2695.00 frames. ], tot_loss[loss=0.01114, over 34971.47 frames. ], batch size: 5\n",
            "2023-07-27 05:11:37,205 INFO [train.py:444] Epoch 11, validation loss=0.01098, over 18067.00 frames. \n",
            "2023-07-27 05:11:37,308 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-11.pt\n",
            "2023-07-27 05:11:37,456 INFO [train.py:422] Epoch 12, batch 0, loss[loss=0.01064, over 2436.00 frames. ], tot_loss[loss=0.01064, over 2436.00 frames. ], batch size: 4\n",
            "2023-07-27 05:11:37,913 INFO [train.py:422] Epoch 12, batch 10, loss[loss=0.009103, over 2828.00 frames. ], tot_loss[loss=0.01001, over 22192.90 frames. ], batch size: 4\n",
            "2023-07-27 05:11:38,386 INFO [train.py:444] Epoch 12, validation loss=0.01099, over 18067.00 frames. \n",
            "2023-07-27 05:11:38,915 INFO [train.py:422] Epoch 12, batch 20, loss[loss=0.01179, over 2695.00 frames. ], tot_loss[loss=0.01057, over 34971.47 frames. ], batch size: 5\n",
            "2023-07-27 05:11:39,644 INFO [train.py:444] Epoch 12, validation loss=0.01083, over 18067.00 frames. \n",
            "2023-07-27 05:11:39,835 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-12.pt\n",
            "2023-07-27 05:11:40,071 INFO [train.py:422] Epoch 13, batch 0, loss[loss=0.01049, over 2436.00 frames. ], tot_loss[loss=0.01049, over 2436.00 frames. ], batch size: 4\n",
            "2023-07-27 05:11:40,773 INFO [train.py:422] Epoch 13, batch 10, loss[loss=0.009193, over 2828.00 frames. ], tot_loss[loss=0.009996, over 22192.90 frames. ], batch size: 4\n",
            "2023-07-27 05:11:41,491 INFO [train.py:444] Epoch 13, validation loss=0.01095, over 18067.00 frames. \n",
            "2023-07-27 05:11:42,152 INFO [train.py:422] Epoch 13, batch 20, loss[loss=0.01182, over 2695.00 frames. ], tot_loss[loss=0.01061, over 34971.47 frames. ], batch size: 5\n",
            "2023-07-27 05:11:42,841 INFO [train.py:444] Epoch 13, validation loss=0.01109, over 18067.00 frames. \n",
            "2023-07-27 05:11:42,946 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-13.pt\n",
            "2023-07-27 05:11:43,099 INFO [train.py:422] Epoch 14, batch 0, loss[loss=0.01043, over 2436.00 frames. ], tot_loss[loss=0.01043, over 2436.00 frames. ], batch size: 4\n",
            "2023-07-27 05:11:43,566 INFO [train.py:422] Epoch 14, batch 10, loss[loss=0.008968, over 2828.00 frames. ], tot_loss[loss=0.009883, over 22192.90 frames. ], batch size: 4\n",
            "2023-07-27 05:11:44,045 INFO [train.py:444] Epoch 14, validation loss=0.01087, over 18067.00 frames. \n",
            "2023-07-27 05:11:44,487 INFO [train.py:422] Epoch 14, batch 20, loss[loss=0.0117, over 2695.00 frames. ], tot_loss[loss=0.01061, over 34971.47 frames. ], batch size: 5\n",
            "2023-07-27 05:11:44,971 INFO [train.py:444] Epoch 14, validation loss=0.01079, over 18067.00 frames. \n",
            "2023-07-27 05:11:45,075 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-14.pt\n",
            "2023-07-27 05:11:45,077 INFO [train.py:555] Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnwI18BKcq6e"
      },
      "source": [
        "## Decoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WS-i2RdYzrx",
        "outputId": "8995f915-1075-4015-ea03-f9d4bcc34e53"
      },
      "source": [
        "! export PYTHONPATH=/content/icefall:$PYTHONPATH && \\\n",
        "  cd /content/icefall/egs/yesno/ASR && \\\n",
        "  ./tdnn/decode.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-27 05:11:48,774 INFO [decode.py:263] Decoding started\n",
            "2023-07-27 05:11:48,774 INFO [decode.py:264] {'exp_dir': PosixPath('tdnn/exp'), 'lang_dir': PosixPath('data/lang_phone'), 'lm_dir': PosixPath('data/lm'), 'feature_dim': 23, 'search_beam': 20, 'output_beam': 8, 'min_active_states': 30, 'max_active_states': 10000, 'use_double_scores': True, 'epoch': 14, 'avg': 2, 'export': False, 'feature_dir': PosixPath('data/fbank'), 'max_duration': 30.0, 'bucketing_sampler': False, 'num_buckets': 10, 'concatenate_cuts': False, 'duration_factor': 1.0, 'gap': 1.0, 'on_the_fly_feats': False, 'shuffle': False, 'return_cuts': True, 'num_workers': 2, 'env_info': {'k2-version': '1.24.3', 'k2-build-type': 'Release', 'k2-with-cuda': True, 'k2-git-sha1': 'e400fa3b456faf8afe0ee5bfe572946b4921a3db', 'k2-git-date': 'Sat Jul 15 04:21:50 2023', 'lhotse-version': '1.16.0.dev+git.7640d66.clean', 'torch-version': '2.0.1+cu118', 'torch-cuda-available': True, 'torch-cuda-version': '11.8', 'python-version': '3.1', 'icefall-git-branch': 'master', 'icefall-git-sha1': '3fb0a43-clean', 'icefall-git-date': 'Thu Jul 27 04:36:05 2023', 'icefall-path': '/content/icefall', 'k2-path': '/usr/local/lib/python3.10/dist-packages/k2/__init__.py', 'lhotse-path': '/usr/local/lib/python3.10/dist-packages/lhotse/__init__.py', 'hostname': '09e0805df543', 'IP address': '172.28.0.12'}}\n",
            "2023-07-27 05:11:48,774 INFO [lexicon.py:168] Loading pre-compiled data/lang_phone/Linv.pt\n",
            "2023-07-27 05:11:48,776 INFO [decode.py:273] device: cuda:0\n",
            "2023-07-27 05:11:50,590 INFO [decode.py:291] averaging ['tdnn/exp/epoch-13.pt', 'tdnn/exp/epoch-14.pt']\n",
            "2023-07-27 05:11:50,597 INFO [asr_datamodule.py:218] About to get test cuts\n",
            "2023-07-27 05:11:50,597 INFO [asr_datamodule.py:252] About to get test cuts\n",
            "2023-07-27 05:11:52,600 INFO [decode.py:204] batch 0/?, cuts processed until now is 4\n",
            "2023-07-27 05:11:54,299 INFO [decode.py:241] The transcripts are stored in tdnn/exp/recogs-test_set.txt\n",
            "2023-07-27 05:11:54,300 INFO [utils.py:564] [test_set] %WER 0.42% [1 / 240, 0 ins, 1 del, 0 sub ]\n",
            "2023-07-27 05:11:54,302 INFO [decode.py:249] Wrote detailed error stats to tdnn/exp/errs-test_set.txt\n",
            "2023-07-27 05:11:54,302 INFO [decode.py:316] Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMENow4cc53b"
      },
      "source": [
        "### Show the decoding result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yQT-VEJc3Xz",
        "outputId": "41e9be4e-d2c4-4062-cfa5-fc260c98e715"
      },
      "source": [
        "! cd /content/icefall/egs/yesno/ASR && \\\n",
        "  cat tdnn/exp/recogs-test_set.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0_0_0_1_0_0_0_1-0:\tref=['NO', 'NO', 'NO', 'YES', 'NO', 'NO', 'NO', 'YES']\n",
            "0_0_0_1_0_0_0_1-0:\thyp=['NO', 'NO', 'NO', 'YES', 'NO', 'NO', 'NO', 'YES']\n",
            "0_0_1_0_0_0_1_0-1:\tref=['NO', 'NO', 'YES', 'NO', 'NO', 'NO', 'YES', 'NO']\n",
            "0_0_1_0_0_0_1_0-1:\thyp=['NO', 'NO', 'YES', 'NO', 'NO', 'NO', 'YES', 'NO']\n",
            "0_0_1_0_0_1_1_1-2:\tref=['NO', 'NO', 'YES', 'NO', 'NO', 'YES', 'YES', 'YES']\n",
            "0_0_1_0_0_1_1_1-2:\thyp=['NO', 'NO', 'YES', 'NO', 'NO', 'YES', 'YES', 'YES']\n",
            "0_0_1_0_1_0_0_1-3:\tref=['NO', 'NO', 'YES', 'NO', 'YES', 'NO', 'NO', 'YES']\n",
            "0_0_1_0_1_0_0_1-3:\thyp=['NO', 'NO', 'YES', 'NO', 'YES', 'NO', 'NO', 'YES']\n",
            "0_0_1_1_0_0_0_1-4:\tref=['NO', 'NO', 'YES', 'YES', 'NO', 'NO', 'NO', 'YES']\n",
            "0_0_1_1_0_0_0_1-4:\thyp=['NO', 'NO', 'YES', 'YES', 'NO', 'NO', 'NO', 'YES']\n",
            "0_0_1_1_0_1_1_0-5:\tref=['NO', 'NO', 'YES', 'YES', 'NO', 'YES', 'YES', 'NO']\n",
            "0_0_1_1_0_1_1_0-5:\thyp=['NO', 'NO', 'YES', 'YES', 'NO', 'YES', 'YES', 'NO']\n",
            "0_0_1_1_1_0_0_0-6:\tref=['NO', 'NO', 'YES', 'YES', 'YES', 'NO', 'NO', 'NO']\n",
            "0_0_1_1_1_0_0_0-6:\thyp=['NO', 'NO', 'YES', 'YES', 'YES', 'NO', 'NO', 'NO']\n",
            "0_0_1_1_1_1_0_0-7:\tref=['NO', 'NO', 'YES', 'YES', 'YES', 'YES', 'NO', 'NO']\n",
            "0_0_1_1_1_1_0_0-7:\thyp=['NO', 'NO', 'YES', 'YES', 'YES', 'YES', 'NO', 'NO']\n",
            "0_1_0_0_0_1_0_0-8:\tref=['NO', 'YES', 'NO', 'NO', 'NO', 'YES', 'NO', 'NO']\n",
            "0_1_0_0_0_1_0_0-8:\thyp=['NO', 'YES', 'NO', 'NO', 'NO', 'YES', 'NO', 'NO']\n",
            "0_1_0_0_1_0_1_0-9:\tref=['NO', 'YES', 'NO', 'NO', 'YES', 'NO', 'YES', 'NO']\n",
            "0_1_0_0_1_0_1_0-9:\thyp=['NO', 'YES', 'NO', 'NO', 'YES', 'NO', 'YES', 'NO']\n",
            "0_1_0_1_0_0_0_0-10:\tref=['NO', 'YES', 'NO', 'YES', 'NO', 'NO', 'NO', 'NO']\n",
            "0_1_0_1_0_0_0_0-10:\thyp=['NO', 'YES', 'NO', 'YES', 'NO', 'NO', 'NO']\n",
            "0_1_0_1_1_1_0_0-11:\tref=['NO', 'YES', 'NO', 'YES', 'YES', 'YES', 'NO', 'NO']\n",
            "0_1_0_1_1_1_0_0-11:\thyp=['NO', 'YES', 'NO', 'YES', 'YES', 'YES', 'NO', 'NO']\n",
            "0_1_1_0_0_1_1_1-12:\tref=['NO', 'YES', 'YES', 'NO', 'NO', 'YES', 'YES', 'YES']\n",
            "0_1_1_0_0_1_1_1-12:\thyp=['NO', 'YES', 'YES', 'NO', 'NO', 'YES', 'YES', 'YES']\n",
            "0_1_1_1_0_0_1_0-13:\tref=['NO', 'YES', 'YES', 'YES', 'NO', 'NO', 'YES', 'NO']\n",
            "0_1_1_1_0_0_1_0-13:\thyp=['NO', 'YES', 'YES', 'YES', 'NO', 'NO', 'YES', 'NO']\n",
            "0_1_1_1_1_0_1_0-14:\tref=['NO', 'YES', 'YES', 'YES', 'YES', 'NO', 'YES', 'NO']\n",
            "0_1_1_1_1_0_1_0-14:\thyp=['NO', 'YES', 'YES', 'YES', 'YES', 'NO', 'YES', 'NO']\n",
            "1_0_0_0_0_0_0_0-15:\tref=['YES', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO']\n",
            "1_0_0_0_0_0_0_0-15:\thyp=['YES', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO']\n",
            "1_0_0_0_0_0_1_1-16:\tref=['YES', 'NO', 'NO', 'NO', 'NO', 'NO', 'YES', 'YES']\n",
            "1_0_0_0_0_0_1_1-16:\thyp=['YES', 'NO', 'NO', 'NO', 'NO', 'NO', 'YES', 'YES']\n",
            "1_0_0_1_0_1_1_1-17:\tref=['YES', 'NO', 'NO', 'YES', 'NO', 'YES', 'YES', 'YES']\n",
            "1_0_0_1_0_1_1_1-17:\thyp=['YES', 'NO', 'NO', 'YES', 'NO', 'YES', 'YES', 'YES']\n",
            "1_0_1_1_0_1_1_1-18:\tref=['YES', 'NO', 'YES', 'YES', 'NO', 'YES', 'YES', 'YES']\n",
            "1_0_1_1_0_1_1_1-18:\thyp=['YES', 'NO', 'YES', 'YES', 'NO', 'YES', 'YES', 'YES']\n",
            "1_0_1_1_1_1_0_1-19:\tref=['YES', 'NO', 'YES', 'YES', 'YES', 'YES', 'NO', 'YES']\n",
            "1_0_1_1_1_1_0_1-19:\thyp=['YES', 'NO', 'YES', 'YES', 'YES', 'YES', 'NO', 'YES']\n",
            "1_1_0_0_0_1_1_1-20:\tref=['YES', 'YES', 'NO', 'NO', 'NO', 'YES', 'YES', 'YES']\n",
            "1_1_0_0_0_1_1_1-20:\thyp=['YES', 'YES', 'NO', 'NO', 'NO', 'YES', 'YES', 'YES']\n",
            "1_1_0_0_1_0_1_1-21:\tref=['YES', 'YES', 'NO', 'NO', 'YES', 'NO', 'YES', 'YES']\n",
            "1_1_0_0_1_0_1_1-21:\thyp=['YES', 'YES', 'NO', 'NO', 'YES', 'NO', 'YES', 'YES']\n",
            "1_1_0_1_0_1_0_0-22:\tref=['YES', 'YES', 'NO', 'YES', 'NO', 'YES', 'NO', 'NO']\n",
            "1_1_0_1_0_1_0_0-22:\thyp=['YES', 'YES', 'NO', 'YES', 'NO', 'YES', 'NO', 'NO']\n",
            "1_1_0_1_1_0_0_1-23:\tref=['YES', 'YES', 'NO', 'YES', 'YES', 'NO', 'NO', 'YES']\n",
            "1_1_0_1_1_0_0_1-23:\thyp=['YES', 'YES', 'NO', 'YES', 'YES', 'NO', 'NO', 'YES']\n",
            "1_1_0_1_1_1_1_0-24:\tref=['YES', 'YES', 'NO', 'YES', 'YES', 'YES', 'YES', 'NO']\n",
            "1_1_0_1_1_1_1_0-24:\thyp=['YES', 'YES', 'NO', 'YES', 'YES', 'YES', 'YES', 'NO']\n",
            "1_1_1_0_0_1_0_1-25:\tref=['YES', 'YES', 'YES', 'NO', 'NO', 'YES', 'NO', 'YES']\n",
            "1_1_1_0_0_1_0_1-25:\thyp=['YES', 'YES', 'YES', 'NO', 'NO', 'YES', 'NO', 'YES']\n",
            "1_1_1_0_1_0_1_0-26:\tref=['YES', 'YES', 'YES', 'NO', 'YES', 'NO', 'YES', 'NO']\n",
            "1_1_1_0_1_0_1_0-26:\thyp=['YES', 'YES', 'YES', 'NO', 'YES', 'NO', 'YES', 'NO']\n",
            "1_1_1_1_0_0_1_0-27:\tref=['YES', 'YES', 'YES', 'YES', 'NO', 'NO', 'YES', 'NO']\n",
            "1_1_1_1_0_0_1_0-27:\thyp=['YES', 'YES', 'YES', 'YES', 'NO', 'NO', 'YES', 'NO']\n",
            "1_1_1_1_1_0_0_0-28:\tref=['YES', 'YES', 'YES', 'YES', 'YES', 'NO', 'NO', 'NO']\n",
            "1_1_1_1_1_0_0_0-28:\thyp=['YES', 'YES', 'YES', 'YES', 'YES', 'NO', 'NO', 'NO']\n",
            "1_1_1_1_1_1_1_1-29:\tref=['YES', 'YES', 'YES', 'YES', 'YES', 'YES', 'YES', 'YES']\n",
            "1_1_1_1_1_1_1_1-29:\thyp=['YES', 'YES', 'YES', 'YES', 'YES', 'YES', 'YES', 'YES']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RIxtJ-IdLob"
      },
      "source": [
        "### Show the detailed WER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lQFBS-KdIVx",
        "outputId": "f2371e04-d9a8-4667-d401-8fb8c4bdb762"
      },
      "source": [
        "! cd /content/icefall/egs/yesno/ASR && \\\n",
        "  cat tdnn/exp/errs-test_set.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "%WER = 0.42\n",
            "Errors: 0 insertions, 1 deletions, 0 substitutions, over 240 reference words (239 correct)\n",
            "Search below for sections starting with PER-UTT DETAILS:, SUBSTITUTIONS:, DELETIONS:, INSERTIONS:, PER-WORD STATS:\n",
            "\n",
            "PER-UTT DETAILS: corr or (ref->hyp)  \n",
            "0_0_0_1_0_0_0_1-0:\tNO NO NO YES NO NO NO YES\n",
            "0_0_1_0_0_0_1_0-1:\tNO NO YES NO NO NO YES NO\n",
            "0_0_1_0_0_1_1_1-2:\tNO NO YES NO NO YES YES YES\n",
            "0_0_1_0_1_0_0_1-3:\tNO NO YES NO YES NO NO YES\n",
            "0_0_1_1_0_0_0_1-4:\tNO NO YES YES NO NO NO YES\n",
            "0_0_1_1_0_1_1_0-5:\tNO NO YES YES NO YES YES NO\n",
            "0_0_1_1_1_0_0_0-6:\tNO NO YES YES YES NO NO NO\n",
            "0_0_1_1_1_1_0_0-7:\tNO NO YES YES YES YES NO NO\n",
            "0_1_0_0_0_1_0_0-8:\tNO YES NO NO NO YES NO NO\n",
            "0_1_0_0_1_0_1_0-9:\tNO YES NO NO YES NO YES NO\n",
            "0_1_0_1_0_0_0_0-10:\tNO YES NO YES NO NO NO (NO->*)\n",
            "0_1_0_1_1_1_0_0-11:\tNO YES NO YES YES YES NO NO\n",
            "0_1_1_0_0_1_1_1-12:\tNO YES YES NO NO YES YES YES\n",
            "0_1_1_1_0_0_1_0-13:\tNO YES YES YES NO NO YES NO\n",
            "0_1_1_1_1_0_1_0-14:\tNO YES YES YES YES NO YES NO\n",
            "1_0_0_0_0_0_0_0-15:\tYES NO NO NO NO NO NO NO\n",
            "1_0_0_0_0_0_1_1-16:\tYES NO NO NO NO NO YES YES\n",
            "1_0_0_1_0_1_1_1-17:\tYES NO NO YES NO YES YES YES\n",
            "1_0_1_1_0_1_1_1-18:\tYES NO YES YES NO YES YES YES\n",
            "1_0_1_1_1_1_0_1-19:\tYES NO YES YES YES YES NO YES\n",
            "1_1_0_0_0_1_1_1-20:\tYES YES NO NO NO YES YES YES\n",
            "1_1_0_0_1_0_1_1-21:\tYES YES NO NO YES NO YES YES\n",
            "1_1_0_1_0_1_0_0-22:\tYES YES NO YES NO YES NO NO\n",
            "1_1_0_1_1_0_0_1-23:\tYES YES NO YES YES NO NO YES\n",
            "1_1_0_1_1_1_1_0-24:\tYES YES NO YES YES YES YES NO\n",
            "1_1_1_0_0_1_0_1-25:\tYES YES YES NO NO YES NO YES\n",
            "1_1_1_0_1_0_1_0-26:\tYES YES YES NO YES NO YES NO\n",
            "1_1_1_1_0_0_1_0-27:\tYES YES YES YES NO NO YES NO\n",
            "1_1_1_1_1_0_0_0-28:\tYES YES YES YES YES NO NO NO\n",
            "1_1_1_1_1_1_1_1-29:\tYES YES YES YES YES YES YES YES\n",
            "\n",
            "SUBSTITUTIONS: count ref -> hyp\n",
            "\n",
            "DELETIONS: count ref\n",
            "1   NO\n",
            "\n",
            "INSERTIONS: count hyp\n",
            "\n",
            "PER-WORD STATS: word  corr tot_errs count_in_ref count_in_hyp\n",
            "NO   115 1 116 115\n",
            "YES   124 0 124 124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lIChKjOr5J0"
      },
      "source": [
        "# Pre-trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzfpY18xr-6P"
      },
      "source": [
        "### Download the pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naBI_K9fr8Qn",
        "outputId": "1834915d-17ef-4540-8e51-d6b0e9f6d716"
      },
      "source": [
        "! cd /content/icefall/egs/yesno/ASR && \\\n",
        "  mkdir tmp && \\\n",
        "  cd tmp && \\\n",
        "  git lfs install && \\\n",
        "  git clone https://huggingface.co/csukuangfj/icefall_asr_yesno_tdnn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated git hooks.\n",
            "Git LFS initialized.\n",
            "Cloning into 'icefall_asr_yesno_tdnn'...\n",
            "remote: Enumerating objects: 60, done.\u001b[K\n",
            "remote: Total 60 (delta 0), reused 0 (delta 0), pack-reused 60\u001b[K\n",
            "Unpacking objects: 100% (60/60), 2.28 MiB | 9.59 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hbcze8EsRcA",
        "outputId": "45ce18c2-cdcd-415f-97b7-30122a3375aa"
      },
      "source": [
        "! sudo apt-get install git-lfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (3.0.2-1ubuntu0.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 8 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCkos-5msW-K",
        "outputId": "fe236298-c544-42b7-aa20-fab93862e68d"
      },
      "source": [
        "! cd /content/icefall/egs/yesno/ASR && \\\n",
        "  mkdir -p tmp && \\\n",
        "  tree tmp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: tree: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufKWViBxsc41",
        "outputId": "841b835f-e6f5-4604-e2cb-8f11ff914fe9"
      },
      "source": [
        "! sudo apt-get install tree"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 8 not upgraded.\n",
            "Need to get 47.9 kB of archives.\n",
            "After this operation, 116 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tree amd64 2.0.2-1 [47.9 kB]\n",
            "Fetched 47.9 kB in 0s (286 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 120493 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_2.0.2-1_amd64.deb ...\n",
            "Unpacking tree (2.0.2-1) ...\n",
            "Setting up tree (2.0.2-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJn7iUw6se1W",
        "outputId": "ba6915dc-3aa2-4ac8-caef-3a86d527de5f"
      },
      "source": [
        "! cd /content/icefall/egs/yesno/ASR && \\\n",
        "  mkdir -p tmp && \\\n",
        "  tree tmp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01;34mtmp\u001b[0m\n",
            "└── \u001b[01;34micefall_asr_yesno_tdnn\u001b[0m\n",
            "    ├── \u001b[01;34mlang_phone\u001b[0m\n",
            "    │   ├── \u001b[00mHLG.pt\u001b[0m\n",
            "    │   ├── \u001b[00mL_disambig.pt\u001b[0m\n",
            "    │   ├── \u001b[00mlexicon_disambig.txt\u001b[0m\n",
            "    │   ├── \u001b[00mlexicon.txt\u001b[0m\n",
            "    │   ├── \u001b[00mLinv.pt\u001b[0m\n",
            "    │   ├── \u001b[00mL.pt\u001b[0m\n",
            "    │   ├── \u001b[00mtokens.txt\u001b[0m\n",
            "    │   └── \u001b[00mwords.txt\u001b[0m\n",
            "    ├── \u001b[01;34mlm\u001b[0m\n",
            "    │   ├── \u001b[00mG.arpa\u001b[0m\n",
            "    │   └── \u001b[00mG.fst.txt\u001b[0m\n",
            "    ├── \u001b[00mpretrained.pt\u001b[0m\n",
            "    ├── \u001b[00mREADME.md\u001b[0m\n",
            "    └── \u001b[01;34mtest_waves\u001b[0m\n",
            "        ├── \u001b[01;35m0_0_0_1_0_0_0_1.wav\u001b[0m\n",
            "        ├── \u001b[01;35m0_0_1_0_0_0_1_0.wav\u001b[0m\n",
            "        ├── \u001b[01;35m0_0_1_0_0_1_1_1.wav\u001b[0m\n",
            "        ├── \u001b[01;35m0_0_1_0_1_0_0_1.wav\u001b[0m\n",
            "        ├── \u001b[01;35m0_0_1_1_0_0_0_1.wav\u001b[0m\n",
            "        ├── \u001b[01;35m0_0_1_1_0_1_1_0.wav\u001b[0m\n",
            "        ├── \u001b[01;35m0_0_1_1_1_0_0_0.wav\u001b[0m\n",
            "        ├── \u001b[01;35m0_0_1_1_1_1_0_0.wav\u001b[0m\n",
            "        ├── \u001b[01;35m0_1_0_0_0_1_0_0.wav\u001b[0m\n",
            "        ├── \u001b[01;35m0_1_0_0_1_0_1_0.wav\u001b[0m\n",
            "        ├── \u001b[01;35m0_1_0_1_0_0_0_0.wav\u001b[0m\n",
            "        ├── \u001b[01;35m0_1_0_1_1_1_0_0.wav\u001b[0m\n",
            "        ├── \u001b[01;35m0_1_1_0_0_1_1_1.wav\u001b[0m\n",
            "        ├── \u001b[01;35m0_1_1_1_0_0_1_0.wav\u001b[0m\n",
            "        ├── \u001b[01;35m0_1_1_1_1_0_1_0.wav\u001b[0m\n",
            "        ├── \u001b[01;35m1_0_0_0_0_0_0_0.wav\u001b[0m\n",
            "        ├── \u001b[01;35m1_0_0_0_0_0_1_1.wav\u001b[0m\n",
            "        ├── \u001b[01;35m1_0_0_1_0_1_1_1.wav\u001b[0m\n",
            "        ├── \u001b[01;35m1_0_1_1_0_1_1_1.wav\u001b[0m\n",
            "        ├── \u001b[01;35m1_0_1_1_1_1_0_1.wav\u001b[0m\n",
            "        ├── \u001b[01;35m1_1_0_0_0_1_1_1.wav\u001b[0m\n",
            "        ├── \u001b[01;35m1_1_0_0_1_0_1_1.wav\u001b[0m\n",
            "        ├── \u001b[01;35m1_1_0_1_0_1_0_0.wav\u001b[0m\n",
            "        ├── \u001b[01;35m1_1_0_1_1_0_0_1.wav\u001b[0m\n",
            "        ├── \u001b[01;35m1_1_0_1_1_1_1_0.wav\u001b[0m\n",
            "        ├── \u001b[01;35m1_1_1_0_0_1_0_1.wav\u001b[0m\n",
            "        ├── \u001b[01;35m1_1_1_0_1_0_1_0.wav\u001b[0m\n",
            "        ├── \u001b[01;35m1_1_1_1_0_0_1_0.wav\u001b[0m\n",
            "        ├── \u001b[01;35m1_1_1_1_1_0_0_0.wav\u001b[0m\n",
            "        └── \u001b[01;35m1_1_1_1_1_1_1_1.wav\u001b[0m\n",
            "\n",
            "4 directories, 42 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8We8jsdsiPw",
        "outputId": "d749994b-5fa4-498f-d86a-cab1fdd5e6fd"
      },
      "source": [
        "! cd /content/icefall/egs/yesno/ASR && \\\n",
        "  soxi tmp/icefall_asr_yesno_tdnn/test_waves/0_0_1_0_1_0_0_1.wav"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: soxi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69CgbooXsorj",
        "outputId": "e16c2b16-7222-437c-d912-ef4bfbbfe9e6"
      },
      "source": [
        "! sudo apt-get install sox"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libopencore-amrnb0 libopencore-amrwb0 libsox-fmt-alsa libsox-fmt-base\n",
            "  libsox3 libwavpack1\n",
            "Suggested packages:\n",
            "  libsox-fmt-all\n",
            "The following NEW packages will be installed:\n",
            "  libopencore-amrnb0 libopencore-amrwb0 libsox-fmt-alsa libsox-fmt-base\n",
            "  libsox3 libwavpack1 sox\n",
            "0 upgraded, 7 newly installed, 0 to remove and 8 not upgraded.\n",
            "Need to get 617 kB of archives.\n",
            "After this operation, 1,760 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libopencore-amrnb0 amd64 0.1.5-1 [94.8 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libopencore-amrwb0 amd64 0.1.5-1 [49.1 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox3 amd64 14.4.2+git20190427-2+deb11u2build0.22.04.1 [240 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-alsa amd64 14.4.2+git20190427-2+deb11u2build0.22.04.1 [11.2 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwavpack1 amd64 5.4.0-1build2 [83.7 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-base amd64 14.4.2+git20190427-2+deb11u2build0.22.04.1 [33.7 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 sox amd64 14.4.2+git20190427-2+deb11u2build0.22.04.1 [104 kB]\n",
            "Fetched 617 kB in 0s (1,718 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 7.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libopencore-amrnb0:amd64.\n",
            "(Reading database ... 120501 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libopencore-amrnb0_0.1.5-1_amd64.deb ...\n",
            "Unpacking libopencore-amrnb0:amd64 (0.1.5-1) ...\n",
            "Selecting previously unselected package libopencore-amrwb0:amd64.\n",
            "Preparing to unpack .../1-libopencore-amrwb0_0.1.5-1_amd64.deb ...\n",
            "Unpacking libopencore-amrwb0:amd64 (0.1.5-1) ...\n",
            "Selecting previously unselected package libsox3:amd64.\n",
            "Preparing to unpack .../2-libsox3_14.4.2+git20190427-2+deb11u2build0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox3:amd64 (14.4.2+git20190427-2+deb11u2build0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-alsa:amd64.\n",
            "Preparing to unpack .../3-libsox-fmt-alsa_14.4.2+git20190427-2+deb11u2build0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-alsa:amd64 (14.4.2+git20190427-2+deb11u2build0.22.04.1) ...\n",
            "Selecting previously unselected package libwavpack1:amd64.\n",
            "Preparing to unpack .../4-libwavpack1_5.4.0-1build2_amd64.deb ...\n",
            "Unpacking libwavpack1:amd64 (5.4.0-1build2) ...\n",
            "Selecting previously unselected package libsox-fmt-base:amd64.\n",
            "Preparing to unpack .../5-libsox-fmt-base_14.4.2+git20190427-2+deb11u2build0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-base:amd64 (14.4.2+git20190427-2+deb11u2build0.22.04.1) ...\n",
            "Selecting previously unselected package sox.\n",
            "Preparing to unpack .../6-sox_14.4.2+git20190427-2+deb11u2build0.22.04.1_amd64.deb ...\n",
            "Unpacking sox (14.4.2+git20190427-2+deb11u2build0.22.04.1) ...\n",
            "Setting up libsox3:amd64 (14.4.2+git20190427-2+deb11u2build0.22.04.1) ...\n",
            "Setting up libopencore-amrwb0:amd64 (0.1.5-1) ...\n",
            "Setting up libsox-fmt-alsa:amd64 (14.4.2+git20190427-2+deb11u2build0.22.04.1) ...\n",
            "Setting up libwavpack1:amd64 (5.4.0-1build2) ...\n",
            "Setting up libopencore-amrnb0:amd64 (0.1.5-1) ...\n",
            "Setting up libsox-fmt-base:amd64 (14.4.2+git20190427-2+deb11u2build0.22.04.1) ...\n",
            "Setting up sox (14.4.2+git20190427-2+deb11u2build0.22.04.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRQ85XoTsqVq",
        "outputId": "1f56bb85-3334-4a11-ba8c-21a9be640715"
      },
      "source": [
        "! cd /content/icefall/egs/yesno/ASR && \\\n",
        "  soxi tmp/icefall_asr_yesno_tdnn/test_waves/0_0_1_0_1_0_0_1.wav"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input File     : 'tmp/icefall_asr_yesno_tdnn/test_waves/0_0_1_0_1_0_0_1.wav'\n",
            "Channels       : 1\n",
            "Sample Rate    : 8000\n",
            "Precision      : 16-bit\n",
            "Duration       : 00:00:06.76 = 54080 samples ~ 507 CDDA sectors\n",
            "File Size      : 108k\n",
            "Bit Rate       : 128k\n",
            "Sample Encoding: 16-bit Signed Integer PCM\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtEYXq6Lt4gH"
      },
      "source": [
        "## Download kaldifeat\n",
        "\n",
        "See https://csukuangfj.github.io/kaldifeat/installation/from_wheels.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yXGrYBbt7Xd",
        "outputId": "e3360454-836f-4056-926a-bd9c33e44c72"
      },
      "source": [
        "! pip install kaldifeat==1.25.0.dev20230726+cuda11.8.torch2.0.1  -f https://csukuangfj.github.io/kaldifeat/cuda.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://csukuangfj.github.io/kaldifeat/cuda.html\n",
            "Collecting kaldifeat==1.25.0.dev20230726+cuda11.8.torch2.0.1\n",
            "  Downloading https://huggingface.co/csukuangfj/kaldifeat/resolve/main/ubuntu-cuda/kaldifeat-1.25.0.dev20230726%2Bcuda11.8.torch2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (574 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m574.0/574.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kaldifeat\n",
            "Successfully installed kaldifeat-1.25.0.dev20230726+cuda11.8.torch2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWmnk4YwsyQn"
      },
      "source": [
        "## Inference with a pre-trained model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-YdwbyfwS7F"
      },
      "source": [
        "### View help information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmU6_ZbLsvAg",
        "outputId": "ae790003-471f-485a-8185-90971dcaacf2"
      },
      "source": [
        "! export PYTHONPATH=/content/icefall:$PYTHONPATH && \\\n",
        "  cd /content/icefall/egs/yesno/ASR && \\\n",
        "  ./tdnn/pretrained.py --help"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: pretrained.py\n",
            "       [-h]\n",
            "       --checkpoint\n",
            "       CHECKPOINT\n",
            "       --words-file\n",
            "       WORDS_FILE\n",
            "       --HLG\n",
            "       HLG\n",
            "       sound_files\n",
            "       [sound_files ...]\n",
            "\n",
            "positional arguments:\n",
            "  sound_files\n",
            "    The input\n",
            "    sound\n",
            "    file(s) to\n",
            "    transcribe.\n",
            "    Supported\n",
            "    formats are\n",
            "    those\n",
            "    supported\n",
            "    by torchaud\n",
            "    io.load().\n",
            "    For\n",
            "    example,\n",
            "    wav and\n",
            "    flac are\n",
            "    supported.\n",
            "    The sample\n",
            "    rate has to\n",
            "    be 16kHz.\n",
            "\n",
            "options:\n",
            "  -h, --help\n",
            "    show this\n",
            "    help\n",
            "    message and\n",
            "    exit\n",
            "  --checkpoint CHECKPOINT\n",
            "    Path to the\n",
            "    checkpoint.\n",
            "    The\n",
            "    checkpoint\n",
            "    is assumed\n",
            "    to be saved\n",
            "    by icefall.\n",
            "    checkpoint.\n",
            "    save_checkp\n",
            "    oint().\n",
            "    (default:\n",
            "    None)\n",
            "  --words-file WORDS_FILE\n",
            "    Path to\n",
            "    words.txt\n",
            "    (default:\n",
            "    None)\n",
            "  --HLG HLG\n",
            "    Path to\n",
            "    HLG.pt.\n",
            "    (default:\n",
            "    None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2zEQaGxwUob"
      },
      "source": [
        "### Decode a single sound file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxMbwNH0s9Sl",
        "outputId": "e1d6f9cb-2a46-40d6-c3f6-7e303990655e"
      },
      "source": [
        "! export PYTHONPATH=/content/icefall:$PYTHONPATH && \\\n",
        "  cd /content/icefall/egs/yesno/ASR && \\\n",
        "  ./tdnn/pretrained.py \\\n",
        "    --checkpoint ./tmp/icefall_asr_yesno_tdnn/pretrained.pt \\\n",
        "    --words-file ./tmp/icefall_asr_yesno_tdnn/lang_phone/words.txt \\\n",
        "    --HLG ./tmp/icefall_asr_yesno_tdnn/lang_phone/HLG.pt \\\n",
        "    ./tmp/icefall_asr_yesno_tdnn/test_waves/0_0_1_0_1_0_0_1.wav"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-27 05:12:18,157 INFO [pretrained.py:116] {'feature_dim': 23, 'num_classes': 4, 'sample_rate': 8000, 'search_beam': 20, 'output_beam': 8, 'min_active_states': 30, 'max_active_states': 10000, 'use_double_scores': True, 'checkpoint': './tmp/icefall_asr_yesno_tdnn/pretrained.pt', 'words_file': './tmp/icefall_asr_yesno_tdnn/lang_phone/words.txt', 'HLG': './tmp/icefall_asr_yesno_tdnn/lang_phone/HLG.pt', 'sound_files': ['./tmp/icefall_asr_yesno_tdnn/test_waves/0_0_1_0_1_0_0_1.wav']}\n",
            "2023-07-27 05:12:18,177 INFO [pretrained.py:122] device: cuda:0\n",
            "2023-07-27 05:12:18,177 INFO [pretrained.py:124] Creating model\n",
            "2023-07-27 05:12:20,058 INFO [pretrained.py:136] Loading HLG from ./tmp/icefall_asr_yesno_tdnn/lang_phone/HLG.pt\n",
            "2023-07-27 05:12:20,061 INFO [pretrained.py:140] Constructing Fbank computer\n",
            "2023-07-27 05:12:20,061 INFO [pretrained.py:150] Reading sound files: ['./tmp/icefall_asr_yesno_tdnn/test_waves/0_0_1_0_1_0_0_1.wav']\n",
            "2023-07-27 05:12:20,062 INFO [pretrained.py:156] Decoding started\n",
            "2023-07-27 05:12:24,120 INFO [pretrained.py:193] \n",
            "./tmp/icefall_asr_yesno_tdnn/test_waves/0_0_1_0_1_0_0_1.wav:\n",
            "NO NO YES NO YES NO NO YES\n",
            "\n",
            "\n",
            "2023-07-27 05:12:24,120 INFO [pretrained.py:195] Decoding Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxT9rP6WwXdO"
      },
      "source": [
        "### Decode multiple sound files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6bYzDehvWZI",
        "outputId": "db9401ec-a73a-4826-bfa6-2080a57909f7"
      },
      "source": [
        "! export PYTHONPATH=/content/icefall:$PYTHONPATH && \\\n",
        "  cd /content/icefall/egs/yesno/ASR && \\\n",
        "  ./tdnn/pretrained.py \\\n",
        "    --checkpoint ./tmp/icefall_asr_yesno_tdnn/pretrained.pt \\\n",
        "    --words-file ./tmp/icefall_asr_yesno_tdnn/lang_phone/words.txt \\\n",
        "    --HLG ./tmp/icefall_asr_yesno_tdnn/lang_phone/HLG.pt \\\n",
        "    ./tmp/icefall_asr_yesno_tdnn/test_waves/0_0_1_0_1_0_0_1.wav \\\n",
        "    ./tmp/icefall_asr_yesno_tdnn/test_waves/1_0_1_1_0_1_1_1.wav"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-27 05:12:28,541 INFO [pretrained.py:116] {'feature_dim': 23, 'num_classes': 4, 'sample_rate': 8000, 'search_beam': 20, 'output_beam': 8, 'min_active_states': 30, 'max_active_states': 10000, 'use_double_scores': True, 'checkpoint': './tmp/icefall_asr_yesno_tdnn/pretrained.pt', 'words_file': './tmp/icefall_asr_yesno_tdnn/lang_phone/words.txt', 'HLG': './tmp/icefall_asr_yesno_tdnn/lang_phone/HLG.pt', 'sound_files': ['./tmp/icefall_asr_yesno_tdnn/test_waves/0_0_1_0_1_0_0_1.wav', './tmp/icefall_asr_yesno_tdnn/test_waves/1_0_1_1_0_1_1_1.wav']}\n",
            "2023-07-27 05:12:28,562 INFO [pretrained.py:122] device: cuda:0\n",
            "2023-07-27 05:12:28,562 INFO [pretrained.py:124] Creating model\n",
            "2023-07-27 05:12:30,331 INFO [pretrained.py:136] Loading HLG from ./tmp/icefall_asr_yesno_tdnn/lang_phone/HLG.pt\n",
            "2023-07-27 05:12:30,334 INFO [pretrained.py:140] Constructing Fbank computer\n",
            "2023-07-27 05:12:30,334 INFO [pretrained.py:150] Reading sound files: ['./tmp/icefall_asr_yesno_tdnn/test_waves/0_0_1_0_1_0_0_1.wav', './tmp/icefall_asr_yesno_tdnn/test_waves/1_0_1_1_0_1_1_1.wav']\n",
            "2023-07-27 05:12:30,336 INFO [pretrained.py:156] Decoding started\n",
            "2023-07-27 05:12:31,935 INFO [pretrained.py:193] \n",
            "./tmp/icefall_asr_yesno_tdnn/test_waves/0_0_1_0_1_0_0_1.wav:\n",
            "NO NO YES NO YES NO NO YES\n",
            "\n",
            "./tmp/icefall_asr_yesno_tdnn/test_waves/1_0_1_1_0_1_1_1.wav:\n",
            "YES NO YES YES NO YES YES YES\n",
            "\n",
            "\n",
            "2023-07-27 05:12:31,935 INFO [pretrained.py:195] Decoding Done\n"
          ]
        }
      ]
    }
  ]
}
