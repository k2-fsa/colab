{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPrNcyWquYvnLMJhEHyMO/q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/csukuangfj/colab/blob/master/sherpa_standalone_offline_transducer_speech_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "This colab notebook demonstrates how to use [sherpa][sherpa]\n",
        "for offline (i.e., non-streaming) speech recognition.\n",
        "\n",
        "It includes:\n",
        "- How to setup the environment\n",
        "- How to download pre-trained models\n",
        "- How to use the pre-trained models for speech recognition\n",
        "\n",
        "\n",
        "[sherpa]: https://github.com/k2-fsa/sherpa"
      ],
      "metadata": {
        "id": "EAcnQKF8PFGb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup the environment"
      ],
      "metadata": {
        "id": "RGiQQ-IKQAvL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install PyTorch"
      ],
      "metadata": {
        "id": "Zv0M1HpsQGXu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Colab has already instaed PyTorch for us. Let us check the version of PyTorch."
      ],
      "metadata": {
        "id": "8anZxo8cQKap"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVgH0BiWOjO2",
        "outputId": "8dace591-78db-424a-c765-0171a734e013"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.1+cu118\n"
          ]
        }
      ],
      "source": [
        "! python3 -c \"import torch; print(torch.__version__)\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install k2"
      ],
      "metadata": {
        "id": "-aX_QHjJj4st"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We follow https://k2-fsa.github.io/k2/installation/from_wheels.html to install [k2][k2]\n",
        "\n",
        "Since the installed torch is of version `2.0.1+cu118`, we have to install a version of `k2` that is compiled against `torch 2.0.1+cu118`.\n",
        "\n",
        "From https://k2-fsa.github.io/k2/cuda.html we know the latest version is `k2-1.24.3.dev20230718+cuda11.8.torch2.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl`, so we use the following command to install [k2][k2]:\n",
        "\n",
        "[k2]: https://github.com/k2-fsa/k2"
      ],
      "metadata": {
        "id": "CKA6rHNbkCN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install k2==1.24.3.dev20230718+cuda11.8.torch2.0.1 -f https://k2-fsa.github.io/k2/cuda.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xos56IgzkBp3",
        "outputId": "768ab80c-64cf-4f5c-da48-d3b45e458426"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://k2-fsa.github.io/k2/cuda.html\n",
            "Collecting k2==1.24.3.dev20230718+cuda11.8.torch2.0.1\n",
            "  Downloading https://huggingface.co/csukuangfj/k2/resolve/main/cuda/k2-1.24.3.dev20230718%2Bcuda11.8.torch2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.9/117.9 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (2.0.1+cu118)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (0.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (1.3.0)\n",
            "Installing collected packages: k2\n",
            "Successfully installed k2-1.24.3.dev20230718+cuda11.8.torch2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check that [k2][k2] has been successfully installed:\n",
        "\n",
        "[k2]: https://github.com/k2-fsa/k2"
      ],
      "metadata": {
        "id": "kykWGYzLWH8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python3 -m k2.version"
      ],
      "metadata": {
        "id": "4W7NcMFGWLlg",
        "outputId": "baeb8a55-684e-43f4-9973-cc546826ccd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting environment information...\n",
            "\n",
            "k2 version: 1.24.3\n",
            "Build type: Release\n",
            "Git SHA1: e400fa3b456faf8afe0ee5bfe572946b4921a3db\n",
            "Git date: Sat Jul 15 04:21:50 2023\n",
            "Cuda used to build k2: 11.8\n",
            "cuDNN used to build k2: \n",
            "Python version used to build k2: 3.10\n",
            "OS used to build k2: CentOS Linux release 7.9.2009 (Core)\n",
            "CMake version: 3.26.4\n",
            "GCC version: 9.3.1\n",
            "CMAKE_CUDA_FLAGS:  -Wno-deprecated-gpu-targets   -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w  --expt-extended-lambda -gencode arch=compute_35,code=sm_35  -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w  --expt-extended-lambda -gencode arch=compute_50,code=sm_50  -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w  --expt-extended-lambda -gencode arch=compute_60,code=sm_60  -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w  --expt-extended-lambda -gencode arch=compute_61,code=sm_61  -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w  --expt-extended-lambda -gencode arch=compute_70,code=sm_70  -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w  --expt-extended-lambda -gencode arch=compute_75,code=sm_75  -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w  --expt-extended-lambda -gencode arch=compute_80,code=sm_80  -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w  --expt-extended-lambda -gencode arch=compute_86,code=sm_86 -DONNX_NAMESPACE=onnx_c2 -gencode arch=compute_35,code=sm_35 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_89,code=compute_89 -gencode arch=compute_90,code=compute_90 -Xcudafe --diag_suppress=cc_clobber_ignored,--diag_suppress=integer_sign_change,--diag_suppress=useless_using_declaration,--diag_suppress=set_but_not_used,--diag_suppress=field_without_dll_interface,--diag_suppress=base_class_has_different_dll_interface,--diag_suppress=dll_interface_conflict_none_assumed,--diag_suppress=dll_interface_conflict_dllexport_assumed,--diag_suppress=implicit_return_from_non_void_function,--diag_suppress=unsigned_compare_with_zero,--diag_suppress=declared_but_not_referenced,--diag_suppress=bad_friend_decl --expt-relaxed-constexpr --expt-extended-lambda -D_GLIBCXX_USE_CXX11_ABI=0 --compiler-options -Wall  --compiler-options -Wno-strict-overflow  --compiler-options -Wno-unknown-pragmas \n",
            "CMAKE_CXX_FLAGS:  -D_GLIBCXX_USE_CXX11_ABI=0 -Wno-unused-variable  -Wno-strict-overflow \n",
            "PyTorch version used to build k2: 2.0.1+cu118\n",
            "PyTorch is using Cuda: 11.8\n",
            "NVTX enabled: True\n",
            "With CUDA: True\n",
            "Disable debug: True\n",
            "Sync kernels : False\n",
            "Disable checks: False\n",
            "Max cpu memory allocate: 214748364800 bytes (or 200.0 GB)\n",
            "k2 abort: False\n",
            "__file__: /usr/local/lib/python3.10/dist-packages/k2/version/version.py\n",
            "_k2.__file__: /usr/local/lib/python3.10/dist-packages/_k2.cpython-310-x86_64-linux-gnu.so\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Kaldifeat"
      ],
      "metadata": {
        "id": "usmPsfhYkeHt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the method from https://csukuangfj.github.io/kaldifeat/installation/from_wheels.html#linux-cuda\n",
        "to install `kaldifeat`."
      ],
      "metadata": {
        "id": "Nn3Umx3snknn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install kaldifeat==1.24.dev20230722+cuda11.8.torch2.0.1 -f https://csukuangfj.github.io/kaldifeat/cuda.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqjcSEoSkfcU",
        "outputId": "3d115330-dd48-42f9-c313-b6f290db092a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://csukuangfj.github.io/kaldifeat/cuda.html\n",
            "Collecting kaldifeat==1.24.dev20230722+cuda11.8.torch2.0.1\n",
            "  Downloading https://huggingface.co/csukuangfj/kaldifeat/resolve/main/ubuntu-cuda/kaldifeat-1.24.dev20230722%2Bcuda11.8.torch2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (574 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m574.0/574.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kaldifeat\n",
            "Successfully installed kaldifeat-1.24.dev20230722+cuda11.8.torch2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check that `kaldifeat` has been installed successfully:"
      ],
      "metadata": {
        "id": "Rlmajtisn50O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python3 -c \"import kaldifeat; print(kaldifeat.__version__)\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zut7mNLyn84Z",
        "outputId": "7cd392cf-d693-4b38-807a-f276a1813295"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.24.dev20230722+cuda11.8.torch2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install sherpa"
      ],
      "metadata": {
        "id": "hrXCaqHIkZxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install https://huggingface.co/csukuangfj/sherpa/resolve/main/ubuntu-cuda/k2_sherpa-1.3.dev20230725%2Bcuda11.8.torch2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FmO2NKMxbo6",
        "outputId": "7c9e5fbf-7c8d-4e5f-f800-6ddbbf5a495e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting k2-sherpa==1.3.dev20230725+cuda11.8.torch2.0.1\n",
            "  Downloading https://huggingface.co/csukuangfj/sherpa/resolve/main/ubuntu-cuda/k2_sherpa-1.3.dev20230725%2Bcuda11.8.torch2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: k2-sherpa\n",
            "Successfully installed k2-sherpa-1.3.dev20230725+cuda11.8.torch2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify that we have installed `sherpa` sucessfully:"
      ],
      "metadata": {
        "id": "xpsweBIk44aX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! sherpa-offline --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7gLiKeT8Hih",
        "outputId": "f359b68b-c0a0-46db-d9b9-7a0a1210aafd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[I] /var/www/sherpa/csrc/parse-options.cc:536:void sherpa::ParseOptions::PrintUsage(bool) const 2023-07-25 07:32:42.318 \n",
            "\n",
            "Offline (non-streaming) automatic speech recognition with sherpa.\n",
            "\n",
            "Usage:\n",
            "(1) View help information.\n",
            "\n",
            "  ./bin/sherpa-offline --help\n",
            "\n",
            "(2) Use a pretrained model for recognition\n",
            "\n",
            "  ./bin/sherpa-offline \\\n",
            "    --nn-model=/path/to/cpu_jit.pt \\\n",
            "    --tokens=/path/to/tokens.txt \\\n",
            "    --use-gpu=false \\\n",
            "    foo.wav \\\n",
            "    bar.wav\n",
            "\n",
            "(3) Decode wav.scp\n",
            "\n",
            "  ./bin/sherpa-offline \\\n",
            "    --nn-model=/path/to/cpu_jit.pt \\\n",
            "    --tokens=/path/to/tokens.txt \\\n",
            "    --use-gpu=false \\\n",
            "    --use-wav-scp=true \\\n",
            "    scp:wav.scp \\\n",
            "    ark,scp,t:results.ark,results.scp\n",
            "\n",
            "(4) Decode feats.scp\n",
            "\n",
            "  ./bin/sherpa-offline \\\n",
            "    --nn-model=/path/to/cpu_jit.pt \\\n",
            "    --tokens=/path/to/tokens.txt \\\n",
            "    --use-gpu=false \\\n",
            "    --use-feats-scp=true \\\n",
            "    scp:feats.scp \\\n",
            "    ark,scp,t:results.ark,results.scp\n",
            "\n",
            "Caution: Models from icefall use normalized audio samples, i.e., samples in\n",
            "the range [-1, 1), to compute features,\n",
            "while Kaldi uses samples in the range [-32768, 32767] to compute features.\n",
            "If you use `feats.scp` from Kaldi with models from icefall, you won't get\n",
            "expected results.\n",
            "\n",
            "See:\n",
            "\n",
            "  https://k2-fsa.github.io/sherpa/sherpa/pretrained_models/offline_transducer.html\n",
            "  https://k2-fsa.github.io/sherpa/sherpa/pretrained_models/offline_ctc/index.html\n",
            "\n",
            "for more details.\n",
            "\n",
            "Options:\n",
            "  --num-active-paths          : Number of active paths for modified_beam_search. Used only when --decoding-method is modified_beam_search (int, default = 4)\n",
            "  --decoding-method           : Decoding method to use. Possible values are: greedy_search, modified_beam_search, and fast_beam_search (string, default = \"greedy_search\")\n",
            "  --use-gpu                   : true to use GPU for computation. false to use CPU.\n",
            "If true, it uses the first device. You can use the environment variable CUDA_VISIBLE_DEVICES to select which device to use. (bool, default = false)\n",
            "  --batch-size                : Used only when --use-wav-scp=true or --use-feats-scp=true. It specifies the batch size to use for decoding (int, default = 10)\n",
            "  --nn-model                  : Path to the torchscript model (string, default = \"\")\n",
            "  --normalize-samples         : true to use samples in the range [-1, 1]. false to use samples in the range [-32768, 32767]. Note: kaldi uses un-normalized samples. (bool, default = true)\n",
            "  --nemo-normalize            : See https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/asr/parts/preprocessing/features.py#L59Current supported value: per_feature or leave it to empty (unset) (string, default = \"\")\n",
            "  --num-mel-bins              : Number of triangular mel-frequency bins (int, default = 80)\n",
            "  --dither                    : Dithering constant (0.0 means no dither). Caution: Samples are normalized to the range [-1, 1). Please select a small value for dither if you want to enable it (float, default = 0)\n",
            "  --frame-shift               : Frame shift in milliseconds (float, default = 10)\n",
            "  --beam                      : Beam used in fast_beam_search (float, default = 20)\n",
            "  --context-score             : The bonus score for each token in context word/phrase. Used only when decoding_method is modified_beam_search (float, default = 1.5)\n",
            "  --frame-length              : Frame length in milliseconds (float, default = 25)\n",
            "  --use-wav-scp               : If true, user should provide two arguments: scp:wav.scp ark,scp,t:results.ark,results.scp (bool, default = false)\n",
            "  --tokens                    : Path to tokens.txt. (string, default = \"\")\n",
            "  --ngram-lm-scale            : Scale the scores from LG.pt. Used only for fast_beam_search in transducer decoding (float, default = 0.01)\n",
            "  --hlg                       : Used only for decoding with an HLG graph.  (string, default = \"\")\n",
            "  --modified                  : Used only for decoding with a CTC topology. true to use a modified CTC topology; useful when vocab_size is large, e.g., > 1000. false to use a standard CTC topology. (bool, default = true)\n",
            "  --lm-scale                  : Used only for decoding with an HLG graph. It specifies the scale for HLG.scores (float, default = 1)\n",
            "  --search-beam               : Used only for CTC decoding. Decoding beam, e.g. 20.  Smaller is faster, larger is more exact (less pruning). This is the default value; it may be modified by `min_active_states` and `max_active_states`.  (float, default = 20)\n",
            "  --min-active-states         : Minimum number of FSA states that are allowed to be active on any given frame for any given intersection/composition task. This is advisory, in that it will try not to have fewer than this number active. Set it to zero if there is no constraint.  (int, default = 30)\n",
            "  --max-active-states         : max_activate_states  Maximum number of FSA states that are allowed to be active on any given frame for any given intersection/composition task. This is advisory, in that it will try not to exceed that but may not always succeed. You can use a very large number if no constraint is needed.  (int, default = 10000)\n",
            "  --use-feats-scp             : If true, user should provide two arguments: scp:feats.scp ark,scp,t:results.ark,results.scp (bool, default = false)\n",
            "  --lg                        : Path to LG.pt. Used only for fast_beam_search in transducer decoding (string, default = \"\")\n",
            "  --sample-frequency          : Waveform data sample frequency (must match the waveform file, if specified there) (float, default = 16000)\n",
            "  --output-beam               : Used only for CTC decoding. Beam to prune output, similar to lattice-beam in Kaldi. Relative to the best path of output.  (float, default = 8)\n",
            "\n",
            "Standard options:\n",
            "  --help                      : Print out usage message (bool, default = false)\n",
            "  --print-args                : Print the command line arguments (to stderr) (bool, default = true)\n",
            "  --config                    : Configuration file to read (this option may be repeated) (string, default = \"\")\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python3 -c \"import sherpa; print(sherpa.__file__)\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwoVUQBBm4xB",
        "outputId": "69cbd822-daf5-43dd-832a-64c6850a0704"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sherpa/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python3 -c \"import sherpa; print(sherpa.__version__)\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdTLJ9Qa9yhv",
        "outputId": "aa8d1ae7-1bb2-4bc5-e483-5cb9d0bc146a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.3.dev20230725+cuda11.8.torch2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! which sherpa-online"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8dWhU-jwgnc",
        "outputId": "c091d39f-21ab-4042-d875-b3c37be385c6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/bin/sherpa-online\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download pre-trained models"
      ],
      "metadata": {
        "id": "TxO_ZP9u_S7a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have a lot of pre-trained models listed at\n",
        "https://k2-fsa.github.io/sherpa/cpp/pretrained_models/offline_transducer.html#\n",
        "for downloading.\n",
        "\n",
        "In the following, we use\n",
        "<https://huggingface.co/WeijiZhuang/icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02>\n",
        "for demonstration. You can replace it with other pre-trained models if you like."
      ],
      "metadata": {
        "id": "4efve-0hA9mf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/WeijiZhuang/icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02\n",
        "! cd icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02 && \\\n",
        "  git lfs pull --include \"exp/cpu_jit-torch-1.10.pt\" && \\\n",
        "  git lfs pull --include \"data/lang_bpe_500/LG.pt\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URVu2_cN_ZJZ",
        "outputId": "b5c5674a-be86-4d86-bae5-4b28d406c029"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02'...\n",
            "remote: Enumerating objects: 2217, done.\u001b[K\n",
            "remote: Total 2217 (delta 0), reused 0 (delta 0), pack-reused 2217\u001b[K\n",
            "Receiving objects: 100% (2217/2217), 15.14 MiB | 24.27 MiB/s, done.\n",
            "Resolving deltas: 100% (1861/1861), done.\n",
            "Updating files: 100% (2530/2530), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Speech recognition"
      ],
      "metadata": {
        "id": "IQbliPKo_9CW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## greedy_search"
      ],
      "metadata": {
        "id": "7ub0AoLD_-uD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! sherpa-offline \\\n",
        "  --nn-model=./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/exp/cpu_jit-torch-1.10.pt \\\n",
        "  --tokens=./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/data/lang_bpe_500/tokens.txt \\\n",
        "  --decoding-method=greedy_search \\\n",
        "  --num-active-paths=4 \\\n",
        "  --use-gpu=false \\\n",
        "  ./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/test_wavs/1089-134686-0001.wav \\\n",
        "  ./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/test_wavs/1221-135766-0001.wav \\\n",
        "  ./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/test_wavs/1221-135766-0002.wav"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zehxw-VM42NT",
        "outputId": "6c8cf881-86f6-4914-9e51-ad66f140f26c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[I] /var/www/sherpa/csrc/parse-options.cc:495:int sherpa::ParseOptions::Read(int, const char* const*) 2023-07-25 07:33:43.089 sherpa-offline --nn-model=./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/exp/cpu_jit-torch-1.10.pt --tokens=./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/data/lang_bpe_500/tokens.txt --decoding-method=greedy_search --num-active-paths=4 --use-gpu=false ./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/test_wavs/1089-134686-0001.wav ./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/test_wavs/1221-135766-0001.wav ./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/test_wavs/1221-135766-0002.wav \n",
            "\n",
            "[I] /var/www/sherpa/cpp_api/bin/offline-recognizer.cc:125:int main(int, char**) 2023-07-25 07:33:43.089 OfflineRecognizerConfig(ctc_decoder_config=OfflineCtcDecoderConfig(modified=True, hlg=\"\", lm_scale=1, search_beam=20, output_beam=8, min_active_states=30, max_active_states=10000), feat_config=FeatureConfig(fbank_opts=FbankOptions(frame_opts=FrameExtractionOptions(samp_freq=16000, frame_shift_ms=10, frame_length_ms=25, dither=0, preemph_coeff=0.97, remove_dc_offset=True, window_type=\"povey\", round_to_power_of_two=True, blackman_coeff=0.42, snip_edges=True, max_feature_vectors=-1), mel_opts=MelBanksOptions(num_bins=80, low_freq=20, high_freq=0, vtln_low=100, vtln_high=-500, debug_mel=False, htk_mode=False), use_energy=False, energy_floor=0, raw_energy=True, htk_compat=False, use_log_fbank=True, use_power=True, device=\"cpu\"), normalize_samples=True, nemo_normalize=\"\"), nn_model=\"./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/exp/cpu_jit-torch-1.10.pt\", tokens=\"./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/data/lang_bpe_500/tokens.txt\", use_gpu=False, decoding_method=\"greedy_search\", num_active_paths=4, context_score=1.5)\n",
            "[I] /var/www/sherpa/cpp_api/offline-recognizer-transducer-impl.h:144:void sherpa::OfflineRecognizerTransducerImpl::WarmUp() 2023-07-25 07:33:44.101 WarmUp begins\n",
            "[I] /var/www/sherpa/cpp_api/offline-recognizer-transducer-impl.h:157:void sherpa::OfflineRecognizerTransducerImpl::WarmUp() 2023-07-25 07:33:44.571 WarmUp ended\n",
            "[I] /var/www/sherpa/cpp_api/bin/offline-recognizer.cc:316:int main(int, char**) 2023-07-25 07:33:48.271 \n",
            "filename: ./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/test_wavs/1089-134686-0001.wav\n",
            "result:  AFTER EARLY NIGHTFALL THE YELLOW LAMPS WOULD LIGHT UP HERE AND THERE THE SQUALID QUARTER OF THE BROTHELS\n",
            "{\"text\":\" AFTER EARLY NIGHTFALL THE YELLOW LAMPS WOULD LIGHT UP HERE AND THERE THE SQUALID QUARTER OF THE BROTHELS\",\"timestamps\":\"[0.00,0.40,0.52,0.68,0.96,1.28,1.36,1.44,1.60,1.76,1.88,1.96,2.16,2.28,2.36,2.48,2.60,2.76,3.04,3.24,3.40,3.56,3.76,4.04,4.20,4.32,4.48,4.64,4.80,4.84,5.00,5.04,5.32,5.44,5.60,5.68,5.84,6.04,6.24]\",\"tokens\":[\" AFTER\",\" E\",\"AR\",\"LY\",\" NIGHT\",\"F\",\"A\",\"LL\",\" THE\",\" YE\",\"LL\",\"OW\",\" LA\",\"M\",\"P\",\"S\",\" WOULD\",\" LIGHT\",\" UP\",\" HE\",\"RE\",\" AND\",\" THERE\",\" THE\",\" S\",\"QUA\",\"LI\",\"D\",\" \",\"QUA\",\"R\",\"TER\",\" OF\",\" THE\",\" B\",\"RO\",\"TH\",\"EL\",\"S\"]}\n",
            "\n",
            "filename: ./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/test_wavs/1221-135766-0001.wav\n",
            "result:  GOD AS A DIRECT CONSEQUENCE OF THE SIN WHICH MAN THUS PUNISHED HAD GIVEN HER A LOVELY CHILD WHOSE PLACE WAS ON THAT SAME DISHONOURED BOSOM TO CONNECT HER PARENT FOR EVER WITH THE RACE AND DESCENT OF MORTALS AND TO BE FINALLY A BLESSED SOUL IN HEAVEN\n",
            "{\"text\":\" GOD AS A DIRECT CONSEQUENCE OF THE SIN WHICH MAN THUS PUNISHED HAD GIVEN HER A LOVELY CHILD WHOSE PLACE WAS ON THAT SAME DISHONOURED BOSOM TO CONNECT HER PARENT FOR EVER WITH THE RACE AND DESCENT OF MORTALS AND TO BE FINALLY A BLESSED SOUL IN HEAVEN\",\"timestamps\":\"[0.00,0.12,0.40,0.64,0.80,0.96,1.04,1.12,1.28,1.44,1.64,1.72,1.84,1.96,2.12,2.28,2.36,2.60,2.84,3.16,3.28,3.48,3.60,3.76,3.92,4.12,4.36,4.52,4.72,4.92,5.12,5.40,5.68,6.04,6.24,6.48,6.84,7.08,7.32,7.56,7.84,8.12,8.24,8.32,8.44,8.56,8.76,8.88,9.08,9.28,9.44,9.56,9.64,9.76,9.96,10.04,10.20,10.40,10.64,10.76,11.00,11.20,11.36,11.56,11.76,12.00,12.08,12.28,12.32,12.52,12.68,12.84,12.96,13.04,13.20,13.36,13.60,13.76,13.96,14.12,14.24,14.36,14.52,14.68,14.76,15.04,15.28,15.52,15.76,16.00,16.16,16.24,16.32]\",\"tokens\":[\" GO\",\"D\",\" AS\",\" A\",\" DI\",\"RE\",\"C\",\"T\",\" CON\",\"SE\",\"QUE\",\"N\",\"CE\",\" OF\",\" THE\",\" S\",\"IN\",\" WHICH\",\" MAN\",\" TH\",\"US\",\" P\",\"UN\",\"ISH\",\"ED\",\" HAD\",\" GIVE\",\"N\",\" HER\",\" A\",\" LOVE\",\"LY\",\" CHILD\",\" WHO\",\"SE\",\" PLACE\",\" WAS\",\" ON\",\" THAT\",\" SAME\",\" DIS\",\"HO\",\"N\",\"OUR\",\"ED\",\" BO\",\"S\",\"OM\",\" TO\",\" CON\",\"NE\",\"C\",\"T\",\" HER\",\" P\",\"AR\",\"ENT\",\" FOR\",\" E\",\"VER\",\" WITH\",\" THE\",\" RA\",\"CE\",\" AND\",\" DE\",\"S\",\"C\",\"ENT\",\" OF\",\" MO\",\"R\",\"T\",\"AL\",\"S\",\" AND\",\" TO\",\" BE\",\" FI\",\"N\",\"AL\",\"LY\",\" A\",\" B\",\"LESS\",\"ED\",\" SO\",\"UL\",\" IN\",\" HE\",\"A\",\"VE\",\"N\"]}\n",
            "\n",
            "filename: ./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/test_wavs/1221-135766-0002.wav\n",
            "result:  YET THESE THOUGHTS AFFECTED HESTER PRYNNE LESS WITH HOPE THAN APPREHENSION\n",
            "{\"text\":\" YET THESE THOUGHTS AFFECTED HESTER PRYNNE LESS WITH HOPE THAN APPREHENSION\",\"timestamps\":\"[0.00,0.08,0.32,0.44,0.64,0.96,1.08,1.20,1.28,1.40,1.44,1.64,1.76,1.84,2.04,2.12,2.24,2.28,2.48,2.52,2.84,3.08,3.28,3.52,3.76,3.88,4.00,4.08,4.20,4.36,4.52]\",\"tokens\":[\" YE\",\"T\",\" THE\",\"SE\",\" THOUGHT\",\"S\",\" A\",\"FF\",\"E\",\"C\",\"TED\",\" HE\",\"S\",\"TER\",\" P\",\"RY\",\"N\",\"NE\",\" \",\"LESS\",\" WITH\",\" HO\",\"PE\",\" THAN\",\" A\",\"PP\",\"RE\",\"HE\",\"N\",\"S\",\"ION\"]}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## modified_beam_search"
      ],
      "metadata": {
        "id": "iPXJ4-wrASvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! sherpa-offline \\\n",
        "  --nn-model=./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/exp/cpu_jit-torch-1.10.pt \\\n",
        "  --tokens=./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/data/lang_bpe_500/tokens.txt \\\n",
        "  --decoding-method=modified_beam_search \\\n",
        "  --num-active-paths=4 \\\n",
        "  --use-gpu=false \\\n",
        "  ./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/test_wavs/1089-134686-0001.wav \\\n",
        "  ./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/test_wavs/1221-135766-0001.wav \\\n",
        "  ./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/test_wavs/1221-135766-0002.wav"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBLP9X_XAN1L",
        "outputId": "63d446a1-972d-4dcb-bf92-31b51e30337b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[I] /var/www/sherpa/csrc/parse-options.cc:495:int sherpa::ParseOptions::Read(int, const char* const*) 2023-07-25 07:35:51.541 sherpa-offline --nn-model=./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/exp/cpu_jit-torch-1.10.pt --tokens=./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/data/lang_bpe_500/tokens.txt --decoding-method=modified_beam_search --num-active-paths=4 --use-gpu=false ./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/test_wavs/1089-134686-0001.wav ./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/test_wavs/1221-135766-0001.wav ./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/test_wavs/1221-135766-0002.wav \n",
            "\n",
            "[I] /var/www/sherpa/cpp_api/bin/offline-recognizer.cc:125:int main(int, char**) 2023-07-25 07:35:51.541 OfflineRecognizerConfig(ctc_decoder_config=OfflineCtcDecoderConfig(modified=True, hlg=\"\", lm_scale=1, search_beam=20, output_beam=8, min_active_states=30, max_active_states=10000), feat_config=FeatureConfig(fbank_opts=FbankOptions(frame_opts=FrameExtractionOptions(samp_freq=16000, frame_shift_ms=10, frame_length_ms=25, dither=0, preemph_coeff=0.97, remove_dc_offset=True, window_type=\"povey\", round_to_power_of_two=True, blackman_coeff=0.42, snip_edges=True, max_feature_vectors=-1), mel_opts=MelBanksOptions(num_bins=80, low_freq=20, high_freq=0, vtln_low=100, vtln_high=-500, debug_mel=False, htk_mode=False), use_energy=False, energy_floor=0, raw_energy=True, htk_compat=False, use_log_fbank=True, use_power=True, device=\"cpu\"), normalize_samples=True, nemo_normalize=\"\"), nn_model=\"./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/exp/cpu_jit-torch-1.10.pt\", tokens=\"./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/data/lang_bpe_500/tokens.txt\", use_gpu=False, decoding_method=\"modified_beam_search\", num_active_paths=4, context_score=1.5)\n",
            "[I] /var/www/sherpa/cpp_api/offline-recognizer-transducer-impl.h:144:void sherpa::OfflineRecognizerTransducerImpl::WarmUp() 2023-07-25 07:35:52.487 WarmUp begins\n",
            "[I] /var/www/sherpa/cpp_api/offline-recognizer-transducer-impl.h:157:void sherpa::OfflineRecognizerTransducerImpl::WarmUp() 2023-07-25 07:35:52.679 WarmUp ended\n",
            "[I] /var/www/sherpa/cpp_api/bin/offline-recognizer.cc:316:int main(int, char**) 2023-07-25 07:35:56.729 \n",
            "filename: ./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/test_wavs/1089-134686-0001.wav\n",
            "result:  AFTER EARLY NIGHTFALL THE YELLOW LAMPS WOULD LIGHT UP HERE AND THERE THE SQUALID QUARTER OF THE BROTHELS\n",
            "{\"text\":\" AFTER EARLY NIGHTFALL THE YELLOW LAMPS WOULD LIGHT UP HERE AND THERE THE SQUALID QUARTER OF THE BROTHELS\",\"timestamps\":\"[0.00,0.40,0.52,0.68,0.96,1.24,1.36,1.44,1.60,1.76,1.88,1.96,2.16,2.28,2.36,2.48,2.60,2.76,3.04,3.24,3.40,3.56,3.76,4.04,4.20,4.32,4.48,4.64,4.80,4.84,5.00,5.04,5.32,5.44,5.60,5.68,5.84,6.04,6.24]\",\"tokens\":[\" AFTER\",\" E\",\"AR\",\"LY\",\" NIGHT\",\"F\",\"A\",\"LL\",\" THE\",\" YE\",\"LL\",\"OW\",\" LA\",\"M\",\"P\",\"S\",\" WOULD\",\" LIGHT\",\" UP\",\" HE\",\"RE\",\" AND\",\" THERE\",\" THE\",\" S\",\"QUA\",\"LI\",\"D\",\" \",\"QUA\",\"R\",\"TER\",\" OF\",\" THE\",\" B\",\"RO\",\"TH\",\"EL\",\"S\"]}\n",
            "\n",
            "filename: ./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/test_wavs/1221-135766-0001.wav\n",
            "result:  GOD AS A DIRECT CONSEQUENCE OF THE SIN WHICH MAN THUS PUNISHED HAD GIVEN HER A LOVELY CHILD WHOSE PLACE WAS ON THAT SAME DISHONOURED BOSOM TO CONNECT HER PARENT FOR EVER WITH THE RACE AND DESCENT OF MORTALS AND TO BE FINALLY A BLESSED SOUL IN HEAVEN\n",
            "{\"text\":\" GOD AS A DIRECT CONSEQUENCE OF THE SIN WHICH MAN THUS PUNISHED HAD GIVEN HER A LOVELY CHILD WHOSE PLACE WAS ON THAT SAME DISHONOURED BOSOM TO CONNECT HER PARENT FOR EVER WITH THE RACE AND DESCENT OF MORTALS AND TO BE FINALLY A BLESSED SOUL IN HEAVEN\",\"timestamps\":\"[0.00,0.12,0.40,0.64,0.80,0.96,1.04,1.12,1.28,1.44,1.64,1.72,1.84,1.96,2.12,2.28,2.36,2.60,2.84,3.16,3.28,3.48,3.60,3.76,3.92,4.12,4.36,4.52,4.72,4.92,5.12,5.40,5.68,6.04,6.24,6.48,6.84,7.08,7.32,7.56,7.84,8.12,8.24,8.32,8.44,8.56,8.76,8.88,9.08,9.28,9.44,9.56,9.64,9.76,9.96,10.04,10.20,10.40,10.64,10.76,11.00,11.20,11.36,11.56,11.76,12.00,12.08,12.28,12.32,12.52,12.68,12.84,12.96,13.04,13.20,13.36,13.60,13.76,13.96,14.12,14.24,14.36,14.52,14.68,14.76,15.04,15.28,15.52,15.76,16.00,16.16,16.24,16.32]\",\"tokens\":[\" GO\",\"D\",\" AS\",\" A\",\" DI\",\"RE\",\"C\",\"T\",\" CON\",\"SE\",\"QUE\",\"N\",\"CE\",\" OF\",\" THE\",\" S\",\"IN\",\" WHICH\",\" MAN\",\" TH\",\"US\",\" P\",\"UN\",\"ISH\",\"ED\",\" HAD\",\" GIVE\",\"N\",\" HER\",\" A\",\" LOVE\",\"LY\",\" CHILD\",\" WHO\",\"SE\",\" PLACE\",\" WAS\",\" ON\",\" THAT\",\" SAME\",\" DIS\",\"HO\",\"N\",\"OUR\",\"ED\",\" BO\",\"S\",\"OM\",\" TO\",\" CON\",\"NE\",\"C\",\"T\",\" HER\",\" P\",\"AR\",\"ENT\",\" FOR\",\" E\",\"VER\",\" WITH\",\" THE\",\" RA\",\"CE\",\" AND\",\" DE\",\"S\",\"C\",\"ENT\",\" OF\",\" MO\",\"R\",\"T\",\"AL\",\"S\",\" AND\",\" TO\",\" BE\",\" FI\",\"N\",\"AL\",\"LY\",\" A\",\" B\",\"LESS\",\"ED\",\" SO\",\"UL\",\" IN\",\" HE\",\"A\",\"VE\",\"N\"]}\n",
            "\n",
            "filename: ./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/test_wavs/1221-135766-0002.wav\n",
            "result:  YET THESE THOUGHTS AFFECTED HESTER PRYNNE LESS WITH HOPE THAN APPREHENSION\n",
            "{\"text\":\" YET THESE THOUGHTS AFFECTED HESTER PRYNNE LESS WITH HOPE THAN APPREHENSION\",\"timestamps\":\"[0.00,0.08,0.32,0.44,0.64,0.96,1.08,1.20,1.28,1.40,1.44,1.64,1.76,1.84,2.04,2.12,2.24,2.32,2.48,2.52,2.84,3.08,3.28,3.52,3.76,3.88,4.00,4.08,4.20,4.36,4.52]\",\"tokens\":[\" YE\",\"T\",\" THE\",\"SE\",\" THOUGHT\",\"S\",\" A\",\"FF\",\"E\",\"C\",\"TED\",\" HE\",\"S\",\"TER\",\" P\",\"RY\",\"N\",\"NE\",\" \",\"LESS\",\" WITH\",\" HO\",\"PE\",\" THAN\",\" A\",\"PP\",\"RE\",\"HE\",\"N\",\"S\",\"ION\"]}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fast_beam_search (without LG)"
      ],
      "metadata": {
        "id": "Fu-_938tAplz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! sherpa-offline \\\n",
        "  --nn-model=./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/exp/cpu_jit-torch-1.10.pt \\\n",
        "  --tokens=./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/data/lang_bpe_500/tokens.txt \\\n",
        "  --decoding-method=fast_beam_search \\\n",
        "  --num-active-paths=4 \\\n",
        "  --use-gpu=false \\\n",
        "  ./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/test_wavs/1089-134686-0001.wav \\\n",
        "  ./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/test_wavs/1221-135766-0001.wav \\\n",
        "  ./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/test_wavs/1221-135766-0002.wav"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8b-fOvCAkl_",
        "outputId": "89842381-2a28-4f91-c779-e722cdd1ba03"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[I] /var/www/sherpa/csrc/parse-options.cc:495:int sherpa::ParseOptions::Read(int, const char* const*) 2023-07-25 07:36:26.071 sherpa-offline --nn-model=./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/exp/cpu_jit-torch-1.10.pt --tokens=./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/data/lang_bpe_500/tokens.txt --decoding-method=fast_beam_search --num-active-paths=4 --use-gpu=false ./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/test_wavs/1089-134686-0001.wav ./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/test_wavs/1221-135766-0001.wav ./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/test_wavs/1221-135766-0002.wav \n",
            "\n",
            "[I] /var/www/sherpa/cpp_api/bin/offline-recognizer.cc:125:int main(int, char**) 2023-07-25 07:36:26.071 OfflineRecognizerConfig(ctc_decoder_config=OfflineCtcDecoderConfig(modified=True, hlg=\"\", lm_scale=1, search_beam=20, output_beam=8, min_active_states=30, max_active_states=10000), feat_config=FeatureConfig(fbank_opts=FbankOptions(frame_opts=FrameExtractionOptions(samp_freq=16000, frame_shift_ms=10, frame_length_ms=25, dither=0, preemph_coeff=0.97, remove_dc_offset=True, window_type=\"povey\", round_to_power_of_two=True, blackman_coeff=0.42, snip_edges=True, max_feature_vectors=-1), mel_opts=MelBanksOptions(num_bins=80, low_freq=20, high_freq=0, vtln_low=100, vtln_high=-500, debug_mel=False, htk_mode=False), use_energy=False, energy_floor=0, raw_energy=True, htk_compat=False, use_log_fbank=True, use_power=True, device=\"cpu\"), normalize_samples=True, nemo_normalize=\"\"), nn_model=\"./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/exp/cpu_jit-torch-1.10.pt\", tokens=\"./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/data/lang_bpe_500/tokens.txt\", use_gpu=False, decoding_method=\"fast_beam_search\", num_active_paths=4, context_score=1.5)\n",
            "[I] /var/www/sherpa/cpp_api/offline-recognizer-transducer-impl.h:144:void sherpa::OfflineRecognizerTransducerImpl::WarmUp() 2023-07-25 07:36:27.011 WarmUp begins\n",
            "[I] /var/www/sherpa/cpp_api/offline-recognizer-transducer-impl.h:157:void sherpa::OfflineRecognizerTransducerImpl::WarmUp() 2023-07-25 07:36:27.214 WarmUp ended\n",
            "[I] /var/www/sherpa/cpp_api/bin/offline-recognizer.cc:316:int main(int, char**) 2023-07-25 07:36:32.281 \n",
            "filename: ./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/test_wavs/1089-134686-0001.wav\n",
            "result:  AFTER EARLY NIGHTFALL THE YELLOW LAMPS WOULD LIGHT UP HERE AND THERE THE SQUALID QUARTER OF THE BROTHELS\n",
            "{\"text\":\" AFTER EARLY NIGHTFALL THE YELLOW LAMPS WOULD LIGHT UP HERE AND THERE THE SQUALID QUARTER OF THE BROTHELS\",\"timestamps\":\"[0.00,0.40,0.52,0.68,0.96,1.28,1.36,1.44,1.60,1.76,1.88,1.96,2.16,2.28,2.36,2.48,2.60,2.76,3.04,3.24,3.40,3.56,3.76,4.04,4.20,4.32,4.48,4.64,4.80,4.84,5.00,5.04,5.32,5.44,5.60,5.68,5.84,6.04,6.24]\",\"tokens\":[\" AFTER\",\" E\",\"AR\",\"LY\",\" NIGHT\",\"F\",\"A\",\"LL\",\" THE\",\" YE\",\"LL\",\"OW\",\" LA\",\"M\",\"P\",\"S\",\" WOULD\",\" LIGHT\",\" UP\",\" HE\",\"RE\",\" AND\",\" THERE\",\" THE\",\" S\",\"QUA\",\"LI\",\"D\",\" \",\"QUA\",\"R\",\"TER\",\" OF\",\" THE\",\" B\",\"RO\",\"TH\",\"EL\",\"S\"]}\n",
            "\n",
            "filename: ./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/test_wavs/1221-135766-0001.wav\n",
            "result:  GOD AS A DIRECT CONSEQUENCE OF THE SIN WHICH MAN THUS PUNISHED HAD GIVEN HER A LOVELY CHILD WHOSE PLACE WAS ON THAT SAME DISHONOURED BOSOM TO CONNECT HER PARENT FOR EVER WITH THE RACE AND DESCENT OF MORTALS AND TO BE FINALLY A BLESSED SOUL IN HEAVEN\n",
            "{\"text\":\" GOD AS A DIRECT CONSEQUENCE OF THE SIN WHICH MAN THUS PUNISHED HAD GIVEN HER A LOVELY CHILD WHOSE PLACE WAS ON THAT SAME DISHONOURED BOSOM TO CONNECT HER PARENT FOR EVER WITH THE RACE AND DESCENT OF MORTALS AND TO BE FINALLY A BLESSED SOUL IN HEAVEN\",\"timestamps\":\"[0.00,0.12,0.40,0.64,0.80,0.96,1.04,1.12,1.28,1.44,1.64,1.72,1.84,1.96,2.12,2.28,2.36,2.60,2.84,3.16,3.28,3.48,3.60,3.76,3.92,4.12,4.36,4.52,4.72,4.92,5.12,5.40,5.68,6.04,6.24,6.48,6.84,7.08,7.32,7.56,7.84,8.12,8.24,8.32,8.44,8.56,8.76,8.88,9.08,9.28,9.44,9.56,9.64,9.76,9.96,10.04,10.20,10.40,10.64,10.76,11.00,11.20,11.36,11.56,11.76,12.00,12.08,12.28,12.32,12.52,12.68,12.84,12.96,13.04,13.20,13.36,13.60,13.76,13.96,14.12,14.24,14.36,14.52,14.68,14.76,15.04,15.28,15.52,15.76,16.00,16.16,16.24,16.32]\",\"tokens\":[\" GO\",\"D\",\" AS\",\" A\",\" DI\",\"RE\",\"C\",\"T\",\" CON\",\"SE\",\"QUE\",\"N\",\"CE\",\" OF\",\" THE\",\" S\",\"IN\",\" WHICH\",\" MAN\",\" TH\",\"US\",\" P\",\"UN\",\"ISH\",\"ED\",\" HAD\",\" GIVE\",\"N\",\" HER\",\" A\",\" LOVE\",\"LY\",\" CHILD\",\" WHO\",\"SE\",\" PLACE\",\" WAS\",\" ON\",\" THAT\",\" SAME\",\" DIS\",\"HO\",\"N\",\"OUR\",\"ED\",\" BO\",\"S\",\"OM\",\" TO\",\" CON\",\"NE\",\"C\",\"T\",\" HER\",\" P\",\"AR\",\"ENT\",\" FOR\",\" E\",\"VER\",\" WITH\",\" THE\",\" RA\",\"CE\",\" AND\",\" DE\",\"S\",\"C\",\"ENT\",\" OF\",\" MO\",\"R\",\"T\",\"AL\",\"S\",\" AND\",\" TO\",\" BE\",\" FI\",\"N\",\"AL\",\"LY\",\" A\",\" B\",\"LESS\",\"ED\",\" SO\",\"UL\",\" IN\",\" HE\",\"A\",\"VE\",\"N\"]}\n",
            "\n",
            "filename: ./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/test_wavs/1221-135766-0002.wav\n",
            "result:  YET THESE THOUGHTS AFFECTED HESTER PRYNNE LESS WITH HOPE THAN APPREHENSION\n",
            "{\"text\":\" YET THESE THOUGHTS AFFECTED HESTER PRYNNE LESS WITH HOPE THAN APPREHENSION\",\"timestamps\":\"[0.00,0.08,0.32,0.44,0.64,0.96,1.08,1.20,1.28,1.40,1.44,1.64,1.76,1.84,2.04,2.12,2.24,2.28,2.48,2.52,2.84,3.08,3.28,3.52,3.76,3.88,4.00,4.08,4.20,4.36,4.52]\",\"tokens\":[\" YE\",\"T\",\" THE\",\"SE\",\" THOUGHT\",\"S\",\" A\",\"FF\",\"E\",\"C\",\"TED\",\" HE\",\"S\",\"TER\",\" P\",\"RY\",\"N\",\"NE\",\" \",\"LESS\",\" WITH\",\" HO\",\"PE\",\" THAN\",\" A\",\"PP\",\"RE\",\"HE\",\"N\",\"S\",\"ION\"]}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fast_beam_search (with LG)"
      ],
      "metadata": {
        "id": "1l1ZfV3XAzDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! sherpa-offline --help\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmAiuIu4-npV",
        "outputId": "1fbf4ff3-65dc-4b17-ea65-c27d50c83a8f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[I] /var/www/sherpa/csrc/parse-options.cc:536:void sherpa::ParseOptions::PrintUsage(bool) const 2023-07-25 07:36:51.401 \n",
            "\n",
            "Offline (non-streaming) automatic speech recognition with sherpa.\n",
            "\n",
            "Usage:\n",
            "(1) View help information.\n",
            "\n",
            "  ./bin/sherpa-offline --help\n",
            "\n",
            "(2) Use a pretrained model for recognition\n",
            "\n",
            "  ./bin/sherpa-offline \\\n",
            "    --nn-model=/path/to/cpu_jit.pt \\\n",
            "    --tokens=/path/to/tokens.txt \\\n",
            "    --use-gpu=false \\\n",
            "    foo.wav \\\n",
            "    bar.wav\n",
            "\n",
            "(3) Decode wav.scp\n",
            "\n",
            "  ./bin/sherpa-offline \\\n",
            "    --nn-model=/path/to/cpu_jit.pt \\\n",
            "    --tokens=/path/to/tokens.txt \\\n",
            "    --use-gpu=false \\\n",
            "    --use-wav-scp=true \\\n",
            "    scp:wav.scp \\\n",
            "    ark,scp,t:results.ark,results.scp\n",
            "\n",
            "(4) Decode feats.scp\n",
            "\n",
            "  ./bin/sherpa-offline \\\n",
            "    --nn-model=/path/to/cpu_jit.pt \\\n",
            "    --tokens=/path/to/tokens.txt \\\n",
            "    --use-gpu=false \\\n",
            "    --use-feats-scp=true \\\n",
            "    scp:feats.scp \\\n",
            "    ark,scp,t:results.ark,results.scp\n",
            "\n",
            "Caution: Models from icefall use normalized audio samples, i.e., samples in\n",
            "the range [-1, 1), to compute features,\n",
            "while Kaldi uses samples in the range [-32768, 32767] to compute features.\n",
            "If you use `feats.scp` from Kaldi with models from icefall, you won't get\n",
            "expected results.\n",
            "\n",
            "See:\n",
            "\n",
            "  https://k2-fsa.github.io/sherpa/sherpa/pretrained_models/offline_transducer.html\n",
            "  https://k2-fsa.github.io/sherpa/sherpa/pretrained_models/offline_ctc/index.html\n",
            "\n",
            "for more details.\n",
            "\n",
            "Options:\n",
            "  --num-active-paths          : Number of active paths for modified_beam_search. Used only when --decoding-method is modified_beam_search (int, default = 4)\n",
            "  --decoding-method           : Decoding method to use. Possible values are: greedy_search, modified_beam_search, and fast_beam_search (string, default = \"greedy_search\")\n",
            "  --use-gpu                   : true to use GPU for computation. false to use CPU.\n",
            "If true, it uses the first device. You can use the environment variable CUDA_VISIBLE_DEVICES to select which device to use. (bool, default = false)\n",
            "  --batch-size                : Used only when --use-wav-scp=true or --use-feats-scp=true. It specifies the batch size to use for decoding (int, default = 10)\n",
            "  --nn-model                  : Path to the torchscript model (string, default = \"\")\n",
            "  --normalize-samples         : true to use samples in the range [-1, 1]. false to use samples in the range [-32768, 32767]. Note: kaldi uses un-normalized samples. (bool, default = true)\n",
            "  --nemo-normalize            : See https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/asr/parts/preprocessing/features.py#L59Current supported value: per_feature or leave it to empty (unset) (string, default = \"\")\n",
            "  --num-mel-bins              : Number of triangular mel-frequency bins (int, default = 80)\n",
            "  --dither                    : Dithering constant (0.0 means no dither). Caution: Samples are normalized to the range [-1, 1). Please select a small value for dither if you want to enable it (float, default = 0)\n",
            "  --frame-shift               : Frame shift in milliseconds (float, default = 10)\n",
            "  --beam                      : Beam used in fast_beam_search (float, default = 20)\n",
            "  --context-score             : The bonus score for each token in context word/phrase. Used only when decoding_method is modified_beam_search (float, default = 1.5)\n",
            "  --frame-length              : Frame length in milliseconds (float, default = 25)\n",
            "  --use-wav-scp               : If true, user should provide two arguments: scp:wav.scp ark,scp,t:results.ark,results.scp (bool, default = false)\n",
            "  --tokens                    : Path to tokens.txt. (string, default = \"\")\n",
            "  --ngram-lm-scale            : Scale the scores from LG.pt. Used only for fast_beam_search in transducer decoding (float, default = 0.01)\n",
            "  --hlg                       : Used only for decoding with an HLG graph.  (string, default = \"\")\n",
            "  --modified                  : Used only for decoding with a CTC topology. true to use a modified CTC topology; useful when vocab_size is large, e.g., > 1000. false to use a standard CTC topology. (bool, default = true)\n",
            "  --lm-scale                  : Used only for decoding with an HLG graph. It specifies the scale for HLG.scores (float, default = 1)\n",
            "  --search-beam               : Used only for CTC decoding. Decoding beam, e.g. 20.  Smaller is faster, larger is more exact (less pruning). This is the default value; it may be modified by `min_active_states` and `max_active_states`.  (float, default = 20)\n",
            "  --min-active-states         : Minimum number of FSA states that are allowed to be active on any given frame for any given intersection/composition task. This is advisory, in that it will try not to have fewer than this number active. Set it to zero if there is no constraint.  (int, default = 30)\n",
            "  --max-active-states         : max_activate_states  Maximum number of FSA states that are allowed to be active on any given frame for any given intersection/composition task. This is advisory, in that it will try not to exceed that but may not always succeed. You can use a very large number if no constraint is needed.  (int, default = 10000)\n",
            "  --use-feats-scp             : If true, user should provide two arguments: scp:feats.scp ark,scp,t:results.ark,results.scp (bool, default = false)\n",
            "  --lg                        : Path to LG.pt. Used only for fast_beam_search in transducer decoding (string, default = \"\")\n",
            "  --sample-frequency          : Waveform data sample frequency (must match the waveform file, if specified there) (float, default = 16000)\n",
            "  --output-beam               : Used only for CTC decoding. Beam to prune output, similar to lattice-beam in Kaldi. Relative to the best path of output.  (float, default = 8)\n",
            "\n",
            "Standard options:\n",
            "  --help                      : Print out usage message (bool, default = false)\n",
            "  --print-args                : Print the command line arguments (to stderr) (bool, default = true)\n",
            "  --config                    : Configuration file to read (this option may be repeated) (string, default = \"\")\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! sherpa-offline \\\n",
        "  --nn-model=./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/exp/cpu_jit-torch-1.10.pt \\\n",
        "  --tokens=./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/data/lang_bpe_500/tokens.txt \\\n",
        "  --decoding-method=fast_beam_search \\\n",
        "  --num-active-paths=4 \\\n",
        "  --use-gpu=false \\\n",
        "  --lg=./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/data/lang_bpe_500/LG.pt \\\n",
        "  ./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/test_wavs/1089-134686-0001.wav \\\n",
        "  ./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/test_wavs/1221-135766-0001.wav \\\n",
        "  ./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/test_wavs/1221-135766-0002.wav"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeEL7wHw-vyK",
        "outputId": "2c83cf4b-19f0-4f6a-fabd-20693c503831"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[I] /var/www/sherpa/csrc/parse-options.cc:495:int sherpa::ParseOptions::Read(int, const char* const*) 2023-07-25 07:37:44.988 sherpa-offline --nn-model=./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/exp/cpu_jit-torch-1.10.pt --tokens=./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/data/lang_bpe_500/tokens.txt --decoding-method=fast_beam_search --num-active-paths=4 --use-gpu=false --lg=./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/data/lang_bpe_500/LG.pt ./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/test_wavs/1089-134686-0001.wav ./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/test_wavs/1221-135766-0001.wav ./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/test_wavs/1221-135766-0002.wav \n",
            "\n",
            "[I] /var/www/sherpa/cpp_api/bin/offline-recognizer.cc:125:int main(int, char**) 2023-07-25 07:37:44.988 OfflineRecognizerConfig(ctc_decoder_config=OfflineCtcDecoderConfig(modified=True, hlg=\"\", lm_scale=1, search_beam=20, output_beam=8, min_active_states=30, max_active_states=10000), feat_config=FeatureConfig(fbank_opts=FbankOptions(frame_opts=FrameExtractionOptions(samp_freq=16000, frame_shift_ms=10, frame_length_ms=25, dither=0, preemph_coeff=0.97, remove_dc_offset=True, window_type=\"povey\", round_to_power_of_two=True, blackman_coeff=0.42, snip_edges=True, max_feature_vectors=-1), mel_opts=MelBanksOptions(num_bins=80, low_freq=20, high_freq=0, vtln_low=100, vtln_high=-500, debug_mel=False, htk_mode=False), use_energy=False, energy_floor=0, raw_energy=True, htk_compat=False, use_log_fbank=True, use_power=True, device=\"cpu\"), normalize_samples=True, nemo_normalize=\"\"), nn_model=\"./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/exp/cpu_jit-torch-1.10.pt\", tokens=\"./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/data/lang_bpe_500/tokens.txt\", use_gpu=False, decoding_method=\"fast_beam_search\", num_active_paths=4, context_score=1.5)\n",
            "[I] /var/www/sherpa/cpp_api/offline-recognizer-transducer-impl.h:144:void sherpa::OfflineRecognizerTransducerImpl::WarmUp() 2023-07-25 07:37:45.949 WarmUp begins\n",
            "[I] /var/www/sherpa/cpp_api/offline-recognizer-transducer-impl.h:157:void sherpa::OfflineRecognizerTransducerImpl::WarmUp() 2023-07-25 07:37:46.148 WarmUp ended\n",
            "[I] /var/www/sherpa/cpp_api/bin/offline-recognizer.cc:316:int main(int, char**) 2023-07-25 07:37:52.846 \n",
            "filename: ./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/test_wavs/1089-134686-0001.wav\n",
            "result:  AFTER EARLY NIGHTFALL THE YELLOW LAMPS WOULD LIGHT UP HERE AND THERE THE SQUALID QUARTER OF THE BROTHELS\n",
            "{\"text\":\" AFTER EARLY NIGHTFALL THE YELLOW LAMPS WOULD LIGHT UP HERE AND THERE THE SQUALID QUARTER OF THE BROTHELS\",\"timestamps\":\"[0.00,0.40,0.52,0.68,0.96,1.28,1.36,1.44,1.60,1.76,1.88,1.96,2.16,2.28,2.36,2.48,2.60,2.76,3.04,3.24,3.40,3.56,3.76,4.04,4.20,4.32,4.48,4.64,4.80,4.84,5.00,5.04,5.32,5.44,5.60,5.68,5.84,6.04,6.24]\",\"tokens\":[\" AFTER\",\" E\",\"AR\",\"LY\",\" NIGHT\",\"F\",\"A\",\"LL\",\" THE\",\" YE\",\"LL\",\"OW\",\" LA\",\"M\",\"P\",\"S\",\" WOULD\",\" LIGHT\",\" UP\",\" HE\",\"RE\",\" AND\",\" THERE\",\" THE\",\" S\",\"QUA\",\"LI\",\"D\",\" \",\"QUA\",\"R\",\"TER\",\" OF\",\" THE\",\" B\",\"RO\",\"TH\",\"EL\",\"S\"]}\n",
            "\n",
            "filename: ./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/test_wavs/1221-135766-0001.wav\n",
            "result:  GOD AS A DIRECT CONSEQUENCE OF THE SIN WHICH MAN THUS PUNISHED HAD GIVEN HER A LOVELY CHILD WHOSE PLACE WAS ON THAT SAME DISHONOURED BOSOM TO CONNECT HER PARENT FOR EVER WITH THE RACE AND DESCENT OF MORTALS AND TO BE FINALLY A BLESSED SOUL IN HEAVEN\n",
            "{\"text\":\" GOD AS A DIRECT CONSEQUENCE OF THE SIN WHICH MAN THUS PUNISHED HAD GIVEN HER A LOVELY CHILD WHOSE PLACE WAS ON THAT SAME DISHONOURED BOSOM TO CONNECT HER PARENT FOR EVER WITH THE RACE AND DESCENT OF MORTALS AND TO BE FINALLY A BLESSED SOUL IN HEAVEN\",\"timestamps\":\"[0.00,0.12,0.40,0.64,0.80,0.96,1.04,1.12,1.28,1.44,1.64,1.72,1.84,1.96,2.12,2.28,2.36,2.60,2.84,3.16,3.28,3.48,3.60,3.76,3.92,4.12,4.36,4.52,4.72,4.92,5.12,5.40,5.68,6.04,6.24,6.48,6.84,7.08,7.32,7.56,7.84,8.12,8.24,8.32,8.44,8.56,8.76,8.88,9.08,9.28,9.44,9.56,9.64,9.76,9.96,10.04,10.20,10.40,10.64,10.76,11.00,11.20,11.36,11.56,11.76,12.00,12.08,12.28,12.32,12.52,12.68,12.84,12.96,13.04,13.20,13.36,13.60,13.76,13.96,14.12,14.24,14.36,14.52,14.68,14.76,15.04,15.28,15.52,15.76,16.00,16.16,16.24,16.32]\",\"tokens\":[\" GO\",\"D\",\" AS\",\" A\",\" DI\",\"RE\",\"C\",\"T\",\" CON\",\"SE\",\"QUE\",\"N\",\"CE\",\" OF\",\" THE\",\" S\",\"IN\",\" WHICH\",\" MAN\",\" TH\",\"US\",\" P\",\"UN\",\"ISH\",\"ED\",\" HAD\",\" GIVE\",\"N\",\" HER\",\" A\",\" LOVE\",\"LY\",\" CHILD\",\" WHO\",\"SE\",\" PLACE\",\" WAS\",\" ON\",\" THAT\",\" SAME\",\" DIS\",\"HO\",\"N\",\"OUR\",\"ED\",\" BO\",\"S\",\"OM\",\" TO\",\" CON\",\"NE\",\"C\",\"T\",\" HER\",\" P\",\"AR\",\"ENT\",\" FOR\",\" E\",\"VER\",\" WITH\",\" THE\",\" RA\",\"CE\",\" AND\",\" DE\",\"S\",\"C\",\"ENT\",\" OF\",\" MO\",\"R\",\"T\",\"AL\",\"S\",\" AND\",\" TO\",\" BE\",\" FI\",\"N\",\"AL\",\"LY\",\" A\",\" B\",\"LESS\",\"ED\",\" SO\",\"UL\",\" IN\",\" HE\",\"A\",\"VE\",\"N\"]}\n",
            "\n",
            "filename: ./icefall-asr-librispeech-pruned-transducer-stateless8-2022-12-02/test_wavs/1221-135766-0002.wav\n",
            "result:  YET THESE THOUGHTS AFFECTED HESTER PRYNNE LESS WITH HOPE THAN APPREHENSION\n",
            "{\"text\":\" YET THESE THOUGHTS AFFECTED HESTER PRYNNE LESS WITH HOPE THAN APPREHENSION\",\"timestamps\":\"[0.00,0.08,0.32,0.44,0.64,0.96,1.08,1.20,1.28,1.40,1.44,1.64,1.76,1.84,2.04,2.12,2.24,2.28,2.48,2.52,2.84,3.08,3.28,3.52,3.76,3.88,4.00,4.08,4.20,4.36,4.52]\",\"tokens\":[\" YE\",\"T\",\" THE\",\"SE\",\" THOUGHT\",\"S\",\" A\",\"FF\",\"E\",\"C\",\"TED\",\" HE\",\"S\",\"TER\",\" P\",\"RY\",\"N\",\"NE\",\" \",\"LESS\",\" WITH\",\" HO\",\"PE\",\" THAN\",\" A\",\"PP\",\"RE\",\"HE\",\"N\",\"S\",\"ION\"]}\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}