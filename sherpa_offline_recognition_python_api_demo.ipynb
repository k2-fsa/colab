{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/csukuangfj/colab/blob/master/sherpa_offline_recognition_python_api_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sherpa Offline Recognition Python API Demo"
      ],
      "metadata": {
        "id": "geCWFQffEu1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "This notebook shows how to install [sherpa][sherpa] and use\n",
        "it as offline recognizer.\n",
        "\n",
        "Sherpa supports the models from icefall, the wenet framework and torchaudio.\n",
        "- Conformer-CTC trained with gigaspeech\n",
        "- Conformer-CTC trained with wenet framework\n",
        "- torchaudio wav2vec 2.0 fine-tuned with CTC\n",
        "\n",
        "\n",
        "[sherpa]: https//github.com/k2-fsa/sherpa"
      ],
      "metadata": {
        "id": "uh7HZTSEEhM8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display the environment provided by Colab"
      ],
      "metadata": {
        "id": "riP12d9gEnHl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1YTvVr9Chdn",
        "outputId": "575f1803-460f-43dc-e7df-276dc7b2c9b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting environment information...\n",
            "PyTorch version: 2.0.1+cu118\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 11.8\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.2 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.25.2\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-5.15.109+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 11.8.89\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 525.105.17\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.0\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.0\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.0\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.0\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.0\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.0\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.0\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                    x86_64\n",
            "CPU op-mode(s):                  32-bit, 64-bit\n",
            "Address sizes:                   46 bits physical, 48 bits virtual\n",
            "Byte Order:                      Little Endian\n",
            "CPU(s):                          2\n",
            "On-line CPU(s) list:             0,1\n",
            "Vendor ID:                       GenuineIntel\n",
            "Model name:                      Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "CPU family:                      6\n",
            "Model:                           63\n",
            "Thread(s) per core:              2\n",
            "Core(s) per socket:              1\n",
            "Socket(s):                       1\n",
            "Stepping:                        0\n",
            "BogoMIPS:                        4599.99\n",
            "Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:               KVM\n",
            "Virtualization type:             full\n",
            "L1d cache:                       32 KiB (1 instance)\n",
            "L1i cache:                       32 KiB (1 instance)\n",
            "L2 cache:                        256 KiB (1 instance)\n",
            "L3 cache:                        45 MiB (1 instance)\n",
            "NUMA node(s):                    1\n",
            "NUMA node0 CPU(s):               0,1\n",
            "Vulnerability Itlb multihit:     Not affected\n",
            "Vulnerability L1tf:              Mitigation; PTE Inversion\n",
            "Vulnerability Mds:               Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:          Vulnerable\n",
            "Vulnerability Mmio stale data:   Vulnerable\n",
            "Vulnerability Retbleed:          Vulnerable\n",
            "Vulnerability Spec store bypass: Vulnerable\n",
            "Vulnerability Spectre v1:        Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:        Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBRS: Not affected\n",
            "Vulnerability Srbds:             Not affected\n",
            "Vulnerability Tsx async abort:   Not affected\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] numpy==1.23.5\n",
            "[pip3] torch==2.0.1+cu118\n",
            "[pip3] torchaudio==2.0.2+cu118\n",
            "[pip3] torchdata==0.6.1\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchtext==0.15.2\n",
            "[pip3] torchvision==0.15.2+cu118\n",
            "[conda] Could not collect\n"
          ]
        }
      ],
      "source": [
        "! python3 -m torch.utils.collect_env"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It shows that the torch version is\n",
        "`2.0.1+cu118`."
      ],
      "metadata": {
        "id": "BxnsszazE-Y_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install k2"
      ],
      "metadata": {
        "id": "O02AwWNiFR0q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please refer to\n",
        "https://k2-fsa.github.io/k2/installation/from_wheels.html\n",
        "for how to install `k2`."
      ],
      "metadata": {
        "id": "J6zYMlS_HSKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Since the above output shows torch is 2.0.1+cu118, we install a version\n",
        "# of k2 that is compiled against torch 2.0.1+cu118\n",
        "\n",
        "! pip install k2==1.24.3.dev20230718+cuda11.8.torch2.0.1 -f https://k2-fsa.github.io/k2/cuda.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKmPv5HvFigr",
        "outputId": "ec3465d7-8c63-4423-9bfa-1783e31f7cfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://k2-fsa.github.io/k2/cuda.html\n",
            "Collecting k2==1.24.3.dev20230718+cuda11.8.torch2.0.1\n",
            "  Downloading https://huggingface.co/csukuangfj/k2/resolve/main/cuda/k2-1.24.3.dev20230718%2Bcuda11.8.torch2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.9/117.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (2.0.1+cu118)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (0.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (1.3.0)\n",
            "Installing collected packages: k2\n",
            "Successfully installed k2-1.24.3.dev20230718+cuda11.8.torch2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install kaldifeat"
      ],
      "metadata": {
        "id": "j0JskI1xFQGu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please refer to\n",
        "https://csukuangfj.github.io/kaldifeat/installation/from_wheels.html\n",
        "to install kaldifeat."
      ],
      "metadata": {
        "id": "c_JVud3VHq4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install kaldifeat==1.25.0.dev20230726+cuda11.8.torch2.0.1 -f https://csukuangfj.github.io/kaldifeat/cuda.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2vEGPtLHPBf",
        "outputId": "66a13519-96a7-4974-e703-27275795febf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://csukuangfj.github.io/kaldifeat/cuda.html\n",
            "Collecting kaldifeat==1.25.0.dev20230726+cuda11.8.torch2.0.1\n",
            "  Downloading https://huggingface.co/csukuangfj/kaldifeat/resolve/main/ubuntu-cuda/kaldifeat-1.25.0.dev20230726%2Bcuda11.8.torch2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (574 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m574.0/574.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kaldifeat\n",
            "Successfully installed kaldifeat-1.25.0.dev20230726+cuda11.8.torch2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Sherpa"
      ],
      "metadata": {
        "id": "yOq4cyhcFU7c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please refer to\n",
        "https://k2-fsa.github.io/sherpa/sherpa/install/from_wheels.html\n",
        "to install sherpa."
      ],
      "metadata": {
        "id": "xntKEc-iIAGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install k2_sherpa==1.3.dev20230725+cuda11.8.torch2.0.1 -f https://k2-fsa.github.io/sherpa/cuda.html"
      ],
      "metadata": {
        "id": "a3LMiqk4H_wa",
        "outputId": "26151bee-1b79-4d3d-b92b-402ed5bf5dfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://k2-fsa.github.io/sherpa/cuda.html\n",
            "Collecting k2_sherpa==1.3.dev20230725+cuda11.8.torch2.0.1\n",
            "  Downloading https://huggingface.co/csukuangfj/sherpa/resolve/main/ubuntu-cuda/k2_sherpa-1.3.dev20230725%2Bcuda11.8.torch2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: k2_sherpa\n",
            "Successfully installed k2_sherpa-1.3.dev20230725+cuda11.8.torch2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check that sherpa has been install successfully:"
      ],
      "metadata": {
        "id": "8urtUJpQIXWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python3 -c \"import sherpa; print(sherpa.__version__)\""
      ],
      "metadata": {
        "id": "trj6I-tPIbcx",
        "outputId": "00773c2f-6219-415f-bb28-ec474e6f5500",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.3.dev20230725+cuda11.8.torch2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! sherpa-version\n",
        "\n",
        "! sherpa-offline --help\n",
        "! sherpa-online --help\n",
        "! sherpa-online-microphone --help\n",
        "\n",
        "! sherpa-offline-websocket-server --help\n",
        "! sherpa-offline-websocket-client --help\n",
        "\n",
        "! sherpa-online-websocket-server --help\n",
        "! sherpa-online-websocket-client --help\n",
        "! sherpa-online-websocket-client-microphone --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WXyRlJdiRYa",
        "outputId": "8d150908-ef1b-4648-c71f-8f151b02fd25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sherpa version: 1.3\n",
            "build type: Release\n",
            "OS used to build sherpa: CentOS Linux release 7.9.2009 (Core)\n",
            "sherpa git sha1: b7b3bb064a263520394fbf99f785597a6e688457\n",
            "sherpa git date: Tue Jul 25 09:49:59 2023\n",
            "sherpa git branch: release\n",
            "PyTorch version used to build sherpa: 2.0.1+cu118\n",
            "CUDA version: 11.8\n",
            "cuDNN version: \n",
            "k2 version used to build sherpa: 1.24.3\n",
            "k2 git sha1: e400fa3b456faf8afe0ee5bfe572946b4921a3db\n",
            "k2 git date: Sat Jul 15 04:21:50 2023\n",
            "k2 with cuda: ON\n",
            "kaldifeat version used to build sherpa: 1.24\n",
            "cmake version: 3.27.0\n",
            "compiler ID: GNU\n",
            "compiler: /opt/rh/devtoolset-9/root/usr/bin/c++\n",
            "compiler version: 9.3.1\n",
            "cmake CXX flags:    -Wall  -g  -D_GLIBCXX_USE_CXX11_ABI=0   -D_GLIBCXX_USE_CXX11_ABI=0 -Wno-unused-variable  -Wno-strict-overflow   -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "Python version: 3.10\n",
            "[I] /var/www/sherpa/csrc/parse-options.cc:536:void sherpa::ParseOptions::PrintUsage(bool) const 2023-08-09 02:30:12.564 \n",
            "\n",
            "Online (streaming) automatic speech recognition with sherpa.\n",
            "\n",
            "Usage:\n",
            "(1) View help information.\n",
            "\n",
            "  sherpa-online --help\n",
            "\n",
            "(2) Use a pretrained model for recognition\n",
            "\n",
            "  sherpa-online \\\n",
            "    --nn-model=/path/to/cpu_jit.pt \\\n",
            "    --tokens=/path/to/tokens.txt \\\n",
            "    --use-gpu=false \\\n",
            "    --decoding-method=greedy_search\n",
            "    foo.wav \\\n",
            "    bar.wav\n",
            "\n",
            "To use fast_beam_search with an LG, use\n",
            "\n",
            "  sherpa-online \\\n",
            "    --decoding-method=fast_beam_search \\\n",
            "    --nn-model=/path/to/cpu_jit.pt \\\n",
            "    --tokens=/path/to/tokens.txt \\\n",
            "    --lg=/path/to/LG.pt \\\n",
            "    --use-gpu=false \\\n",
            "    foo.wav \\\n",
            "    bar.wav\n",
            "\n",
            "(3) To use an LSTM model for recognition\n",
            "\n",
            "  sherpa-online \\\n",
            "    --encoder-model=/path/to/encoder_jit_trace.pt \\\n",
            "    --decoder-model=/path/to/decoder_jit_trace.pt \\\n",
            "    --joiner-model=/path/to/joiner_jit_trace.pt \\\n",
            "    --tokens=/path/to/tokens.txt \\\n",
            "    --use-gpu=false \\\n",
            "    foo.wav \\\n",
            "    bar.wav\n",
            "\n",
            "(4) To use a streaming Zipformer model for recognition\n",
            "\n",
            "  sherpa-online \\\n",
            "    --nn-model=/path/to/cpu_jit.pt \\\n",
            "    --tokens=/path/to/tokens.txt \\\n",
            "    --use-gpu=false \\\n",
            "    foo.wav \\\n",
            "    bar.wav\n",
            "\n",
            "(5) To decode wav.scp\n",
            "\n",
            "  sherpa-online \\\n",
            "    --nn-model=/path/to/cpu_jit.pt \\\n",
            "    --tokens=/path/to/tokens.txt \\\n",
            "    --use-gpu=false \\\n",
            "    --use-wav-scp=true \\\n",
            "    scp:wav.scp \\\n",
            "    ark,scp,t:result.ark,result.scp\n",
            "\n",
            "See\n",
            "https://k2-fsa.github.io/sherpa/sherpa/pretrained_models/online_transducer.html\n",
            "for more details.\n",
            "\n",
            "Options:\n",
            "  --decode-chunk-size         : Used only for streaming Conformer, i.e, models from pruned_transducer_statelessX, and streaming Zipformer, i.e, models from pruned_transducer_stateless7_streaming in icefall.Number of frames before subsampling during decoding. (int, default = 12)\n",
            "  --decode-left-context       : Used only for streaming Conformer, i.e, models from pruned_transducer_statelessX, and streaming Zipformer, i.e, models from pruned_transducer_stateless7_streaming in icefall.Number of frames before subsampling during decoding. (int, default = 64)\n",
            "  --num-active-paths          : Number of active paths for modified_beam_search. Used only when --decoding-method is modified_beam_search (int, default = 4)\n",
            "  --use-endpoint              : true to enable Endpoint, false to disable Endpoint, default is false.\n",
            " (bool, default = false)\n",
            "  --use-gpu                   : true to use GPU for computation. false to use CPU.\n",
            "If true, it uses the first device. You can use the environment variable CUDA_VISIBLE_DEVICES to select which device to use. (bool, default = false)\n",
            "  --dither                    : Dithering constant (0.0 means no dither). Caution: Samples are normalized to the range [-1, 1). Please select a small value for dither if you want to enable it (float, default = 0)\n",
            "  --num-mel-bins              : Number of triangular mel-frequency bins (int, default = 80)\n",
            "  --lg                        : Path to LG.pt. Used only for fast_beam_search in transducer decoding (string, default = \"\")\n",
            "  --rule1-must-contain-nonsilence : If True, for this endpointing rule1 to apply there must be nonsilence in the best-path traceback. For decoding, a non-blank token is considered as non-silence (bool, default = false)\n",
            "  --padding-seconds           : Number of seconds for tail padding. (float, default = 0.8)\n",
            "  --use-wav-scp               : If true, user should provide two arguments: scp:wav.scp ark,scp,t:results.ark,results.scp (bool, default = false)\n",
            "  --sample-frequency          : Waveform data sample frequency (must match the waveform file, if specified there) (float, default = 16000)\n",
            "  --ngram-lm-scale            : Scale the scores from LG.pt. Used only for fast_beam_search in transducer decoding (float, default = 0.01)\n",
            "  --decoding-method           : Decoding method to use. Possible values are: greedy_search, modified_beam_search, and fast_beam_search. Used only for transducer. (string, default = \"greedy_search\")\n",
            "  --tokens                    : Path to tokens.txt. (string, default = \"\")\n",
            "  --frame-length              : Frame length in milliseconds (float, default = 25)\n",
            "  --frame-shift               : Frame shift in milliseconds (float, default = 10)\n",
            "  --decode-right-context      : Used only for streaming Conformer, i.e, models from pruned_transducer_statelessX, and streaming Zipformer, i.e, models from pruned_transducer_stateless7_streaming in icefall.Number of frames before subsampling during decoding. (int, default = 0)\n",
            "  --rule1-min-trailing-silence : This endpointing rule1 requires duration of trailing silence in seconds) to be >= this value. (float, default = 2.4)\n",
            "  --rule1-min-utterance-length : This endpointing rule1 requires utterance-length (in seconds) to be >= this value. (float, default = 0)\n",
            "  --rule2-must-contain-nonsilence : If True, for this endpointing rule2 to apply there must be nonsilence in the best-path traceback. For decoding, a non-blank token is considered as non-silence (bool, default = true)\n",
            "  --rule2-min-trailing-silence : This endpointing rule2 requires duration of trailing silence in seconds) to be >= this value. (float, default = 1.2)\n",
            "  --normalize-samples         : true to use samples in the range [-1, 1]. false to use samples in the range [-32768, 32767]. Note: kaldi uses un-normalized samples. (bool, default = true)\n",
            "  --joiner-model              : Path to the joiner model for OnlineLstmTransducerModel. (string, default = \"\")\n",
            "  --context-score             : The bonus score for each token in context word/phrase. Used only when decoding_method is modified_beam_search (float, default = 1.5)\n",
            "  --rule2-min-utterance-length : This endpointing rule2 requires utterance-length (in seconds) to be >= this value. (float, default = 0)\n",
            "  --beam                      : Beam used in fast_beam_search (float, default = 20)\n",
            "  --rule3-must-contain-nonsilence : If True, for this endpointing rule3 to apply there must be nonsilence in the best-path traceback. For decoding, a non-blank token is considered as non-silence (bool, default = false)\n",
            "  --rule3-min-utterance-length : This endpointing rule3 requires utterance-length (in seconds) to be >= this value. (float, default = 20)\n",
            "  --rule3-min-trailing-silence : This endpointing rule3 requires duration of trailing silence in seconds) to be >= this value. (float, default = 0)\n",
            "  --nemo-normalize            : See https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/asr/parts/preprocessing/features.py#L59Current supported value: per_feature or leave it to empty (unset) (string, default = \"\")\n",
            "  --nn-model                  : Path to the torchscript model (string, default = \"\")\n",
            "  --encoder-model             : Path to the encoder model for OnlineLstmTransducerModel. (string, default = \"\")\n",
            "  --decoder-model             : Path to the decoder model for OnlineLstmTransducerModel. (string, default = \"\")\n",
            "\n",
            "Standard options:\n",
            "  --help                      : Print out usage message (bool, default = false)\n",
            "  --config                    : Configuration file to read (this option may be repeated) (string, default = \"\")\n",
            "  --print-args                : Print the command line arguments (to stderr) (bool, default = true)\n",
            "\n",
            "\n",
            "[I] /var/www/sherpa/csrc/parse-options.cc:536:void sherpa::ParseOptions::PrintUsage(bool) const 2023-08-09 02:30:13.268 \n",
            "\n",
            "Automatic speech recognition with sherpa using websocket.\n",
            "\n",
            "Usage:\n",
            "\n",
            "sherpa-offline-websocket-server --help\n",
            "\n",
            "sherpa-offline-websocket-server \\\n",
            "  --use-gpu=false \\\n",
            "  --port=6006 \\\n",
            "  --num-io-threads=3 \\\n",
            "  --num-work-threads=5 \\\n",
            "  --max-batch-size=5 \\\n",
            "  --nn-model=/path/to/cpu.jit \\\n",
            "  --tokens=/path/to/tokens.txt \\\n",
            "  --decoding-method=greedy_search \\\n",
            "  --max-utterance-length=300 \\\n",
            "  --doc-root=../sherpa/bin/web \\\n",
            "  --log-file=./log.txt\n",
            "\n",
            "Options:\n",
            "  --max-batch-size            : Max batch size for decoding. If you are using CPU, increasing it will increase the memory usage if there are many active connections. We suggest that you use a small value for it for CPU decoding, e.g., 5, since it is pretty fast for CPU decoding (int, default = 5)\n",
            "  --context-score             : The bonus score for each token in context word/phrase. Used only when decoding_method is modified_beam_search (float, default = 1.5)\n",
            "  --num-active-paths          : Number of active paths for modified_beam_search. Used only when --decoding-method is modified_beam_search (int, default = 4)\n",
            "  --lm-scale                  : Used only for decoding with an HLG graph. It specifies the scale for HLG.scores (float, default = 1)\n",
            "  --log-file                  : Path to the log file. Logs are appended to this file (string, default = \"./log.txt\")\n",
            "  --use-gpu                   : true to use GPU for computation. false to use CPU.\n",
            "If true, it uses the first device. You can use the environment variable CUDA_VISIBLE_DEVICES to select which device to use. (bool, default = false)\n",
            "  --search-beam               : Used only for CTC decoding. Decoding beam, e.g. 20.  Smaller is faster, larger is more exact (less pruning). This is the default value; it may be modified by `min_active_states` and `max_active_states`.  (float, default = 20)\n",
            "  --min-active-states         : Minimum number of FSA states that are allowed to be active on any given frame for any given intersection/composition task. This is advisory, in that it will try not to have fewer than this number active. Set it to zero if there is no constraint.  (int, default = 30)\n",
            "  --max-active-states         : max_activate_states  Maximum number of FSA states that are allowed to be active on any given frame for any given intersection/composition task. This is advisory, in that it will try not to exceed that but may not always succeed. You can use a very large number if no constraint is needed.  (int, default = 10000)\n",
            "  --doc-root                  : Path to the directory where files like index.html for the HTTP server locate (string, default = \"../sherpa/bin/web\")\n",
            "  --port                      : The port on which the server will listen. (int, default = 6006)\n",
            "  --hlg                       : Used only for decoding with an HLG graph.  (string, default = \"\")\n",
            "  --num-work-threads          : Number of threads to use for neural network computation and decoding. (int, default = 5)\n",
            "  --tokens                    : Path to tokens.txt. (string, default = \"\")\n",
            "  --decoding-method           : Decoding method to use. Possible values are: greedy_search, modified_beam_search, and fast_beam_search (string, default = \"greedy_search\")\n",
            "  --max-utterance-length      : Max utterance length in seconds. If we receive an utterance longer than this value, we will reject the connection. If you have enough memory, you can select a large value for it. (float, default = 300)\n",
            "  --beam                      : Beam used in fast_beam_search (float, default = 20)\n",
            "  --num-io-threads            : Number of threads to use for network connections. (int, default = 3)\n",
            "  --dither                    : Dithering constant (0.0 means no dither). Caution: Samples are normalized to the range [-1, 1). Please select a small value for dither if you want to enable it (float, default = 0)\n",
            "  --output-beam               : Used only for CTC decoding. Beam to prune output, similar to lattice-beam in Kaldi. Relative to the best path of output.  (float, default = 8)\n",
            "  --frame-length              : Frame length in milliseconds (float, default = 25)\n",
            "  --ngram-lm-scale            : Scale the scores from LG.pt. Used only for fast_beam_search in transducer decoding (float, default = 0.01)\n",
            "  --sample-frequency          : Waveform data sample frequency (must match the waveform file, if specified there) (float, default = 16000)\n",
            "  --frame-shift               : Frame shift in milliseconds (float, default = 10)\n",
            "  --lg                        : Path to LG.pt. Used only for fast_beam_search in transducer decoding (string, default = \"\")\n",
            "  --num-mel-bins              : Number of triangular mel-frequency bins (int, default = 80)\n",
            "  --modified                  : Used only for decoding with a CTC topology. true to use a modified CTC topology; useful when vocab_size is large, e.g., > 1000. false to use a standard CTC topology. (bool, default = true)\n",
            "  --normalize-samples         : true to use samples in the range [-1, 1]. false to use samples in the range [-32768, 32767]. Note: kaldi uses un-normalized samples. (bool, default = true)\n",
            "  --nemo-normalize            : See https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/asr/parts/preprocessing/features.py#L59Current supported value: per_feature or leave it to empty (unset) (string, default = \"\")\n",
            "  --nn-model                  : Path to the torchscript model (string, default = \"\")\n",
            "\n",
            "Standard options:\n",
            "  --print-args                : Print the command line arguments (to stderr) (bool, default = true)\n",
            "  --help                      : Print out usage message (bool, default = false)\n",
            "  --config                    : Configuration file to read (this option may be repeated) (string, default = \"\")\n",
            "\n",
            "\n",
            "[I] /var/www/sherpa/csrc/parse-options.cc:536:void sherpa::ParseOptions::PrintUsage(bool) const 2023-08-09 02:30:13.789 \n",
            "\n",
            "Automatic speech recognition with sherpa using websocket.\n",
            "\n",
            "Usage:\n",
            "\n",
            "sherpa-offline-websocket-client --help\n",
            "\n",
            "sherpa-offline-websocket-client \\\n",
            "  --server-ip=127.0.0.1 \\\n",
            "  --server-port=6006 \\\n",
            "  /path/to/foo.wav\n",
            "\n",
            "Options:\n",
            "  --server-ip                 : IP address of the websocket server (string, default = \"127.0.0.1\")\n",
            "  --num-seconds-per-message   : The number of samples per message equals to num_seconds_per_message*sample_rate (float, default = 10)\n",
            "  --server-port               : Port of the websocket server (int, default = 6006)\n",
            "\n",
            "Standard options:\n",
            "  --print-args                : Print the command line arguments (to stderr) (bool, default = true)\n",
            "  --help                      : Print out usage message (bool, default = false)\n",
            "  --config                    : Configuration file to read (this option may be repeated) (string, default = \"\")\n",
            "\n",
            "\n",
            "[I] /var/www/sherpa/csrc/parse-options.cc:536:void sherpa::ParseOptions::PrintUsage(bool) const 2023-08-09 02:30:14.294 \n",
            "\n",
            "Automatic speech recognition with sherpa using websocket.\n",
            "\n",
            "Usage:\n",
            "\n",
            "sherpa-online-websocket-server --help\n",
            "\n",
            "sherpa-online-websocket-server \\\n",
            "  --use-gpu=false \\\n",
            "  --port=6006 \\\n",
            "  --num-work-threads=5 \\\n",
            "  --nn-model=/path/to/cpu.jit \\\n",
            "  --tokens=/path/to/tokens.txt \\\n",
            "  --decoding-method=greedy_search \\\n",
            "  --log-file=./log.txt\n",
            "\n",
            "Options:\n",
            "  --log-file                  : Path to the log file. Logs are appended to this file (string, default = \"./log.txt\")\n",
            "  --max-batch-size            : Max batch size for recognition. (int, default = 5)\n",
            "  --loop-interval-ms          : It determines how often the decoder loop runs.  (int, default = 10)\n",
            "  --decode-chunk-size         : Used only for streaming Conformer, i.e, models from pruned_transducer_statelessX, and streaming Zipformer, i.e, models from pruned_transducer_stateless7_streaming in icefall.Number of frames before subsampling during decoding. (int, default = 12)\n",
            "  --decode-left-context       : Used only for streaming Conformer, i.e, models from pruned_transducer_statelessX, and streaming Zipformer, i.e, models from pruned_transducer_stateless7_streaming in icefall.Number of frames before subsampling during decoding. (int, default = 64)\n",
            "  --num-active-paths          : Number of active paths for modified_beam_search. Used only when --decoding-method is modified_beam_search (int, default = 4)\n",
            "  --use-endpoint              : true to enable Endpoint, false to disable Endpoint, default is false.\n",
            " (bool, default = false)\n",
            "  --use-gpu                   : true to use GPU for computation. false to use CPU.\n",
            "If true, it uses the first device. You can use the environment variable CUDA_VISIBLE_DEVICES to select which device to use. (bool, default = false)\n",
            "  --decoding-method           : Decoding method to use. Possible values are: greedy_search, modified_beam_search, and fast_beam_search. Used only for transducer. (string, default = \"greedy_search\")\n",
            "  --tokens                    : Path to tokens.txt. (string, default = \"\")\n",
            "  --dither                    : Dithering constant (0.0 means no dither). Caution: Samples are normalized to the range [-1, 1). Please select a small value for dither if you want to enable it (float, default = 0)\n",
            "  --port                      : The port on which the server will listen. (int, default = 6006)\n",
            "  --context-score             : The bonus score for each token in context word/phrase. Used only when decoding_method is modified_beam_search (float, default = 1.5)\n",
            "  --rule2-min-utterance-length : This endpointing rule2 requires utterance-length (in seconds) to be >= this value. (float, default = 0)\n",
            "  --num-mel-bins              : Number of triangular mel-frequency bins (int, default = 80)\n",
            "  --lg                        : Path to LG.pt. Used only for fast_beam_search in transducer decoding (string, default = \"\")\n",
            "  --rule1-must-contain-nonsilence : If True, for this endpointing rule1 to apply there must be nonsilence in the best-path traceback. For decoding, a non-blank token is considered as non-silence (bool, default = false)\n",
            "  --frame-length              : Frame length in milliseconds (float, default = 25)\n",
            "  --frame-shift               : Frame shift in milliseconds (float, default = 10)\n",
            "  --num-io-threads            : Number of threads to use for network connections. (int, default = 1)\n",
            "  --decode-right-context      : Used only for streaming Conformer, i.e, models from pruned_transducer_statelessX, and streaming Zipformer, i.e, models from pruned_transducer_stateless7_streaming in icefall.Number of frames before subsampling during decoding. (int, default = 0)\n",
            "  --rule1-min-trailing-silence : This endpointing rule1 requires duration of trailing silence in seconds) to be >= this value. (float, default = 2.4)\n",
            "  --rule1-min-utterance-length : This endpointing rule1 requires utterance-length (in seconds) to be >= this value. (float, default = 0)\n",
            "  --rule2-must-contain-nonsilence : If True, for this endpointing rule2 to apply there must be nonsilence in the best-path traceback. For decoding, a non-blank token is considered as non-silence (bool, default = true)\n",
            "  --rule2-min-trailing-silence : This endpointing rule2 requires duration of trailing silence in seconds) to be >= this value. (float, default = 1.2)\n",
            "  --normalize-samples         : true to use samples in the range [-1, 1]. false to use samples in the range [-32768, 32767]. Note: kaldi uses un-normalized samples. (bool, default = true)\n",
            "  --joiner-model              : Path to the joiner model for OnlineLstmTransducerModel. (string, default = \"\")\n",
            "  --beam                      : Beam used in fast_beam_search (float, default = 20)\n",
            "  --rule3-must-contain-nonsilence : If True, for this endpointing rule3 to apply there must be nonsilence in the best-path traceback. For decoding, a non-blank token is considered as non-silence (bool, default = false)\n",
            "  --doc-root                  : Path to the directory where files like index.html for the HTTP server locate. (string, default = \"../sherpa/bin/web\")\n",
            "  --rule3-min-utterance-length : This endpointing rule3 requires utterance-length (in seconds) to be >= this value. (float, default = 20)\n",
            "  --rule3-min-trailing-silence : This endpointing rule3 requires duration of trailing silence in seconds) to be >= this value. (float, default = 0)\n",
            "  --sample-frequency          : Waveform data sample frequency (must match the waveform file, if specified there) (float, default = 16000)\n",
            "  --ngram-lm-scale            : Scale the scores from LG.pt. Used only for fast_beam_search in transducer decoding (float, default = 0.01)\n",
            "  --nemo-normalize            : See https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/asr/parts/preprocessing/features.py#L59Current supported value: per_feature or leave it to empty (unset) (string, default = \"\")\n",
            "  --nn-model                  : Path to the torchscript model (string, default = \"\")\n",
            "  --num-work-threads          : Number of threads to use for neural network computation and decoding. (int, default = 5)\n",
            "  --encoder-model             : Path to the encoder model for OnlineLstmTransducerModel. (string, default = \"\")\n",
            "  --decoder-model             : Path to the decoder model for OnlineLstmTransducerModel. (string, default = \"\")\n",
            "\n",
            "Standard options:\n",
            "  --help                      : Print out usage message (bool, default = false)\n",
            "  --config                    : Configuration file to read (this option may be repeated) (string, default = \"\")\n",
            "  --print-args                : Print the command line arguments (to stderr) (bool, default = true)\n",
            "\n",
            "\n",
            "[I] /var/www/sherpa/csrc/parse-options.cc:536:void sherpa::ParseOptions::PrintUsage(bool) const 2023-08-09 02:30:14.796 \n",
            "\n",
            "Automatic speech recognition with sherpa using websocket.\n",
            "\n",
            "Usage:\n",
            "\n",
            "sherpa-online-websocket-client --help\n",
            "\n",
            "sherpa-online-websocket-client \\\n",
            "  --server-ip=127.0.0.1 \\\n",
            "  --server-port=6006 \\\n",
            "  /path/to/foo.wav\n",
            "\n",
            "Options:\n",
            "  --ctm-filename              : Name of the CTM output file (string, default = \"\")\n",
            "  --samplerate                : SampleRate of the recorded audio (expecting wav, no resampling is done) (int, default = 16000)\n",
            "  --server-ip                 : IP address of the websocket server (string, default = \"127.0.0.1\")\n",
            "  --num-seconds-per-message   : The number of samples per message equals to seconds_per_message*sample_rate (float, default = 10)\n",
            "  --server-port               : Port of the websocket server (int, default = 6006)\n",
            "\n",
            "Standard options:\n",
            "  --print-args                : Print the command line arguments (to stderr) (bool, default = true)\n",
            "  --help                      : Print out usage message (bool, default = false)\n",
            "  --config                    : Configuration file to read (this option may be repeated) (string, default = \"\")\n",
            "\n",
            "\n",
            "[I] /var/www/sherpa/csrc/parse-options.cc:536:void sherpa::ParseOptions::PrintUsage(bool) const 2023-08-09 02:30:15.301 \n",
            "\n",
            "Automatic speech recognition with sherpa using websocket.\n",
            "\n",
            "Usage:\n",
            "\n",
            "./bin/sherpa-online-websocket-client-microphone --help\n",
            "\n",
            "./bin/sherpa-online-websocket-client-microphone \\\n",
            "  --server-ip=127.0.0.1 \\\n",
            "  --server-port=6006\n",
            "\n",
            "Options:\n",
            "  --server-ip                 : IP address of the websocket server (string, default = \"127.0.0.1\")\n",
            "  --server-port               : Port of the websocket server (int, default = 6006)\n",
            "\n",
            "Standard options:\n",
            "  --print-args                : Print the command line arguments (to stderr) (bool, default = true)\n",
            "  --help                      : Print out usage message (bool, default = false)\n",
            "  --config                    : Configuration file to read (this option may be repeated) (string, default = \"\")\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using CTC model as offline recognizer"
      ],
      "metadata": {
        "id": "90kLW8a-YDBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! sudo apt-get install -y git-lfs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6YpmioaavX-",
        "outputId": "e5c004c9-7ca0-4f1e-c31c-d2dc26cd0351"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (3.0.2-1ubuntu0.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 16 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Icefall"
      ],
      "metadata": {
        "id": "Gcun-4WYf_G7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This model is trained using GigaSpeech\n",
        "! GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/wgb14/icefall-asr-gigaspeech-conformer-ctc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuYwe6ade50H",
        "outputId": "89aaa913-6faa-4a2a-e846-5fa958a326cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'icefall-asr-gigaspeech-conformer-ctc'...\n",
            "remote: Enumerating objects: 36, done.\u001b[K\n",
            "remote: Total 36 (delta 0), reused 0 (delta 0), pack-reused 36\u001b[K\n",
            "Unpacking objects: 100% (36/36), 1.09 MiB | 7.86 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cd icefall-asr-gigaspeech-conformer-ctc && \\\n",
        "  git lfs pull --include \"exp/cpu_jit.pt\" && \\\n",
        "  git lfs pull --include \"data/lang_bpe_500/tokens.txt\" && \\\n",
        "  git lfs pull --include \"data/lang_bpe_500/HLG.pt\""
      ],
      "metadata": {
        "id": "Uui1ByZVgWHt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cb62e6c-43f3-44bc-e35a-e3fd8b31d98c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cd icefall-asr-gigaspeech-conformer-ctc && \\\n",
        "  mkdir test_wavs && \\\n",
        "  cd test_wavs && \\\n",
        "  wget https://huggingface.co/csukuangfj/wav2vec2.0-torchaudio/resolve/main/test_wavs/1089-134686-0001.wav && \\\n",
        "  wget https://huggingface.co/csukuangfj/wav2vec2.0-torchaudio/resolve/main/test_wavs/1221-135766-0001.wav && \\\n",
        "  wget https://huggingface.co/csukuangfj/wav2vec2.0-torchaudio/resolve/main/test_wavs/1221-135766-0002.wav"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55qGBTOPaxFJ",
        "outputId": "36ad5a24-9704-4e00-f590-143d045f4ba1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-09 02:32:21--  https://huggingface.co/csukuangfj/wav2vec2.0-torchaudio/resolve/main/test_wavs/1089-134686-0001.wav\n",
            "Resolving huggingface.co (huggingface.co)... 65.8.49.24, 65.8.49.53, 65.8.49.2, ...\n",
            "Connecting to huggingface.co (huggingface.co)|65.8.49.24|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 212044 (207K) [audio/wave]\n",
            "Saving to: ‘1089-134686-0001.wav’\n",
            "\n",
            "\r1089-134686-0001.wa   0%[                    ]       0  --.-KB/s               \r1089-134686-0001.wa 100%[===================>] 207.07K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-08-09 02:32:21 (3.94 MB/s) - ‘1089-134686-0001.wav’ saved [212044/212044]\n",
            "\n",
            "--2023-08-09 02:32:21--  https://huggingface.co/csukuangfj/wav2vec2.0-torchaudio/resolve/main/test_wavs/1221-135766-0001.wav\n",
            "Resolving huggingface.co (huggingface.co)... 65.8.49.24, 65.8.49.53, 65.8.49.2, ...\n",
            "Connecting to huggingface.co (huggingface.co)|65.8.49.24|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 534924 (522K) [audio/wave]\n",
            "Saving to: ‘1221-135766-0001.wav’\n",
            "\n",
            "1221-135766-0001.wa 100%[===================>] 522.39K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-08-09 02:32:22 (9.92 MB/s) - ‘1221-135766-0001.wav’ saved [534924/534924]\n",
            "\n",
            "--2023-08-09 02:32:22--  https://huggingface.co/csukuangfj/wav2vec2.0-torchaudio/resolve/main/test_wavs/1221-135766-0002.wav\n",
            "Resolving huggingface.co (huggingface.co)... 65.8.49.24, 65.8.49.53, 65.8.49.2, ...\n",
            "Connecting to huggingface.co (huggingface.co)|65.8.49.24|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 154444 (151K) [audio/wave]\n",
            "Saving to: ‘1221-135766-0002.wav’\n",
            "\n",
            "1221-135766-0002.wa 100%[===================>] 150.82K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-08-09 02:32:22 (5.91 MB/s) - ‘1221-135766-0002.wav’ saved [154444/154444]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode with H\n",
        "import os\n",
        "os.chdir(\"/content/icefall-asr-gigaspeech-conformer-ctc\")\n",
        "\n",
        "from pathlib import Path\n",
        "import sherpa\n",
        "\n",
        "nn_model = \"./exp/cpu_jit.pt\"\n",
        "tokens = \"./data/lang_bpe_500/tokens.txt\"\n",
        "wave1 = \"./test_wavs/1089-134686-0001.wav\"\n",
        "wave2 = \"./test_wavs/1221-135766-0001.wav\"\n",
        "wave3 = \"./test_wavs/1221-135766-0002.wav\"\n",
        "\n",
        "if not Path(nn_model).is_file():\n",
        "  print(\"skipping test_icefall_ctc_model()\")\n",
        "  exit\n",
        "\n",
        "print()\n",
        "print(\"test_icefall_ctc_model()\")\n",
        "\n",
        "feat_config = sherpa.FeatureConfig()\n",
        "\n",
        "feat_config.fbank_opts.frame_opts.samp_freq = 16000\n",
        "feat_config.fbank_opts.mel_opts.num_bins = 80\n",
        "feat_config.fbank_opts.frame_opts.dither = 0\n",
        "\n",
        "config = sherpa.OfflineRecognizerConfig(\n",
        "  nn_model=nn_model,\n",
        "  tokens=tokens,\n",
        "  use_gpu=False,\n",
        "  feat_config=feat_config,\n",
        ")\n",
        "\n",
        "recognizer = sherpa.OfflineRecognizer(config)\n",
        "\n",
        "s1 = recognizer.create_stream()\n",
        "s2 = recognizer.create_stream()\n",
        "s3 = recognizer.create_stream()\n",
        "\n",
        "s1.accept_wave_file(wave1)\n",
        "s2.accept_wave_file(wave2)\n",
        "s3.accept_wave_file(wave2)\n",
        "\n",
        "recognizer.decode_streams([s1, s2, s3])\n",
        "print(s1.result)\n",
        "print(s2.result)\n",
        "print(s3.result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgDKM6IhhI9x",
        "outputId": "aea84bef-cc34-4aec-c234-7c37a9850d91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "test_icefall_ctc_model()\n",
            "{\"text\":\" AFTER EARLY NIGHTFALL THE YELLOW LAMPS WOULD LIGHT UP HERE AND THERE THE SQUALID QUARTER OF THE BROTHELS\",\"timestamps\":\"[0.36,0.76,0.84,0.96,1.08,1.12,1.24,1.40,1.48,1.56,1.76,1.84,1.88,1.92,2.00,2.08,2.24,2.36,2.48,2.60,2.76,2.80,2.92,3.08,3.32,3.52,3.72,4.12,4.24,4.36,4.52,4.64,4.80,4.84,4.96,5.12,5.28,5.44,5.56,5.76,5.88,6.00]\",\"tokens\":[\" AFTER\",\" E\",\"AR\",\"LY\",\" \",\"N\",\"IGHT\",\"F\",\"AL\",\"L\",\" THE\",\" \",\"Y\",\"E\",\"LL\",\"OW\",\" LA\",\"MP\",\"S\",\" WOULD\",\" \",\"L\",\"IGHT\",\" UP\",\" HERE\",\" AND\",\" THERE\",\" THE\",\" S\",\"QU\",\"AL\",\"ID\",\" \",\"QU\",\"AR\",\"TER\",\" OF\",\" THE\",\" BRO\",\"TH\",\"EL\",\"S\"]}\n",
            "{\"text\":\" GOD AS A DIRECT CONSEQUENCE OF THE SIN WHICH MAN THUS PUNISHED HAD GIVEN HER A LOVELY CHILD WHOSE PLACE WAS ON THAT SAME DISHONOURED BOSOM TO CONNECT HER PARENT FOR EVER WITH THE RACE AND DESCENT OF MORTALS AND TO BE FINALLY A BLESED SOUL IN HEAVEN\",\"timestamps\":\"[0.32,0.48,0.72,0.88,1.00,1.08,1.16,1.24,1.40,1.56,1.60,1.72,1.88,2.08,2.24,2.36,2.52,2.68,2.92,3.20,3.32,3.48,3.60,3.72,3.84,4.28,4.48,4.68,4.80,5.00,5.16,5.40,5.60,5.76,5.80,6.24,6.40,6.48,6.60,6.96,7.16,7.40,7.64,8.00,8.24,8.32,8.36,8.44,8.52,8.64,8.76,8.88,9.12,9.32,9.44,9.52,9.60,9.72,9.92,10.04,10.12,10.64,10.80,10.96,11.12,11.28,11.44,11.60,11.80,12.00,12.08,12.20,12.28,12.56,12.76,12.84,12.96,13.04,13.16,13.60,13.80,13.92,14.08,14.24,14.40,14.60,14.72,14.80,14.88,15.04,15.28,15.48,15.72,15.88,15.96,16.04,16.12]\",\"tokens\":[\" GO\",\"D\",\" AS\",\" A\",\" DI\",\"RE\",\"C\",\"T\",\" CON\",\"S\",\"E\",\"QU\",\"ENCE\",\" OF\",\" THE\",\" S\",\"IN\",\" WHICH\",\" MAN\",\" TH\",\"US\",\" P\",\"UN\",\"ISH\",\"ED\",\" HAD\",\" GIVE\",\"N\",\" HER\",\" A\",\" LOVE\",\"LY\",\" CHI\",\"L\",\"D\",\" WHO\",\"S\",\"E\",\" PLACE\",\" WAS\",\" ON\",\" THAT\",\" SAME\",\" DIS\",\"HO\",\"N\",\"O\",\"UR\",\"ED\",\" BO\",\"S\",\"OM\",\" TO\",\" CON\",\"NE\",\"C\",\"T\",\" HER\",\" PA\",\"R\",\"ENT\",\" FOR\",\" E\",\"VER\",\" WITH\",\" THE\",\" RA\",\"CE\",\" AND\",\" DE\",\"S\",\"C\",\"ENT\",\" OF\",\" MO\",\"R\",\"T\",\"AL\",\"S\",\" AND\",\" TO\",\" BE\",\" FI\",\"N\",\"ALLY\",\" A\",\" B\",\"LE\",\"S\",\"ED\",\" SO\",\"UL\",\" IN\",\" HE\",\"A\",\"VE\",\"N\"]}\n",
            "{\"text\":\" GOD AS A DIRECT CONSEQUENCE OF THE SIN WHICH MAN THUS PUNISHED HAD GIVEN HER A LOVELY CHILD WHOSE PLACE WAS ON THAT SAME DISHONOURED BOSOM TO CONNECT HER PARENT FOR EVER WITH THE RACE AND DESCENT OF MORTALS AND TO BE FINALLY A BLESED SOUL IN HEAVEN\",\"timestamps\":\"[0.32,0.48,0.72,0.88,1.00,1.08,1.16,1.24,1.40,1.56,1.60,1.72,1.88,2.08,2.24,2.36,2.52,2.68,2.92,3.20,3.32,3.48,3.60,3.72,3.84,4.28,4.48,4.68,4.80,5.00,5.16,5.40,5.60,5.76,5.80,6.24,6.40,6.48,6.60,6.96,7.16,7.40,7.64,8.00,8.24,8.32,8.36,8.44,8.52,8.64,8.76,8.88,9.12,9.32,9.44,9.52,9.60,9.72,9.92,10.04,10.12,10.64,10.80,10.96,11.12,11.28,11.44,11.60,11.80,12.00,12.08,12.20,12.28,12.56,12.76,12.84,12.96,13.04,13.16,13.60,13.80,13.92,14.08,14.24,14.40,14.60,14.72,14.80,14.88,15.04,15.28,15.48,15.72,15.88,15.96,16.04,16.12]\",\"tokens\":[\" GO\",\"D\",\" AS\",\" A\",\" DI\",\"RE\",\"C\",\"T\",\" CON\",\"S\",\"E\",\"QU\",\"ENCE\",\" OF\",\" THE\",\" S\",\"IN\",\" WHICH\",\" MAN\",\" TH\",\"US\",\" P\",\"UN\",\"ISH\",\"ED\",\" HAD\",\" GIVE\",\"N\",\" HER\",\" A\",\" LOVE\",\"LY\",\" CHI\",\"L\",\"D\",\" WHO\",\"S\",\"E\",\" PLACE\",\" WAS\",\" ON\",\" THAT\",\" SAME\",\" DIS\",\"HO\",\"N\",\"O\",\"UR\",\"ED\",\" BO\",\"S\",\"OM\",\" TO\",\" CON\",\"NE\",\"C\",\"T\",\" HER\",\" PA\",\"R\",\"ENT\",\" FOR\",\" E\",\"VER\",\" WITH\",\" THE\",\" RA\",\"CE\",\" AND\",\" DE\",\"S\",\"C\",\"ENT\",\" OF\",\" MO\",\"R\",\"T\",\"AL\",\"S\",\" AND\",\" TO\",\" BE\",\" FI\",\"N\",\"ALLY\",\" A\",\" B\",\"LE\",\"S\",\"ED\",\" SO\",\"UL\",\" IN\",\" HE\",\"A\",\"VE\",\"N\"]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode with HLG\n",
        "nn_model = \"./exp/cpu_jit.pt\"\n",
        "tokens = \"./data/lang_bpe_500/tokens.txt\"\n",
        "hlg = \"./data/lang_bpe_500/HLG.pt\"\n",
        "wave1 = \"./test_wavs/1089-134686-0001.wav\"\n",
        "wave2 = \"./test_wavs/1221-135766-0001.wav\"\n",
        "wave3 = \"./test_wavs/1221-135766-0002.wav\"\n",
        "\n",
        "if not Path(nn_model).is_file():\n",
        "  print(\"skipping test_icefall_ctc_model_hlg_decoding()\")\n",
        "  exit\n",
        "\n",
        "print()\n",
        "print(\"test_icefall_ctc_model_hlg_decoding()\")\n",
        "\n",
        "feat_config = sherpa.FeatureConfig()\n",
        "\n",
        "feat_config.fbank_opts.frame_opts.samp_freq = 16000\n",
        "feat_config.fbank_opts.mel_opts.num_bins = 80\n",
        "feat_config.fbank_opts.frame_opts.dither = 0\n",
        "\n",
        "ctc_decoder_config = sherpa.OfflineCtcDecoderConfig(hlg=hlg)\n",
        "\n",
        "config = sherpa.OfflineRecognizerConfig(\n",
        "  nn_model=nn_model,\n",
        "  tokens=tokens,\n",
        "  feat_config=feat_config,\n",
        "  ctc_decoder_config=ctc_decoder_config,\n",
        ")\n",
        "\n",
        "recognizer = sherpa.OfflineRecognizer(config)\n",
        "\n",
        "s1 = recognizer.create_stream()\n",
        "s2 = recognizer.create_stream()\n",
        "s3 = recognizer.create_stream()\n",
        "\n",
        "s1.accept_wave_file(wave1)\n",
        "s2.accept_wave_file(wave2)\n",
        "s3.accept_wave_file(wave2)\n",
        "\n",
        "recognizer.decode_streams([s1, s2, s3])\n",
        "print(s1.result)\n",
        "print(s2.result)\n",
        "print(s3.result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pcoRy9chSSn",
        "outputId": "fa64900f-4d9e-4eb2-8df2-4916683e6dc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "test_icefall_ctc_model_hlg_decoding()\n",
            "{\"text\":\" AFTER EARLY NIGHTFALL THE YELLOW LAMPS WOULD LIGHT UP HERE AND THERE THE SQUALID QUARTER OF THE BROTHELS\",\"timestamps\":\"[0.36,0.76,0.84,0.96,1.08,1.12,1.24,1.40,1.48,1.56,1.76,1.84,1.88,1.92,2.00,2.08,2.24,2.36,2.48,2.60,2.76,2.80,2.92,3.08,3.32,3.52,3.72,4.12,4.24,4.36,4.52,4.64,4.80,4.84,4.96,5.12,5.28,5.44,5.56,5.76,5.88,6.00]\",\"tokens\":[\" AFTER\",\" E\",\"AR\",\"LY\",\" \",\"N\",\"IGHT\",\"F\",\"AL\",\"L\",\" THE\",\" \",\"Y\",\"E\",\"LL\",\"OW\",\" LA\",\"MP\",\"S\",\" WOULD\",\" \",\"L\",\"IGHT\",\" UP\",\" HERE\",\" AND\",\" THERE\",\" THE\",\" S\",\"QU\",\"AL\",\"ID\",\" \",\"QU\",\"AR\",\"TER\",\" OF\",\" THE\",\" BRO\",\"TH\",\"EL\",\"S\"]}\n",
            "{\"text\":\" GOD AS A DIRECT CONSEQUENCE OF THE SIN WHICH MAN THUS PUNISHED HAD GIVEN HER A LOVELY CHILD WHOSE PLACE WAS ON THAT SAME DISHONOURED BOSOM TO CONNECT HER PARENT FOR EVER WITH THE RACE AND DESCENT OF MORTALS AND TO BE FINALLY A BLESED SOUL IN HEAVEN\",\"timestamps\":\"[0.32,0.48,0.72,0.88,1.00,1.08,1.16,1.24,1.40,1.56,1.60,1.72,1.88,2.08,2.24,2.36,2.52,2.68,2.92,3.20,3.32,3.48,3.60,3.72,3.84,4.28,4.48,4.68,4.80,5.00,5.16,5.40,5.60,5.76,5.80,6.24,6.40,6.48,6.60,6.96,7.16,7.40,7.64,8.00,8.24,8.32,8.36,8.44,8.52,8.64,8.76,8.88,9.12,9.32,9.44,9.52,9.60,9.72,9.92,10.04,10.12,10.64,10.80,10.96,11.12,11.28,11.44,11.60,11.80,12.00,12.08,12.20,12.28,12.56,12.76,12.84,12.96,13.04,13.16,13.60,13.80,13.92,14.08,14.24,14.40,14.60,14.72,14.80,14.88,15.04,15.28,15.48,15.72,15.88,15.96,16.04,16.12]\",\"tokens\":[\" GO\",\"D\",\" AS\",\" A\",\" DI\",\"RE\",\"C\",\"T\",\" CON\",\"S\",\"E\",\"QU\",\"ENCE\",\" OF\",\" THE\",\" S\",\"IN\",\" WHICH\",\" MAN\",\" TH\",\"US\",\" P\",\"UN\",\"ISH\",\"ED\",\" HAD\",\" GIVE\",\"N\",\" HER\",\" A\",\" LOVE\",\"LY\",\" CHI\",\"L\",\"D\",\" WHO\",\"S\",\"E\",\" PLACE\",\" WAS\",\" ON\",\" THAT\",\" SAME\",\" DIS\",\"HO\",\"N\",\"O\",\"UR\",\"ED\",\" BO\",\"S\",\"OM\",\" TO\",\" CON\",\"NE\",\"C\",\"T\",\" HER\",\" PA\",\"R\",\"ENT\",\" FOR\",\" E\",\"VER\",\" WITH\",\" THE\",\" RA\",\"CE\",\" AND\",\" DE\",\"S\",\"C\",\"ENT\",\" OF\",\" MO\",\"R\",\"T\",\"AL\",\"S\",\" AND\",\" TO\",\" BE\",\" FI\",\"N\",\"ALLY\",\" A\",\" B\",\"LE\",\"S\",\"ED\",\" SO\",\"UL\",\" IN\",\" HE\",\"A\",\"VE\",\"N\"]}\n",
            "{\"text\":\" GOD AS A DIRECT CONSEQUENCE OF THE SIN WHICH MAN THUS PUNISHED HAD GIVEN HER A LOVELY CHILD WHOSE PLACE WAS ON THAT SAME DISHONOURED BOSOM TO CONNECT HER PARENT FOR EVER WITH THE RACE AND DESCENT OF MORTALS AND TO BE FINALLY A BLESED SOUL IN HEAVEN\",\"timestamps\":\"[0.32,0.48,0.72,0.88,1.00,1.08,1.16,1.24,1.40,1.56,1.60,1.72,1.88,2.08,2.24,2.36,2.52,2.68,2.92,3.20,3.32,3.48,3.60,3.72,3.84,4.28,4.48,4.68,4.80,5.00,5.16,5.40,5.60,5.76,5.80,6.24,6.40,6.48,6.60,6.96,7.16,7.40,7.64,8.00,8.24,8.32,8.36,8.44,8.52,8.64,8.76,8.88,9.12,9.32,9.44,9.52,9.60,9.72,9.92,10.04,10.12,10.64,10.80,10.96,11.12,11.28,11.44,11.60,11.80,12.00,12.08,12.20,12.28,12.56,12.76,12.84,12.96,13.04,13.16,13.60,13.80,13.92,14.08,14.24,14.40,14.60,14.72,14.80,14.88,15.04,15.28,15.48,15.72,15.88,15.96,16.04,16.12]\",\"tokens\":[\" GO\",\"D\",\" AS\",\" A\",\" DI\",\"RE\",\"C\",\"T\",\" CON\",\"S\",\"E\",\"QU\",\"ENCE\",\" OF\",\" THE\",\" S\",\"IN\",\" WHICH\",\" MAN\",\" TH\",\"US\",\" P\",\"UN\",\"ISH\",\"ED\",\" HAD\",\" GIVE\",\"N\",\" HER\",\" A\",\" LOVE\",\"LY\",\" CHI\",\"L\",\"D\",\" WHO\",\"S\",\"E\",\" PLACE\",\" WAS\",\" ON\",\" THAT\",\" SAME\",\" DIS\",\"HO\",\"N\",\"O\",\"UR\",\"ED\",\" BO\",\"S\",\"OM\",\" TO\",\" CON\",\"NE\",\"C\",\"T\",\" HER\",\" PA\",\"R\",\"ENT\",\" FOR\",\" E\",\"VER\",\" WITH\",\" THE\",\" RA\",\"CE\",\" AND\",\" DE\",\"S\",\"C\",\"ENT\",\" OF\",\" MO\",\"R\",\"T\",\"AL\",\"S\",\" AND\",\" TO\",\" BE\",\" FI\",\"N\",\"ALLY\",\" A\",\" B\",\"LE\",\"S\",\"ED\",\" SO\",\"UL\",\" IN\",\" HE\",\"A\",\"VE\",\"N\"]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### models from the wenet framework"
      ],
      "metadata": {
        "id": "DN0l7G0xol9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### English"
      ],
      "metadata": {
        "id": "5Z1xYJgppJsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! cd /content && \\\n",
        "  GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/csukuangfj/wenet-english-model && \\\n",
        "  cd wenet-english-model  && \\\n",
        "  git lfs pull --include \"final.zip\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6hx45obpHMD",
        "outputId": "117917f9-c398-4a87-97da-dad66f43e681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'wenet-english-model'...\n",
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "Unpacking objects:   5% (1/19)\rUnpacking objects:  10% (2/19)\rUnpacking objects:  15% (3/19)\rUnpacking objects:  21% (4/19)\rUnpacking objects:  26% (5/19)\rUnpacking objects:  31% (6/19)\rUnpacking objects:  36% (7/19)\rUnpacking objects:  42% (8/19)\rUnpacking objects:  47% (9/19)\rUnpacking objects:  52% (10/19)\rUnpacking objects:  57% (11/19)\rremote: Total 19 (delta 0), reused 0 (delta 0), pack-reused 19\u001b[K\n",
            "Unpacking objects: 100% (19/19), 752.53 KiB | 10.75 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/wenet-english-model\")\n",
        "\n",
        "nn_model = \"./final.zip\"\n",
        "tokens = \"./units.txt\"\n",
        "wave1 = \"./test_wavs/1089-134686-0001.wav\"\n",
        "wave2 = \"./test_wavs/1221-135766-0001.wav\"\n",
        "wave3 = \"./test_wavs/1221-135766-0002.wav\"\n",
        "\n",
        "if not Path(nn_model).is_file():\n",
        "  print(\"skipping test_wenet_ctc_model()\")\n",
        "exit\n",
        "\n",
        "print()\n",
        "print(\"------test_wenet_ctc_model()------\")\n",
        "\n",
        "# models from wenet expect un-normalized audio samples\n",
        "feat_config = sherpa.FeatureConfig(normalize_samples=False)\n",
        "\n",
        "feat_config.fbank_opts.frame_opts.samp_freq = 16000\n",
        "feat_config.fbank_opts.mel_opts.num_bins = 80\n",
        "feat_config.fbank_opts.frame_opts.dither = 0\n",
        "\n",
        "config = sherpa.OfflineRecognizerConfig(\n",
        "    nn_model=nn_model,\n",
        "    tokens=tokens,\n",
        "    use_gpu=False,\n",
        "    feat_config=feat_config,\n",
        ")\n",
        "\n",
        "recognizer = sherpa.OfflineRecognizer(config)\n",
        "\n",
        "s1 = recognizer.create_stream()\n",
        "s2 = recognizer.create_stream()\n",
        "s3 = recognizer.create_stream()\n",
        "\n",
        "s1.accept_wave_file(wave1)\n",
        "s2.accept_wave_file(wave2)\n",
        "s3.accept_wave_file(wave2)\n",
        "\n",
        "recognizer.decode_streams([s1, s2, s3])\n",
        "print(s1.result)\n",
        "print(s2.result)\n",
        "print(s3.result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ztdq4IgGpgKp",
        "outputId": "c95361b8-e2d7-4f5a-f4e8-fe61f46ebf16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------test_wenet_ctc_model()------\n",
            "{\"text\":\" AFTER EARLY NIGHTFALL THE YELLOW LAMPS WOULD LIGHT UP HERE AND THERE THE SQUALID QUARTER OF THE BROTHELS\",\"timestamps\":\"[0.60,0.96,1.38,1.62,1.74,1.98,2.10,2.40,2.64,2.82,3.00,3.24,3.48,3.72,3.90,4.32,4.50,4.62,4.74,4.86,5.10,5.46,5.58,5.76,5.94,6.12,6.18]\",\"tokens\":[\" AFTER\",\" EARLY\",\" NIGHT\",\"FA\",\"LL\",\" THE\",\" YELLOW\",\" LAMP\",\"S\",\" WOULD\",\" LIGHT\",\" UP\",\" HERE\",\" AND\",\" THERE\",\" THE\",\" S\",\"QUA\",\"LI\",\"D\",\" QUARTER\",\" OF\",\" THE\",\" BRO\",\"T\",\"HEL\",\"S\"]}\n",
            "{\"text\":\" GOD AS A DIRECT CONSEQUENCE OF THE SIN WHICH MAN THUS PUNISHED HAD GIVEN HER A LOVELY CHILD WHOSE PLACE WAS ON THAT SAME DISHONORED BOSOM TO CONNECT HER PARENT FOR EVER WITH THE RACE AND DESCENT OF MORTALS AND TO BE FINALLY A BLESSED SOUL IN HEAVEN\",\"timestamps\":\"[0.54,0.90,1.02,1.26,1.62,2.28,2.40,2.64,2.88,3.12,3.42,3.72,4.02,4.44,4.68,4.98,5.22,5.40,5.88,6.48,6.84,7.14,7.38,7.62,7.86,8.22,8.46,8.58,8.70,8.82,8.88,9.00,9.12,9.36,9.66,9.90,10.20,10.80,11.04,11.28,11.46,11.70,12.00,12.18,12.30,12.48,12.72,12.96,13.32,13.80,13.98,14.10,14.34,14.82,15.00,15.24,15.54,15.90,16.08]\",\"tokens\":[\" GOD\",\" AS\",\" A\",\" DIRECT\",\" CONSEQUENCE\",\" OF\",\" THE\",\" SIN\",\" WHICH\",\" MAN\",\" THUS\",\" PUNISH\",\"ED\",\" HAD\",\" GIVEN\",\" HER\",\" A\",\" LOVELY\",\" CHILD\",\" WHOSE\",\" PLACE\",\" WAS\",\" ON\",\" THAT\",\" SAME\",\" DISH\",\"ON\",\"OR\",\"ED\",\" BO\",\"S\",\"O\",\"M\",\" TO\",\" CONNECT\",\" HER\",\" PARENT\",\" FOR\",\" EVER\",\" WITH\",\" THE\",\" RACE\",\" AND\",\" DE\",\"S\",\"CENT\",\" OF\",\" MORTAL\",\"S\",\" AND\",\" TO\",\" BE\",\" FINALLY\",\" A\",\" BLESS\",\"ED\",\" SOUL\",\" IN\",\" HEAVEN\"]}\n",
            "{\"text\":\" GOD AS A DIRECT CONSEQUENCE OF THE SIN WHICH MAN THUS PUNISHED HAD GIVEN HER A LOVELY CHILD WHOSE PLACE WAS ON THAT SAME DISHONORED BOSOM TO CONNECT HER PARENT FOR EVER WITH THE RACE AND DESCENT OF MORTALS AND TO BE FINALLY A BLESSED SOUL IN HEAVEN\",\"timestamps\":\"[0.54,0.90,1.02,1.26,1.62,2.28,2.40,2.64,2.88,3.12,3.42,3.72,4.02,4.44,4.68,4.98,5.22,5.40,5.88,6.48,6.84,7.14,7.38,7.62,7.86,8.22,8.46,8.58,8.70,8.82,8.88,9.00,9.12,9.36,9.66,9.90,10.20,10.80,11.04,11.28,11.46,11.70,12.00,12.18,12.30,12.48,12.72,12.96,13.32,13.80,13.98,14.10,14.34,14.82,15.00,15.24,15.54,15.90,16.08]\",\"tokens\":[\" GOD\",\" AS\",\" A\",\" DIRECT\",\" CONSEQUENCE\",\" OF\",\" THE\",\" SIN\",\" WHICH\",\" MAN\",\" THUS\",\" PUNISH\",\"ED\",\" HAD\",\" GIVEN\",\" HER\",\" A\",\" LOVELY\",\" CHILD\",\" WHOSE\",\" PLACE\",\" WAS\",\" ON\",\" THAT\",\" SAME\",\" DISH\",\"ON\",\"OR\",\"ED\",\" BO\",\"S\",\"O\",\"M\",\" TO\",\" CONNECT\",\" HER\",\" PARENT\",\" FOR\",\" EVER\",\" WITH\",\" THE\",\" RACE\",\" AND\",\" DE\",\"S\",\"CENT\",\" OF\",\" MORTAL\",\"S\",\" AND\",\" TO\",\" BE\",\" FINALLY\",\" A\",\" BLESS\",\"ED\",\" SOUL\",\" IN\",\" HEAVEN\"]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chinese"
      ],
      "metadata": {
        "id": "WaiXuoNVpOw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! cd /content && \\\n",
        "  GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/csukuangfj/wenet-chinese-model && \\\n",
        "  cd wenet-chinese-model && \\\n",
        "  git lfs pull --include \"final.zip\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1_UxhkMprFi",
        "outputId": "225c3748-114d-40ff-9b0f-0e976b18dc0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'wenet-chinese-model'...\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Counting objects:   6% (1/16)\u001b[K\rremote: Counting objects:  12% (2/16)\u001b[K\rremote: Counting objects:  18% (3/16)\u001b[K\rremote: Counting objects:  25% (4/16)\u001b[K\rremote: Counting objects:  31% (5/16)\u001b[K\rremote: Counting objects:  37% (6/16)\u001b[K\rremote: Counting objects:  43% (7/16)\u001b[K\rremote: Counting objects:  50% (8/16)\u001b[K\rremote: Counting objects:  56% (9/16)\u001b[K\rremote: Counting objects:  62% (10/16)\u001b[K\rremote: Counting objects:  68% (11/16)\u001b[K\rremote: Counting objects:  75% (12/16)\u001b[K\rremote: Counting objects:  81% (13/16)\u001b[K\rremote: Counting objects:  87% (14/16)\u001b[K\rremote: Counting objects:  93% (15/16)\u001b[K\rremote: Counting objects: 100% (16/16)\u001b[K\rremote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 16 (delta 0), reused 16 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (16/16), 697.35 KiB | 10.25 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/wenet-chinese-model\")\n",
        "\n",
        "nn_model = \"./final.zip\"\n",
        "tokens = \"./units.txt\"\n",
        "wave1 = \"./test_wavs/BAC009S0764W0121.wav\"\n",
        "wave2 = \"./test_wavs/BAC009S0764W0122.wav\"\n",
        "wave3 = \"./test_wavs/BAC009S0764W0123.wav\"\n",
        "wave4 = \"./test_wavs/DEV_T0000000000.wav\"\n",
        "wave5 = \"./test_wavs/DEV_T0000000001.wav\"\n",
        "wave6 = \"./test_wavs/DEV_T0000000002.wav\"\n",
        "\n",
        "if not Path(nn_model).is_file():\n",
        "  print(\"skipping test_wenet_ctc_model()\")\n",
        "exit\n",
        "\n",
        "print()\n",
        "print(\"------test_wenet_ctc_model()------\")\n",
        "\n",
        "# models from wenet expect un-normalized audio samples\n",
        "feat_config = sherpa.FeatureConfig(normalize_samples=False)\n",
        "\n",
        "feat_config.fbank_opts.frame_opts.samp_freq = 16000\n",
        "feat_config.fbank_opts.mel_opts.num_bins = 80\n",
        "feat_config.fbank_opts.frame_opts.dither = 0\n",
        "\n",
        "config = sherpa.OfflineRecognizerConfig(\n",
        "    nn_model=nn_model,\n",
        "    tokens=tokens,\n",
        "    use_gpu=False,\n",
        "    feat_config=feat_config,\n",
        ")\n",
        "\n",
        "recognizer = sherpa.OfflineRecognizer(config)\n",
        "\n",
        "s1 = recognizer.create_stream()\n",
        "s2 = recognizer.create_stream()\n",
        "s3 = recognizer.create_stream()\n",
        "s4 = recognizer.create_stream()\n",
        "s5 = recognizer.create_stream()\n",
        "s6 = recognizer.create_stream()\n",
        "\n",
        "s1.accept_wave_file(wave1)\n",
        "s2.accept_wave_file(wave2)\n",
        "s3.accept_wave_file(wave3)\n",
        "s4.accept_wave_file(wave4)\n",
        "s5.accept_wave_file(wave5)\n",
        "s6.accept_wave_file(wave6)\n",
        "\n",
        "recognizer.decode_streams([s1, s2, s3, s4, s5, s6])\n",
        "print(s1.result)\n",
        "print(s2.result)\n",
        "print(s3.result)\n",
        "print(s4.result)\n",
        "print(s5.result)\n",
        "print(s6.result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aO3Gxf8lpxT2",
        "outputId": "a5afdfe3-1739-47c6-942e-e2a233c27daa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------test_wenet_ctc_model()------\n",
            "{\"text\":\"甚至出现交易几乎停滞的情况\",\"timestamps\":\"[0.76,1.00,1.24,1.48,1.80,2.04,2.28,2.48,2.72,2.92,3.12,3.36,3.56]\",\"tokens\":[\"甚\",\"至\",\"出\",\"现\",\"交\",\"易\",\"几\",\"乎\",\"停\",\"滞\",\"的\",\"情\",\"况\"]}\n",
            "{\"text\":\"一二线城市虽然也处于调整中\",\"timestamps\":\"[0.72,0.96,1.20,1.56,1.76,1.96,2.20,2.48,2.68,2.84,3.08,3.28,3.52]\",\"tokens\":[\"一\",\"二\",\"线\",\"城\",\"市\",\"虽\",\"然\",\"也\",\"处\",\"于\",\"调\",\"整\",\"中\"]}\n",
            "{\"text\":\"但因为聚集了过多公共资源\",\"timestamps\":\"[0.76,1.00,1.24,1.52,1.72,1.96,2.28,2.52,2.76,2.96,3.24,3.40]\",\"tokens\":[\"但\",\"因\",\"为\",\"聚\",\"集\",\"了\",\"过\",\"多\",\"公\",\"共\",\"资\",\"源\"]}\n",
            "{\"text\":\"对我做了介绍那么我想说的是大家如果对我的研究感兴趣呢\",\"timestamps\":\"[0.48,0.64,0.76,0.88,1.04,1.20,1.96,2.08,2.28,2.40,2.52,2.64,2.76,3.40,3.52,3.68,3.76,3.88,3.96,4.04,4.16,4.28,4.40,4.60,4.76,4.96]\",\"tokens\":[\"对\",\"我\",\"做\",\"了\",\"介\",\"绍\",\"那\",\"么\",\"我\",\"想\",\"说\",\"的\",\"是\",\"大\",\"家\",\"如\",\"果\",\"对\",\"我\",\"的\",\"研\",\"究\",\"感\",\"兴\",\"趣\",\"呢\"]}\n",
            "{\"text\":\"重点呢想谈三个问题首先呢就是这一轮全球金融动荡的表现啊\",\"timestamps\":\"[0.32,0.44,0.60,0.72,0.92,1.08,1.20,1.32,1.48,2.32,2.48,2.60,2.76,2.92,3.20,3.28,3.44,3.68,3.80,3.96,4.04,4.20,4.32,4.40,4.60,4.76,5.12]\",\"tokens\":[\"重\",\"点\",\"呢\",\"想\",\"谈\",\"三\",\"个\",\"问\",\"题\",\"首\",\"先\",\"呢\",\"就\",\"是\",\"这\",\"一\",\"轮\",\"全\",\"球\",\"金\",\"融\",\"动\",\"荡\",\"的\",\"表\",\"现\",\"啊\"]}\n",
            "{\"text\":\"深入地分析这一次全球金融动荡背后的根源\",\"timestamps\":\"[0.56,0.68,0.84,1.00,1.24,1.76,1.92,2.12,2.72,2.92,3.04,3.16,3.32,3.48,3.60,3.72,3.84,4.00,4.16]\",\"tokens\":[\"深\",\"入\",\"地\",\"分\",\"析\",\"这\",\"一\",\"次\",\"全\",\"球\",\"金\",\"融\",\"动\",\"荡\",\"背\",\"后\",\"的\",\"根\",\"源\"]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### torchaudio wav2vec 2.0"
      ],
      "metadata": {
        "id": "R2x0lLnrqCSx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### English"
      ],
      "metadata": {
        "id": "xA2MytRZq1Bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: There are other kinds of models fine-tuned with different\n",
        "# amount of data. We use a model that is fine-tuned with 10 minutes of data.\n",
        "! cd /content && \\\n",
        "  GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/csukuangfj/wav2vec2.0-torchaudio && \\\n",
        "  cd wav2vec2.0-torchaudio && \\\n",
        "  git lfs pull --include \"wav2vec2_asr_base_10m.pt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiVF7bwjq5oo",
        "outputId": "ecce95d1-6213-4dd8-96ee-85e2b2e2fc05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'wav2vec2.0-torchaudio'...\n",
            "remote: Enumerating objects: 46, done.\u001b[K\n",
            "remote: Total 46 (delta 0), reused 0 (delta 0), pack-reused 46\u001b[K\n",
            "Unpacking objects: 100% (46/46), 1.27 MiB | 8.26 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/wav2vec2.0-torchaudio\")\n",
        "\n",
        "nn_model = \"./wav2vec2_asr_base_10m.pt\"\n",
        "tokens = \"./tokens.txt\"\n",
        "wave1 = \"./test_wavs/1089-134686-0001.wav\"\n",
        "wave2 = \"./test_wavs/1221-135766-0001.wav\"\n",
        "wave3 = \"./test_wavs/1221-135766-0002.wav\"\n",
        "\n",
        "if not Path(nn_model).is_file():\n",
        "  print(\"skipping test_torchaudio_wav2vec2_0_ctc_model()\")\n",
        "exit\n",
        "\n",
        "print()\n",
        "print(\"test_torchaudio_wav2vec2_0_ctc_model()\")\n",
        "\n",
        "config = sherpa.OfflineRecognizerConfig(\n",
        "    nn_model=nn_model,\n",
        "    tokens=tokens,\n",
        "    use_gpu=False,\n",
        ")\n",
        "\n",
        "recognizer = sherpa.OfflineRecognizer(config)\n",
        "\n",
        "s1 = recognizer.create_stream()\n",
        "s2 = recognizer.create_stream()\n",
        "s3 = recognizer.create_stream()\n",
        "\n",
        "s1.accept_wave_file(wave1)\n",
        "s2.accept_wave_file(wave2)\n",
        "s3.accept_wave_file(wave3)\n",
        "\n",
        "recognizer.decode_streams([s1, s2, s3])\n",
        "print(s1.result)\n",
        "print(s2.result)\n",
        "print(s3.result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWfWsmFZrDbg",
        "outputId": "c973c6a8-8592-4fb8-867d-cdec725eba56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "test_torchaudio_wav2vec2_0_ctc_model()\n",
            "{\"text\":\"E\",\"timestamps\":\"[0.22]\",\"tokens\":[\"E\"]}\n",
            "{\"text\":\"GAD AS A DERECT CONSEQUENCE OF THE SIN WHICH MAN THUS PUNISHED HAD GIVEN HERE A LOVELY CHILED WHOWS PLASE WAS ON THAT SAME DIS ANERED BOSOM TO CANECT HER PARENT FOR EVER WITHE RACSE AND DESENT OF MORTLES AND TO BE FINALY ABLESED SOL IN HEVEN \",\"timestamps\":\"[0.32,0.58,0.60,0.68,0.78,0.82,0.86,0.92,0.94,0.98,1.04,1.06,1.14,1.22,1.26,1.30,1.36,1.48,1.50,1.58,1.66,1.74,1.78,1.88,1.92,1.98,2.02,2.04,2.12,2.16,2.20,2.22,2.26,2.28,2.32,2.36,2.52,2.58,2.62,2.68,2.70,2.74,2.78,2.80,2.84,2.90,3.04,3.10,3.14,3.20,3.22,3.28,3.32,3.42,3.50,3.60,3.62,3.74,3.78,3.82,3.88,3.94,3.98,4.32,4.36,4.40,4.44,4.48,4.56,4.58,4.66,4.68,4.74,4.78,4.84,4.88,4.94,4.96,5.04,5.08,5.14,5.26,5.30,5.34,5.38,5.46,5.50,5.54,5.58,5.80,5.86,5.92,5.94,5.98,6.30,6.34,6.38,6.42,6.44,6.50,6.58,6.64,6.74,6.86,6.90,6.92,6.96,7.02,7.06,7.10,7.24,7.28,7.34,7.36,7.40,7.46,7.50,7.56,7.60,7.76,7.86,7.92,7.94,8.02,8.08,8.10,8.22,8.32,8.36,8.42,8.46,8.50,8.54,8.58,8.66,8.74,8.76,8.90,8.92,9.00,9.14,9.18,9.26,9.34,9.40,9.42,9.50,9.58,9.62,9.68,9.72,9.76,9.78,9.82,9.88,10.02,10.06,10.16,10.18,10.26,10.30,10.66,10.70,10.72,10.76,10.88,10.92,10.98,11.02,11.06,11.10,11.16,11.18,11.30,11.32,11.36,11.42,11.52,11.64,11.68,11.72,11.76,11.86,11.90,11.94,11.96,11.98,12.04,12.10,12.26,12.32,12.42,12.48,12.62,12.64,12.70,12.74,12.82,12.88,12.98,13.04,13.12,13.14,13.24,13.70,13.72,13.76,13.78,13.82,13.86,13.90,13.94,13.98,14.04,14.10,14.24,14.28,14.36,14.40,14.50,14.54,14.64,14.72,14.76,14.86,14.92,15.04,15.10,15.14,15.22,15.44,15.62,15.66,15.76,15.80,15.86,15.90,16.00,16.02,16.14,16.16,16.24]\",\"tokens\":[\"G\",\"A\",\"D\",\" \",\"A\",\"S\",\" \",\"A\",\" \",\"D\",\"E\",\"R\",\"E\",\"C\",\"T\",\" \",\"C\",\"O\",\"N\",\"S\",\"E\",\"Q\",\"U\",\"E\",\"N\",\"C\",\"E\",\" \",\"O\",\"F\",\" \",\"T\",\"H\",\"E\",\" \",\"S\",\"I\",\"N\",\" \",\"W\",\"H\",\"I\",\"C\",\"H\",\" \",\"M\",\"A\",\"N\",\" \",\"T\",\"H\",\"U\",\"S\",\" \",\"P\",\"U\",\"N\",\"I\",\"S\",\"H\",\"E\",\"D\",\" \",\"H\",\"A\",\"D\",\" \",\"G\",\"I\",\"V\",\"E\",\"N\",\" \",\"H\",\"E\",\"R\",\"E\",\" \",\"A\",\" \",\"L\",\"O\",\"V\",\"E\",\"L\",\"Y\",\" \",\"C\",\"H\",\"I\",\"L\",\"E\",\"D\",\" \",\"W\",\"H\",\"O\",\"W\",\"S\",\" \",\"P\",\"L\",\"A\",\"S\",\"E\",\" \",\"W\",\"A\",\"S\",\" \",\"O\",\"N\",\" \",\"T\",\"H\",\"A\",\"T\",\" \",\"S\",\"A\",\"M\",\"E\",\" \",\"D\",\"I\",\"S\",\" \",\"A\",\"N\",\"E\",\"R\",\"E\",\"D\",\" \",\"B\",\"O\",\"S\",\"O\",\"M\",\" \",\"T\",\"O\",\" \",\"C\",\"A\",\"N\",\"E\",\"C\",\"T\",\" \",\"H\",\"E\",\"R\",\" \",\"P\",\"A\",\"R\",\"E\",\"N\",\"T\",\" \",\"F\",\"O\",\"R\",\" \",\"E\",\"V\",\"E\",\"R\",\" \",\"W\",\"I\",\"T\",\"H\",\"E\",\" \",\"R\",\"A\",\"C\",\"S\",\"E\",\" \",\"A\",\"N\",\"D\",\" \",\"D\",\"E\",\"S\",\"E\",\"N\",\"T\",\" \",\"O\",\"F\",\" \",\"M\",\"O\",\"R\",\"T\",\"L\",\"E\",\"S\",\" \",\"A\",\"N\",\"D\",\" \",\"T\",\"O\",\" \",\"B\",\"E\",\" \",\"F\",\"I\",\"N\",\"A\",\"L\",\"Y\",\" \",\"A\",\"B\",\"L\",\"E\",\"S\",\"E\",\"D\",\" \",\"S\",\"O\",\"L\",\" \",\"I\",\"N\",\" \",\"H\",\"E\",\"V\",\"E\",\"N\",\" \"]}\n",
            "{\"text\":\"AFAFAFAFAFAFAFAFAF\",\"timestamps\":\"[0.14,0.46,0.80,0.82,0.94,1.02,1.08,1.10,1.12,1.20,1.38,1.80,1.84,1.88,1.94,2.70,3.16,3.72]\",\"tokens\":[\"A\",\"F\",\"A\",\"F\",\"A\",\"F\",\"A\",\"F\",\"A\",\"F\",\"A\",\"F\",\"A\",\"F\",\"A\",\"F\",\"A\",\"F\"]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### German"
      ],
      "metadata": {
        "id": "TvgWZhbuq3x5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git lfs pull --include \"voxpopuli_asr_base_10k_de.pt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08dScsLOqAnM",
        "outputId": "82310efa-5338-4ce1-9cee-2faba70c1413"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn_model = \"./wav2vec2_asr_base_10m.pt\"\n",
        "tokens = \"./tokens.txt\"\n",
        "wave1 = \"./test_wavs/20120315-0900-PLENARY-14-de_20120315.wav\"\n",
        "wave2 = \"./test_wavs/20170517-0900-PLENARY-16-de_20170517.wav\"\n",
        "\n",
        "if not Path(nn_model).is_file():\n",
        "  print(\"skipping test_torchaudio_wav2vec2_0_ctc_model()\")\n",
        "exit\n",
        "\n",
        "print()\n",
        "print(\"test_torchaudio_wav2vec2_0_ctc_model()\")\n",
        "\n",
        "config = sherpa.OfflineRecognizerConfig(\n",
        "    nn_model=nn_model,\n",
        "    tokens=tokens,\n",
        "    use_gpu=False,\n",
        ")\n",
        "\n",
        "recognizer = sherpa.OfflineRecognizer(config)\n",
        "\n",
        "s1 = recognizer.create_stream()\n",
        "s2 = recognizer.create_stream()\n",
        "\n",
        "s1.accept_wave_file(wave1)\n",
        "s2.accept_wave_file(wave2)\n",
        "\n",
        "recognizer.decode_streams([s1, s2])\n",
        "print(s1.result)\n",
        "print(s2.result)"
      ],
      "metadata": {
        "id": "7oo6t2_LrTc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93eb46cc-75da-40f0-85a2-e24f1bc1512a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "test_torchaudio_wav2vec2_0_ctc_model()\n",
            "{\"text\":\"NADLISH GUPT ES EBWR GUFERE THIS HASCA HOWSES OR LOE SETE HITS A B ARTERS OR DESODIS ISETAIN THE VERSTAK MONDEGAN BARCIN AGLIHEINIAN STRAD IN DIGLAN RIRLRBEN \",\"timestamps\":\"[0.12,0.20,0.32,0.52,0.62,0.70,0.74,0.80,0.92,1.00,1.08,1.18,1.20,1.32,1.38,1.52,1.74,1.78,1.80,1.86,1.94,1.98,2.06,2.20,2.28,2.38,2.42,2.44,2.92,2.94,3.00,3.04,3.12,3.18,3.26,3.30,3.34,3.46,3.50,3.52,3.60,3.66,3.70,3.82,3.88,3.94,3.98,4.02,4.04,4.08,4.12,4.14,4.16,4.20,4.30,4.36,4.42,4.46,4.48,4.58,4.64,4.68,4.72,4.82,4.86,4.92,5.06,5.20,5.24,5.34,5.44,5.46,5.48,5.62,6.04,6.08,6.10,6.16,6.26,6.36,6.52,6.70,6.80,6.86,6.94,7.08,7.10,7.20,7.32,7.40,7.50,7.54,7.60,7.62,7.66,7.68,7.72,7.76,7.82,7.84,7.88,7.98,8.08,8.12,8.20,8.24,8.28,8.34,8.40,8.46,8.52,8.64,8.66,8.74,8.80,8.88,8.92,8.96,9.14,9.18,9.22,9.36,9.46,9.54,9.58,9.66,9.70,9.74,9.76,9.84,9.92,9.96,10.02,10.04,10.14,10.18,10.28,10.38,10.42,10.50,10.52,10.56,10.62,10.70,10.76,10.82,10.96,10.98,11.06,11.12,11.22,11.24,11.28,11.30,11.42,11.50,11.54,11.66]\",\"tokens\":[\"N\",\"A\",\"D\",\"L\",\"I\",\"S\",\"H\",\" \",\"G\",\"U\",\"P\",\"T\",\" \",\"E\",\"S\",\" \",\"E\",\"B\",\"W\",\"R\",\" \",\"G\",\"U\",\"F\",\"E\",\"R\",\"E\",\" \",\"T\",\"H\",\"I\",\"S\",\" \",\"H\",\"A\",\"S\",\"C\",\"A\",\" \",\"H\",\"O\",\"W\",\"S\",\"E\",\"S\",\" \",\"O\",\"R\",\" \",\"L\",\"O\",\"E\",\" \",\"S\",\"E\",\"T\",\"E\",\" \",\"H\",\"I\",\"T\",\"S\",\" \",\"A\",\" \",\"B\",\" \",\"A\",\"R\",\"T\",\"E\",\"R\",\"S\",\" \",\"O\",\"R\",\" \",\"D\",\"E\",\"S\",\"O\",\"D\",\"I\",\"S\",\" \",\"I\",\"S\",\"E\",\"T\",\"A\",\"I\",\"N\",\" \",\"T\",\"H\",\"E\",\" \",\"V\",\"E\",\"R\",\"S\",\"T\",\"A\",\"K\",\" \",\"M\",\"O\",\"N\",\"D\",\"E\",\"G\",\"A\",\"N\",\" \",\"B\",\"A\",\"R\",\"C\",\"I\",\"N\",\" \",\"A\",\"G\",\"L\",\"I\",\"H\",\"E\",\"I\",\"N\",\"I\",\"A\",\"N\",\" \",\"S\",\"T\",\"R\",\"A\",\"D\",\" \",\"I\",\"N\",\" \",\"D\",\"I\",\"G\",\"L\",\"A\",\"N\",\" \",\"R\",\"I\",\"R\",\"L\",\"R\",\"B\",\"E\",\"N\",\" \"]}\n",
            "{\"text\":\"OR TDATE GDIEASESLIALEIA VL \",\"timestamps\":\"[0.22,0.24,0.30,0.36,0.38,0.52,0.60,0.64,0.70,0.76,1.02,1.86,3.74,3.84,4.06,4.46,4.48,5.02,5.30,5.64,5.66,6.90,7.12,7.74,7.78,8.10,8.18,8.24]\",\"tokens\":[\"O\",\"R\",\" \",\"T\",\"D\",\"A\",\"T\",\"E\",\" \",\"G\",\"D\",\"I\",\"E\",\"A\",\"S\",\"E\",\"S\",\"L\",\"I\",\"A\",\"L\",\"E\",\"I\",\"A\",\" \",\"V\",\"L\",\" \"]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p8yxuXtgJ8qw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}