{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPe9QuGVn8t4/1XbIboKZDe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/k2-fsa/colab/blob/master/kaldi-native-fbank/speech_recognition_with_kaldi_native_fbank_and_whisper_and_onnxruntime.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "BmIQxtlx6o57"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This colab notebooks demonstrates how to combine [kaldi-native-fbank][kaldi-native-fbank], [Whisper][Whisper], and [onnxruntime][onnxruntime] for speech recognition.\n",
        "\n",
        "[kaldi-native-fbank]: https://github.com/csukuangfj/kaldi-native-fbank\n",
        "[Whisper]: https://github.com/openai/whisper/\n",
        "[onnxruntime]: https://github.com/microsoft/onnxruntime"
      ],
      "metadata": {
        "id": "FxNMKvj16qkm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install dependencies"
      ],
      "metadata": {
        "id": "MlNLUu8r7CHa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Cn17FNR6flP",
        "outputId": "941f5553-351d-46c4-9935-70fd89c87d18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kaldi-native-fbank\n",
            "  Downloading kaldi_native_fbank-1.18.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.5/210.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime\n",
            "  Downloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (23.5.26)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (23.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.12)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Installing collected packages: kaldi-native-fbank, humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 kaldi-native-fbank-1.18.4 onnxruntime-1.15.1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "pip install kaldi-native-fbank onnxruntime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloand Whisper onnxruntime models"
      ],
      "metadata": {
        "id": "kBPV_GD_7JKl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please see\n",
        "https://k2-fsa.github.io/sherpa/onnx/pretrained_models/whisper/index.html\n",
        "for details."
      ],
      "metadata": {
        "id": "zthQfGsp7O-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "sudo apt-get install git-lfs\n",
        "\n",
        "GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/csukuangfj/sherpa-onnx-whisper-tiny.en\n",
        "cd sherpa-onnx-whisper-tiny.en\n",
        "git lfs pull --include \"*.onnx\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vp1AbzWx7IoO",
        "outputId": "293e302d-ddf0-4038-f513-53fa0308297a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (3.0.2-1ubuntu0.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 16 not upgraded.\n",
            "Cloning into 'sherpa-onnx-whisper-tiny.en'...\n",
            "remote: Enumerating objects: 46, done.\u001b[K\n",
            "remote: Counting objects: 100% (46/46), done.\u001b[K\n",
            "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
            "remote: Total 46 (delta 5), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (46/46), 1.00 MiB | 7.38 MiB/s, done.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/csukuangfj/sherpa-onnx-whisper-tiny\n",
        "cd sherpa-onnx-whisper-tiny\n",
        "git lfs pull --include \"*.onnx\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXOu_DQj7wqt",
        "outputId": "68b5f9e9-c27b-4d42-ba86-e343120f47ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'sherpa-onnx-whisper-tiny'...\n",
            "remote: Enumerating objects: 31, done.\u001b[K\n",
            "remote: Counting objects:   3% (1/31)\u001b[K\rremote: Counting objects:   6% (2/31)\u001b[K\rremote: Counting objects:   9% (3/31)\u001b[K\rremote: Counting objects:  12% (4/31)\u001b[K\rremote: Counting objects:  16% (5/31)\u001b[K\rremote: Counting objects:  19% (6/31)\u001b[K\rremote: Counting objects:  22% (7/31)\u001b[K\rremote: Counting objects:  25% (8/31)\u001b[K\rremote: Counting objects:  29% (9/31)\u001b[K\rremote: Counting objects:  32% (10/31)\u001b[K\rremote: Counting objects:  35% (11/31)\u001b[K\rremote: Counting objects:  38% (12/31)\u001b[K\rremote: Counting objects:  41% (13/31)\u001b[K\rremote: Counting objects:  45% (14/31)\u001b[K\rremote: Counting objects:  48% (15/31)\u001b[K\rremote: Counting objects:  51% (16/31)\u001b[K\rremote: Counting objects:  54% (17/31)\u001b[K\rremote: Counting objects:  58% (18/31)\u001b[K\rremote: Counting objects:  61% (19/31)\u001b[K\rremote: Counting objects:  64% (20/31)\u001b[K\rremote: Counting objects:  67% (21/31)\u001b[K\rremote: Counting objects:  70% (22/31)\u001b[K\rremote: Counting objects:  74% (23/31)\u001b[K\rremote: Counting objects:  77% (24/31)\u001b[K\rremote: Counting objects:  80% (25/31)\u001b[K\rremote: Counting objects:  83% (26/31)\u001b[K\rremote: Counting objects:  87% (27/31)\u001b[K\rremote: Counting objects:  90% (28/31)\u001b[K\rremote: Counting objects:  93% (29/31)\u001b[K\rremote: Counting objects:  96% (30/31)\u001b[K\rremote: Counting objects: 100% (31/31)\u001b[K\rremote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 31 (delta 3), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (31/31), 387.00 KiB | 3.25 MiB/s, done.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute features with kaldi-native-fbank"
      ],
      "metadata": {
        "id": "cDfL0_rN7erk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import kaldi_native_fbank as knf\n",
        "\n",
        "def compute_features(filename: str) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      filename:\n",
        "        Path to an audio file.\n",
        "    Returns:\n",
        "      Return a 1-D float32 tensor of shape (1, 80, 3000) containing the features.\n",
        "    \"\"\"\n",
        "    wave, sample_rate = torchaudio.load(filename)\n",
        "    audio = wave[0].contiguous()  # only use the first channel\n",
        "    if sample_rate != 16000:\n",
        "        audio = torchaudio.functional.resample(\n",
        "            audio, orig_freq=sample_rate, new_freq=16000\n",
        "        )\n",
        "\n",
        "    features = []\n",
        "    online_whisper_fbank = knf.OnlineWhisperFbank(knf.FrameExtractionOptions())\n",
        "    online_whisper_fbank.accept_waveform(16000, audio.numpy())\n",
        "    online_whisper_fbank.input_finished()\n",
        "    for i in range(online_whisper_fbank.num_frames_ready):\n",
        "        f = online_whisper_fbank.get_frame(i)\n",
        "        f = torch.from_numpy(f)\n",
        "        features.append(f)\n",
        "\n",
        "    features = torch.stack(features)\n",
        "\n",
        "    log_spec = torch.clamp(features, min=1e-10).log10()\n",
        "    log_spec = torch.maximum(log_spec, log_spec.max() - 8.0)\n",
        "    mel = (log_spec + 4.0) / 4.0\n",
        "    target = 3000\n",
        "    mel = torch.nn.functional.pad(mel, (0, 0, 0, target - mel.shape[0]), \"constant\", 0)\n",
        "    return mel.t().unsqueeze(0)"
      ],
      "metadata": {
        "id": "uZONzvwg7co2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Whisper onnxruntime model"
      ],
      "metadata": {
        "id": "MlW90Sdr8Eqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime as ort\n",
        "from typing import Tuple\n",
        "\n",
        "\n",
        "class OnnxModel:\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder: str,\n",
        "        decoder: str,\n",
        "    ):\n",
        "        session_opts = ort.SessionOptions()\n",
        "        session_opts.inter_op_num_threads = 1\n",
        "        session_opts.intra_op_num_threads = 2\n",
        "\n",
        "        self.session_opts = session_opts\n",
        "\n",
        "        self.init_encoder(encoder)\n",
        "        self.init_decoder(decoder)\n",
        "\n",
        "    def init_encoder(self, encoder: str):\n",
        "        self.encoder = ort.InferenceSession(\n",
        "            encoder,\n",
        "            sess_options=self.session_opts,\n",
        "        )\n",
        "\n",
        "        meta = self.encoder.get_modelmeta().custom_metadata_map\n",
        "        self.n_text_layer = int(meta[\"n_text_layer\"])\n",
        "        self.n_text_ctx = int(meta[\"n_text_ctx\"])\n",
        "        self.n_text_state = int(meta[\"n_text_state\"])\n",
        "        self.sot = int(meta[\"sot\"])\n",
        "        self.eot = int(meta[\"eot\"])\n",
        "        self.translate = int(meta[\"translate\"])\n",
        "        self.transcribe = int(meta[\"transcribe\"])\n",
        "        self.no_timestamps = int(meta[\"no_timestamps\"])\n",
        "        self.no_speech = int(meta[\"no_speech\"])\n",
        "        self.blank = int(meta[\"blank_id\"])\n",
        "\n",
        "        self.sot_sequence = list(map(int, meta[\"sot_sequence\"].split(\",\")))\n",
        "\n",
        "        self.sot_sequence.append(self.no_timestamps)\n",
        "\n",
        "        self.all_language_tokens = list(\n",
        "            map(int, meta[\"all_language_tokens\"].split(\",\"))\n",
        "        )\n",
        "        self.all_language_codes = meta[\"all_language_codes\"].split(\",\")\n",
        "        self.lang2id = dict(zip(self.all_language_codes, self.all_language_tokens))\n",
        "        self.id2lang = dict(zip(self.all_language_tokens, self.all_language_codes))\n",
        "\n",
        "        self.is_multilingual = int(meta[\"is_multilingual\"]) == 1\n",
        "\n",
        "    def init_decoder(self, decoder: str):\n",
        "        self.decoder = ort.InferenceSession(\n",
        "            decoder,\n",
        "            sess_options=self.session_opts,\n",
        "        )\n",
        "\n",
        "    def run_encoder(\n",
        "        self,\n",
        "        mel: torch.Tensor,\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        n_layer_cross_k, n_layer_cross_v = self.encoder.run(\n",
        "            [\n",
        "                self.encoder.get_outputs()[0].name,\n",
        "                self.encoder.get_outputs()[1].name,\n",
        "            ],\n",
        "            {\n",
        "                self.encoder.get_inputs()[0].name: mel.numpy(),\n",
        "            },\n",
        "        )\n",
        "        return torch.from_numpy(n_layer_cross_k), torch.from_numpy(n_layer_cross_v)\n",
        "\n",
        "    def run_decoder(\n",
        "        self,\n",
        "        tokens: torch.Tensor,\n",
        "        n_layer_self_k_cache: torch.Tensor,\n",
        "        n_layer_self_v_cache: torch.Tensor,\n",
        "        n_layer_cross_k: torch.Tensor,\n",
        "        n_layer_cross_v: torch.Tensor,\n",
        "        offset: torch.Tensor,\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        logits, out_n_layer_self_k_cache, out_n_layer_self_v_cache = self.decoder.run(\n",
        "            [\n",
        "                self.decoder.get_outputs()[0].name,\n",
        "                self.decoder.get_outputs()[1].name,\n",
        "                self.decoder.get_outputs()[2].name,\n",
        "            ],\n",
        "            {\n",
        "                self.decoder.get_inputs()[0].name: tokens.numpy(),\n",
        "                self.decoder.get_inputs()[1].name: n_layer_self_k_cache.numpy(),\n",
        "                self.decoder.get_inputs()[2].name: n_layer_self_v_cache.numpy(),\n",
        "                self.decoder.get_inputs()[3].name: n_layer_cross_k.numpy(),\n",
        "                self.decoder.get_inputs()[4].name: n_layer_cross_v.numpy(),\n",
        "                self.decoder.get_inputs()[5].name: offset.numpy(),\n",
        "            },\n",
        "        )\n",
        "        return (\n",
        "            torch.from_numpy(logits),\n",
        "            torch.from_numpy(out_n_layer_self_k_cache),\n",
        "            torch.from_numpy(out_n_layer_self_v_cache),\n",
        "        )\n",
        "\n",
        "    def get_self_cache(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        batch_size = 1\n",
        "        n_layer_self_k_cache = torch.zeros(\n",
        "            self.n_text_layer,\n",
        "            batch_size,\n",
        "            self.n_text_ctx,\n",
        "            self.n_text_state,\n",
        "        )\n",
        "        n_layer_self_v_cache = torch.zeros(\n",
        "            self.n_text_layer,\n",
        "            batch_size,\n",
        "            self.n_text_ctx,\n",
        "            self.n_text_state,\n",
        "        )\n",
        "        return n_layer_self_k_cache, n_layer_self_v_cache\n",
        "\n",
        "    def suppress_tokens(self, logits, is_initial: bool) -> None:\n",
        "        # suppress blank\n",
        "        if is_initial:\n",
        "            logits[self.eot] = float(\"-inf\")\n",
        "            logits[self.blank] = float(\"-inf\")\n",
        "\n",
        "        # suppress <|notimestamps|>\n",
        "        logits[self.no_timestamps] = float(\"-inf\")\n",
        "\n",
        "        logits[self.sot] = float(\"-inf\")\n",
        "        logits[self.no_speech] = float(\"-inf\")\n",
        "\n",
        "        # logits is changed in-place\n",
        "        logits[self.translate] = float(\"-inf\")\n",
        "\n",
        "    def detect_language(\n",
        "        self, n_layer_cross_k: torch.Tensor, n_layer_cross_v: torch.Tensor\n",
        "    ) -> int:\n",
        "        tokens = torch.tensor([[self.sot]], dtype=torch.int64)\n",
        "        offset = torch.zeros(1, dtype=torch.int64)\n",
        "        n_layer_self_k_cache, n_layer_self_v_cache = self.get_self_cache()\n",
        "\n",
        "        logits, n_layer_self_k_cache, n_layer_self_v_cache = self.run_decoder(\n",
        "            tokens=tokens,\n",
        "            n_layer_self_k_cache=n_layer_self_k_cache,\n",
        "            n_layer_self_v_cache=n_layer_self_v_cache,\n",
        "            n_layer_cross_k=n_layer_cross_k,\n",
        "            n_layer_cross_v=n_layer_cross_v,\n",
        "            offset=offset,\n",
        "        )\n",
        "        logits = logits.reshape(-1)\n",
        "        mask = torch.ones(logits.shape[0], dtype=torch.int64)\n",
        "        mask[self.all_language_tokens] = 0\n",
        "        logits[mask] = float(\"-inf\")\n",
        "        lang_id = logits.argmax().item()\n",
        "        print(\"detected language: \", self.id2lang[lang_id])\n",
        "        return lang_id\n",
        "\n",
        "\n",
        "def load_tokens(filename):\n",
        "    tokens = dict()\n",
        "    with open(filename, \"r\") as f:\n",
        "        for line in f:\n",
        "            t, i = line.split()\n",
        "            tokens[int(i)] = t\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "KPj4qkob7_cU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoding"
      ],
      "metadata": {
        "id": "-7SuhfBA8dAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "from typing import Optional\n",
        "\n",
        "def decode(sound_file: str, encoder: str, decoder: str, tokens_txt: str,\n",
        "           language: Optional[str] = None,\n",
        "           task: Optional[str] = None):\n",
        "    '''\n",
        "    Args:\n",
        "        - sound_file (str): Path to the sound file for decoding.\n",
        "        - encoder (str): Path to the encoder model in onnx format\n",
        "        - decoder (str): Path to the decoder model in onnx format\n",
        "        - tokens (str): Path to the tokens.txt\n",
        "        - language (str): If not empty, it specifies the spoken language\n",
        "            of the sound_file. Example values: en, zh, jp, fr, de.\n",
        "        - task (str): It can be empty or take the following two values: transcribe, translate\n",
        "    '''\n",
        "    mel = compute_features(sound_file)\n",
        "    model = OnnxModel(encoder, decoder)\n",
        "\n",
        "    n_layer_cross_k, n_layer_cross_v = model.run_encoder(mel)\n",
        "\n",
        "    if language is not None:\n",
        "        if model.is_multilingual is False and language != \"en\":\n",
        "            print(f\"This model supports only English. Given: {language}\")\n",
        "            return\n",
        "\n",
        "        if args.language not in model.lang2id:\n",
        "            print(f\"Invalid language: {language}\")\n",
        "            print(f\"Valid values are: {list(model.lang2id.keys())}\")\n",
        "            return\n",
        "\n",
        "        # [sot, lang, task, notimestamps]\n",
        "        model.sot_sequence[1] = model.lang2id[language]\n",
        "    elif model.is_multilingual is True:\n",
        "        print(\"detecting language\")\n",
        "        lang = model.detect_language(n_layer_cross_k, n_layer_cross_v)\n",
        "        model.sot_sequence[1] = lang\n",
        "\n",
        "    if task is not None:\n",
        "        if model.is_multilingual is False and task != \"transcribe\":\n",
        "            print(\"This model supports only English. Please use --task=transcribe\")\n",
        "            return\n",
        "        assert task in [\"transcribe\", \"translate\"], task\n",
        "\n",
        "        if task == \"translate\":\n",
        "            model.sot_sequence[2] = model.translate\n",
        "\n",
        "    n_layer_self_k_cache, n_layer_self_v_cache = model.get_self_cache()\n",
        "\n",
        "    tokens = torch.tensor([model.sot_sequence], dtype=torch.int64)\n",
        "    offset = torch.zeros(1, dtype=torch.int64)\n",
        "    logits, n_layer_self_k_cache, n_layer_self_v_cache = model.run_decoder(\n",
        "        tokens=tokens,\n",
        "        n_layer_self_k_cache=n_layer_self_k_cache,\n",
        "        n_layer_self_v_cache=n_layer_self_v_cache,\n",
        "        n_layer_cross_k=n_layer_cross_k,\n",
        "        n_layer_cross_v=n_layer_cross_v,\n",
        "        offset=offset,\n",
        "    )\n",
        "    offset += len(model.sot_sequence)\n",
        "    # logits.shape (batch_size, tokens.shape[1], vocab_size)\n",
        "    logits = logits[0, -1]\n",
        "    model.suppress_tokens(logits, is_initial=True)\n",
        "    #  logits = logits.softmax(dim=-1)\n",
        "    # for greedy search, we don't need to compute softmax or log_softmax\n",
        "    max_token_id = logits.argmax(dim=-1)\n",
        "    results = []\n",
        "    for i in range(model.n_text_ctx):\n",
        "        if max_token_id == model.eot:\n",
        "            break\n",
        "        results.append(max_token_id.item())\n",
        "        tokens = torch.tensor([[results[-1]]])\n",
        "\n",
        "        logits, n_layer_self_k_cache, n_layer_self_v_cache = model.run_decoder(\n",
        "            tokens=tokens,\n",
        "            n_layer_self_k_cache=n_layer_self_k_cache,\n",
        "            n_layer_self_v_cache=n_layer_self_v_cache,\n",
        "            n_layer_cross_k=n_layer_cross_k,\n",
        "            n_layer_cross_v=n_layer_cross_v,\n",
        "            offset=offset,\n",
        "        )\n",
        "        offset += 1\n",
        "        logits = logits[0, -1]\n",
        "        model.suppress_tokens(logits, is_initial=False)\n",
        "        max_token_id = logits.argmax(dim=-1)\n",
        "    token_table = load_tokens(tokens_txt)\n",
        "    s = b\"\"\n",
        "    for i in results:\n",
        "        if i in token_table:\n",
        "            s += base64.b64decode(token_table[i])\n",
        "\n",
        "    print(s.decode().strip())"
      ],
      "metadata": {
        "id": "aRz80Khm8aJ9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test tiny.en (English only model)"
      ],
      "metadata": {
        "id": "tKEsaYBp9RWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    \"sound_file\": \"./sherpa-onnx-whisper-tiny.en/test_wavs/0.wav\",\n",
        "    \"encoder\": \"./sherpa-onnx-whisper-tiny.en/tiny.en-encoder.onnx\",\n",
        "    \"decoder\": \"./sherpa-onnx-whisper-tiny.en/tiny.en-decoder.onnx\",\n",
        "    \"tokens_txt\": \"./sherpa-onnx-whisper-tiny.en/tiny.en-tokens.txt\",\n",
        "}\n",
        "decode(**args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEaPCAPM9OoH",
        "outputId": "60c476ad-b93b-45f7-ba05-509a4c735b1a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After early nightfall, the yellow lamps would light up here and there, the squalid quarter of the brothels.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test tiny (Multilingual model)"
      ],
      "metadata": {
        "id": "rHwhgeLiBVJE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transcribe"
      ],
      "metadata": {
        "id": "kCuqp-5SBijP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    \"sound_file\": \"./sherpa-onnx-whisper-tiny/test_wavs/chinese-i-love-you.wav\",\n",
        "    \"encoder\": \"./sherpa-onnx-whisper-tiny/tiny-encoder.onnx\",\n",
        "    \"decoder\": \"./sherpa-onnx-whisper-tiny/tiny-decoder.onnx\",\n",
        "    \"tokens_txt\": \"./sherpa-onnx-whisper-tiny/tiny-tokens.txt\",\n",
        "}\n",
        "decode(**args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xKICeUw9nXL",
        "outputId": "7f48fe32-d147-4c71-ff9c-f18c80e3b05d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detecting language\n",
            "detected language:  zh\n",
            "我愛你\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Translate to English"
      ],
      "metadata": {
        "id": "f6sou3cvBj7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    \"sound_file\": \"./sherpa-onnx-whisper-tiny/test_wavs/chinese-i-love-you.wav\",\n",
        "    \"encoder\": \"./sherpa-onnx-whisper-tiny/tiny-encoder.onnx\",\n",
        "    \"decoder\": \"./sherpa-onnx-whisper-tiny/tiny-decoder.onnx\",\n",
        "    \"tokens_txt\": \"./sherpa-onnx-whisper-tiny/tiny-tokens.txt\",\n",
        "    \"task\": \"translate\",\n",
        "}\n",
        "decode(**args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvZWul_h-0px",
        "outputId": "80a70bc3-1606-450a-d3b1-8ef314f42b70"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detecting language\n",
            "detected language:  zh\n",
            "I love you\n"
          ]
        }
      ]
    }
  ]
}
