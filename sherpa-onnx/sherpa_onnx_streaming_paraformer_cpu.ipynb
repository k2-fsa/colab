{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNH6dBFkyxsU570VH93x5es",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/k2-fsa/colab/blob/master/sherpa-onnx/sherpa_onnx_streaming_paraformer_cpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "This colab notebook shows how to use [sherpa-onnx](https://github.com/k2-fsa/sherpa-onnx) to run streaming [Paraformer](https://www.modelscope.cn/models/damo/speech_paraformer_asr_nat-zh-cn-16k-common-vocab8404-online/summary) models on CPU.\n",
        "\n"
      ],
      "metadata": {
        "id": "iW4DaE7675u0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install sherpa-onnx"
      ],
      "metadata": {
        "id": "Kx_OQz338ZHW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKOcfEZ47wsD",
        "outputId": "d4b1e1f8-9cfc-44a4-ec46-0f462ebe7a6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sherpa-onnx\n",
            "  Downloading sherpa_onnx-1.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sherpa-onnx) (1.23.5)\n",
            "Collecting sentencepiece==0.1.96 (from sherpa-onnx)\n",
            "  Downloading sentencepiece-0.1.96-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece, sherpa-onnx\n",
            "Successfully installed sentencepiece-0.1.96 sherpa-onnx-1.7.5\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "pip install sherpa-onnx\n",
        "\n",
        "# Please refer to\n",
        "# https://k2-fsa.github.io/sherpa/onnx/install/index.html\n",
        "# for other installation methods"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check that `sherpa-onnx` has been installed successfully:"
      ],
      "metadata": {
        "id": "3WXv3NFt8o3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "# C++ binaries: Decoding files with streaming models\n",
        "sherpa-onnx --help\n",
        "\n",
        "# C++ binaries: Streaming WebSocket server\n",
        "sherpa-onnx-online-websocket-server --help\n",
        "\n",
        "# Test Python binding\n",
        "python3 -c \"import sherpa_onnx; print(sherpa_onnx.__file__, sherpa_onnx.__version__)\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5fWv7238ihS",
        "outputId": "ffdb9d68-b689-4575-b3fc-a49609078db6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/project/sherpa-onnx/csrc/parse-options.cc:PrintUsage:402 \n",
            "\n",
            "Usage:\n",
            "\n",
            "  ./bin/sherpa-onnx \\\n",
            "    --tokens=/path/to/tokens.txt \\\n",
            "    --encoder=/path/to/encoder.onnx \\\n",
            "    --decoder=/path/to/decoder.onnx \\\n",
            "    --joiner=/path/to/joiner.onnx \\\n",
            "    --provider=cpu \\\n",
            "    --num-threads=2 \\\n",
            "    --decoding-method=greedy_search \\\n",
            "    /path/to/foo.wav [bar.wav foobar.wav ...]\n",
            "\n",
            "Note: It supports decoding multiple files in batches\n",
            "\n",
            "Default value for num_threads is 2.\n",
            "Valid values for decoding_method: greedy_search (default), modified_beam_search.\n",
            "Valid values for provider: cpu (default), cuda, coreml.\n",
            "foo.wav should be of single channel, 16-bit PCM encoded wave file; its\n",
            "sampling rate can be arbitrary and does not need to be 16kHz.\n",
            "\n",
            "Please refer to\n",
            "https://k2-fsa.github.io/sherpa/onnx/pretrained_models/index.html\n",
            "for a list of pre-trained models to download.\n",
            "\n",
            "Options:\n",
            "  --max-active-paths          : beam size used in modified beam search. (int, default = 4)\n",
            "  --debug                     : true to print model information while loading it. (bool, default = false)\n",
            "  --decoding-method           : decoding method,now support greedy_search and modified_beam_search. (string, default = \"greedy_search\")\n",
            "  --tokens                    : Path to tokens.txt (string, default = \"\")\n",
            "  --rule2-must-contain-nonsilence : If True, for this endpointing rule2 to apply there must be nonsilence in the best-path traceback. For decoding, a non-blank token is considered as non-silence (bool, default = true)\n",
            "  --num-threads               : Number of threads to run the neural network (int, default = 1)\n",
            "  --encoder                   : Path to encoder.onnx (string, default = \"\")\n",
            "  --sample-rate               : Sampling rate of the input waveform. Note: You can have a different sample rate for the input waveform. We will do resampling inside the feature extractor (int, default = 16000)\n",
            "  --paraformer-decoder        : Path to decoder.onnx of paraformer. (string, default = \"\")\n",
            "  --rule3-min-trailing-silence : This endpointing rule3 requires duration of trailing silence in seconds) to be >= this value. (float, default = 0)\n",
            "  --lm-num-threads            : Number of threads to run the neural network of LM model (int, default = 1)\n",
            "  --decoder                   : Path to decoder.onnx (string, default = \"\")\n",
            "  --rule3-must-contain-nonsilence : If True, for this endpointing rule3 to apply there must be nonsilence in the best-path traceback. For decoding, a non-blank token is considered as non-silence (bool, default = false)\n",
            "  --joiner                    : Path to joiner.onnx (string, default = \"\")\n",
            "  --provider                  : Specify a provider to use: cpu, cuda, coreml (string, default = \"cpu\")\n",
            "  --model-type                : Specify it to reduce model initialization time. Valid values are: conformer, lstm, zipformer, zipformer2.All other values lead to loading the model twice. (string, default = \"\")\n",
            "  --feat-dim                  : Feature dimension. Must match the one expected by the model. (int, default = 80)\n",
            "  --rule1-must-contain-nonsilence : If True, for this endpointing rule1 to apply there must be nonsilence in the best-path traceback. For decoding, a non-blank token is considered as non-silence (bool, default = false)\n",
            "  --lm                        : Path to LM model. (string, default = \"\")\n",
            "  --paraformer-encoder        : Path to encoder.onnx of paraformer. (string, default = \"\")\n",
            "  --rule1-min-utterance-length : This endpointing rule1 requires utterance-length (in seconds) to be >= this value. (float, default = 0)\n",
            "  --rule2-min-trailing-silence : This endpointing rule2 requires duration of trailing silence in seconds) to be >= this value. (float, default = 1.2)\n",
            "  --context-score             : The bonus score for each token in context word/phrase. Used only when decoding_method is modified_beam_search (float, default = 1.5)\n",
            "  --rule2-min-utterance-length : This endpointing rule2 requires utterance-length (in seconds) to be >= this value. (float, default = 0)\n",
            "  --lm-scale                  : LM scale. (float, default = 0.5)\n",
            "  --rule3-min-utterance-length : This endpointing rule3 requires utterance-length (in seconds) to be >= this value. (float, default = 20)\n",
            "  --lm-provider               : Specify a provider to LM model use: cpu, cuda, coreml (string, default = \"cpu\")\n",
            "  --rule1-min-trailing-silence : This endpointing rule1 requires duration of trailing silence in seconds) to be >= this value. (float, default = 2.4)\n",
            "  --enable-endpoint           : True to enable endpoint detection. False to disable it. (bool, default = true)\n",
            "\n",
            "Standard options:\n",
            "  --print-args                : Print the command line arguments (to stderr) (bool, default = true)\n",
            "  --help                      : Print out usage message (bool, default = false)\n",
            "  --config                    : Configuration file to read (this option may be repeated) (string, default = \"\")\n",
            "\n",
            "\n",
            "/project/sherpa-onnx/csrc/parse-options.cc:PrintUsage:402 \n",
            "\n",
            "Automatic speech recognition with sherpa-onnx using websocket.\n",
            "\n",
            "Usage:\n",
            "\n",
            "./bin/sherpa-onnx-online-websocket-server --help\n",
            "\n",
            "./bin/sherpa-onnx-online-websocket-server \\\n",
            "  --port=6006 \\\n",
            "  --num-work-threads=5 \\\n",
            "  --tokens=/path/to/tokens.txt \\\n",
            "  --encoder=/path/to/encoder.onnx \\\n",
            "  --decoder=/path/to/decoder.onnx \\\n",
            "  --joiner=/path/to/joiner.onnx \\\n",
            "  --log-file=./log.txt \\\n",
            "  --max-batch-size=5 \\\n",
            "  --loop-interval-ms=10\n",
            "\n",
            "Please refer to\n",
            "https://k2-fsa.github.io/sherpa/onnx/pretrained_models/index.html\n",
            "for a list of pre-trained models to download.\n",
            "\n",
            "Options:\n",
            "  --max-batch-size            : Max batch size for recognition. (int, default = 5)\n",
            "  --loop-interval-ms          : It determines how often the decoder loop runs.  (int, default = 10)\n",
            "  --max-active-paths          : beam size used in modified beam search. (int, default = 4)\n",
            "  --lm-num-threads            : Number of threads to run the neural network of LM model (int, default = 1)\n",
            "  --rule2-must-contain-nonsilence : If True, for this endpointing rule2 to apply there must be nonsilence in the best-path traceback. For decoding, a non-blank token is considered as non-silence (bool, default = true)\n",
            "  --lm-provider               : Specify a provider to LM model use: cpu, cuda, coreml (string, default = \"cpu\")\n",
            "  --rule3-min-utterance-length : This endpointing rule3 requires utterance-length (in seconds) to be >= this value. (float, default = 20)\n",
            "  --num-threads               : Number of threads to run the neural network (int, default = 1)\n",
            "  --log-file                  : Path to the log file. Logs are appended to this file (string, default = \"./log.txt\")\n",
            "  --encoder                   : Path to encoder.onnx (string, default = \"\")\n",
            "  --sample-rate               : Sampling rate of the input waveform. Note: You can have a different sample rate for the input waveform. We will do resampling inside the feature extractor (int, default = 16000)\n",
            "  --paraformer-decoder        : Path to decoder.onnx of paraformer. (string, default = \"\")\n",
            "  --rule3-min-trailing-silence : This endpointing rule3 requires duration of trailing silence in seconds) to be >= this value. (float, default = 0)\n",
            "  --port                      : The port on which the server will listen. (int, default = 6006)\n",
            "  --context-score             : The bonus score for each token in context word/phrase. Used only when decoding_method is modified_beam_search (float, default = 1.5)\n",
            "  --rule2-min-utterance-length : This endpointing rule2 requires utterance-length (in seconds) to be >= this value. (float, default = 0)\n",
            "  --num-work-threads          : Thread pool size for for neural network computation and decoding. (int, default = 3)\n",
            "  --rule1-must-contain-nonsilence : If True, for this endpointing rule1 to apply there must be nonsilence in the best-path traceback. For decoding, a non-blank token is considered as non-silence (bool, default = false)\n",
            "  --rule3-must-contain-nonsilence : If True, for this endpointing rule3 to apply there must be nonsilence in the best-path traceback. For decoding, a non-blank token is considered as non-silence (bool, default = false)\n",
            "  --joiner                    : Path to joiner.onnx (string, default = \"\")\n",
            "  --provider                  : Specify a provider to use: cpu, cuda, coreml (string, default = \"cpu\")\n",
            "  --model-type                : Specify it to reduce model initialization time. Valid values are: conformer, lstm, zipformer, zipformer2.All other values lead to loading the model twice. (string, default = \"\")\n",
            "  --feat-dim                  : Feature dimension. Must match the one expected by the model. (int, default = 80)\n",
            "  --num-io-threads            : Thread pool size for network connections. (int, default = 1)\n",
            "  --enable-endpoint           : True to enable endpoint detection. False to disable it. (bool, default = true)\n",
            "  --rule1-min-trailing-silence : This endpointing rule1 requires duration of trailing silence in seconds) to be >= this value. (float, default = 2.4)\n",
            "  --lm                        : Path to LM model. (string, default = \"\")\n",
            "  --decoder                   : Path to decoder.onnx (string, default = \"\")\n",
            "  --debug                     : true to print model information while loading it. (bool, default = false)\n",
            "  --decoding-method           : decoding method,now support greedy_search and modified_beam_search. (string, default = \"greedy_search\")\n",
            "  --tokens                    : Path to tokens.txt (string, default = \"\")\n",
            "  --paraformer-encoder        : Path to encoder.onnx of paraformer. (string, default = \"\")\n",
            "  --rule1-min-utterance-length : This endpointing rule1 requires utterance-length (in seconds) to be >= this value. (float, default = 0)\n",
            "  --rule2-min-trailing-silence : This endpointing rule2 requires duration of trailing silence in seconds) to be >= this value. (float, default = 1.2)\n",
            "  --lm-scale                  : LM scale. (float, default = 0.5)\n",
            "\n",
            "Standard options:\n",
            "  --print-args                : Print the command line arguments (to stderr) (bool, default = true)\n",
            "  --help                      : Print out usage message (bool, default = false)\n",
            "  --config                    : Configuration file to read (this option may be repeated) (string, default = \"\")\n",
            "\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sherpa_onnx/__init__.py 1.7.5\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download pre-trained streaming paraformer model"
      ],
      "metadata": {
        "id": "vxKZdOxe9J4N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please see\n",
        "https://k2-fsa.github.io/sherpa/onnx/pretrained_models/online-paraformer/index.html\n",
        "for details"
      ],
      "metadata": {
        "id": "piHd64El9PVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "sudo apt-get install git-lfs\n",
        "git clone https://huggingface.co/csukuangfj/sherpa-onnx-streaming-paraformer-bilingual-zh-en\n",
        "ls -lh sherpa-onnx-streaming-paraformer-bilingual-zh-en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWHs4qig8vwm",
        "outputId": "ef90ae45-8b79-4988-afb2-cd3524279ea1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (3.0.2-1ubuntu0.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 16 not upgraded.\n",
            "Cloning into 'sherpa-onnx-streaming-paraformer-bilingual-zh-en'...\n",
            "remote: Enumerating objects: 18, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 18 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (18/18), 949.80 KiB | 9.22 MiB/s, done.\n",
            "Filtering content: 100% (4/4), 1.02 GiB | 23.34 MiB/s, done.\n",
            "total 1.1G\n",
            "-rw-r--r-- 1 root root  69M Aug 14 13:02 decoder.int8.onnx\n",
            "-rw-r--r-- 1 root root 218M Aug 14 13:02 decoder.onnx\n",
            "-rw-r--r-- 1 root root 158M Aug 14 13:02 encoder.int8.onnx\n",
            "-rw-r--r-- 1 root root 607M Aug 14 13:03 encoder.onnx\n",
            "-rw-r--r-- 1 root root  415 Aug 14 13:02 README.md\n",
            "drwxr-xr-x 2 root root 4.0K Aug 14 13:02 test_wavs\n",
            "-rw-r--r-- 1 root root  74K Aug 14 13:02 tokens.txt\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real-time factor (RTF) test"
      ],
      "metadata": {
        "id": "B2nLF-U19ez3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## float32 models"
      ],
      "metadata": {
        "id": "pRh82vtw-GOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "sherpa-onnx \\\n",
        "  --tokens=./sherpa-onnx-streaming-paraformer-bilingual-zh-en/tokens.txt \\\n",
        "  --paraformer-encoder=./sherpa-onnx-streaming-paraformer-bilingual-zh-en/encoder.onnx \\\n",
        "  --paraformer-decoder=./sherpa-onnx-streaming-paraformer-bilingual-zh-en/decoder.onnx \\\n",
        "  --num-threads=1 \\\n",
        "  ./sherpa-onnx-streaming-paraformer-bilingual-zh-en/test_wavs/0.wav"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmZnMS9f9XVe",
        "outputId": "d4c7863b-6eda-4934-dd8e-66a74ba79024"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/project/sherpa-onnx/csrc/parse-options.cc:Read:361 sherpa-onnx --tokens=./sherpa-onnx-streaming-paraformer-bilingual-zh-en/tokens.txt --paraformer-encoder=./sherpa-onnx-streaming-paraformer-bilingual-zh-en/encoder.onnx --paraformer-decoder=./sherpa-onnx-streaming-paraformer-bilingual-zh-en/decoder.onnx --num-threads=1 ./sherpa-onnx-streaming-paraformer-bilingual-zh-en/test_wavs/0.wav \n",
            "\n",
            "OnlineRecognizerConfig(feat_config=FeatureExtractorConfig(sampling_rate=16000, feature_dim=80), model_config=OnlineModelConfig(transducer=OnlineTransducerModelConfig(encoder=\"\", decoder=\"\", joiner=\"\"), paraformer=OnlineParaformerModelConfig(encoder=\"./sherpa-onnx-streaming-paraformer-bilingual-zh-en/encoder.onnx\", decoder=\"./sherpa-onnx-streaming-paraformer-bilingual-zh-en/decoder.onnx\"), tokens=\"./sherpa-onnx-streaming-paraformer-bilingual-zh-en/tokens.txt\", num_threads=1, debug=False, provider=\"cpu\", model_type=\"\"), lm_config=OnlineLMConfig(model=\"\", scale=0.5), endpoint_config=EndpointConfig(rule1=EndpointRule(must_contain_nonsilence=False, min_trailing_silence=2.4, min_utterance_length=0), rule2=EndpointRule(must_contain_nonsilence=True, min_trailing_silence=1.2, min_utterance_length=0), rule3=EndpointRule(must_contain_nonsilence=False, min_trailing_silence=0, min_utterance_length=20)), enable_endpoint=True, max_active_paths=4, context_score=1.5, decoding_method=\"greedy_search\")\n",
            "./sherpa-onnx-streaming-paraformer-bilingual-zh-en/test_wavs/0.wav\n",
            "Elapsed seconds: 4.1, Real time factor (RTF): 0.41\n",
            "昨天是 monday today day is 零八二 the day after tomorrow 是星期三\n",
            "{\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"昨天是 monday today day is 零八二 the day after tomorrow 是星期三\",\"timestamps\":\"[]\",\"tokens\":[\"昨\",\"天\",\"是\",\"mon@@\",\"day\",\"today\",\"day\",\"is\",\"零\",\"八\",\"二\",\"the\",\"day\",\"after\",\"tom@@\",\"or@@\",\"row\",\"是\",\"星\",\"期\",\"三\"]}\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## int8 models"
      ],
      "metadata": {
        "id": "pE3o4XGh94xp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "sherpa-onnx \\\n",
        "  --tokens=./sherpa-onnx-streaming-paraformer-bilingual-zh-en/tokens.txt \\\n",
        "  --paraformer-encoder=./sherpa-onnx-streaming-paraformer-bilingual-zh-en/encoder.int8.onnx \\\n",
        "  --paraformer-decoder=./sherpa-onnx-streaming-paraformer-bilingual-zh-en/decoder.int8.onnx \\\n",
        "  --num-threads=1 \\\n",
        "  ./sherpa-onnx-streaming-paraformer-bilingual-zh-en/test_wavs/0.wav"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gToE-AWy9sEM",
        "outputId": "79eeb7a3-28ea-4089-ab6c-eaf3c4eb4684"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/project/sherpa-onnx/csrc/parse-options.cc:Read:361 sherpa-onnx --tokens=./sherpa-onnx-streaming-paraformer-bilingual-zh-en/tokens.txt --paraformer-encoder=./sherpa-onnx-streaming-paraformer-bilingual-zh-en/encoder.int8.onnx --paraformer-decoder=./sherpa-onnx-streaming-paraformer-bilingual-zh-en/decoder.int8.onnx --num-threads=1 ./sherpa-onnx-streaming-paraformer-bilingual-zh-en/test_wavs/0.wav \n",
            "\n",
            "OnlineRecognizerConfig(feat_config=FeatureExtractorConfig(sampling_rate=16000, feature_dim=80), model_config=OnlineModelConfig(transducer=OnlineTransducerModelConfig(encoder=\"\", decoder=\"\", joiner=\"\"), paraformer=OnlineParaformerModelConfig(encoder=\"./sherpa-onnx-streaming-paraformer-bilingual-zh-en/encoder.int8.onnx\", decoder=\"./sherpa-onnx-streaming-paraformer-bilingual-zh-en/decoder.int8.onnx\"), tokens=\"./sherpa-onnx-streaming-paraformer-bilingual-zh-en/tokens.txt\", num_threads=1, debug=False, provider=\"cpu\", model_type=\"\"), lm_config=OnlineLMConfig(model=\"\", scale=0.5), endpoint_config=EndpointConfig(rule1=EndpointRule(must_contain_nonsilence=False, min_trailing_silence=2.4, min_utterance_length=0), rule2=EndpointRule(must_contain_nonsilence=True, min_trailing_silence=1.2, min_utterance_length=0), rule3=EndpointRule(must_contain_nonsilence=False, min_trailing_silence=0, min_utterance_length=20)), enable_endpoint=True, max_active_paths=4, context_score=1.5, decoding_method=\"greedy_search\")\n",
            "./sherpa-onnx-streaming-paraformer-bilingual-zh-en/test_wavs/0.wav\n",
            "Elapsed seconds: 2.8, Real time factor (RTF): 0.28\n",
            "昨天是 monday today day is 零八二 the day after tomorrow 是星期三\n",
            "{\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"昨天是 monday today day is 零八二 the day after tomorrow 是星期三\",\"timestamps\":\"[]\",\"tokens\":[\"昨\",\"天\",\"是\",\"mon@@\",\"day\",\"today\",\"day\",\"is\",\"零\",\"八\",\"二\",\"the\",\"day\",\"after\",\"tom@@\",\"or@@\",\"row\",\"是\",\"星\",\"期\",\"三\"]}\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start the C++ streaming websocket server"
      ],
      "metadata": {
        "id": "IuvlEF84-KNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! nproc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcI7dKZR-pYL",
        "outputId": "c900d642-e2cf-4d89-b447-93d152aebb54"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This colab notebook provides only 2 CPUs."
      ],
      "metadata": {
        "id": "Wu2pS1uX-qS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "nohup sherpa-onnx-online-websocket-server \\\n",
        "  --tokens=./sherpa-onnx-streaming-paraformer-bilingual-zh-en/tokens.txt \\\n",
        "  --paraformer-encoder=./sherpa-onnx-streaming-paraformer-bilingual-zh-en/encoder.int8.onnx \\\n",
        "  --paraformer-decoder=./sherpa-onnx-streaming-paraformer-bilingual-zh-en/decoder.int8.onnx \\\n",
        "  --port=6006 \\\n",
        "  --num-work-threads=1 \\\n",
        "  --num-io-threads=1 > cpp-log-server.txt &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sf_rxq1N9-ID",
        "outputId": "9b05d261-c6e1-432c-a922-6fcce7256f2d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: redirecting stderr to stdout\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# sleep 10 seconds to wait for the server startup\n",
        "sleep 10\n",
        "cat cpp-log-server.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRUvYD0E-kgY",
        "outputId": "8ad0bb86-899f-4207-da71-0c4aa5e97f57"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/project/sherpa-onnx/csrc/parse-options.cc:Read:361 sherpa-onnx-online-websocket-server --tokens=./sherpa-onnx-streaming-paraformer-bilingual-zh-en/tokens.txt --paraformer-encoder=./sherpa-onnx-streaming-paraformer-bilingual-zh-en/encoder.int8.onnx --paraformer-decoder=./sherpa-onnx-streaming-paraformer-bilingual-zh-en/decoder.int8.onnx --port=6006 --num-work-threads=1 --num-io-threads=1 \n",
            "\n",
            "/project/sherpa-onnx/csrc/online-websocket-server.cc:main:79 Started!\n",
            "/project/sherpa-onnx/csrc/online-websocket-server.cc:main:80 Listening on: 6006\n",
            "/project/sherpa-onnx/csrc/online-websocket-server.cc:main:81 Number of work threads: 1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start a C++ webscoket client to decode files"
      ],
      "metadata": {
        "id": "hXWN6tKFCL6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "sherpa-onnx-online-websocket-client \\\n",
        "  --server-ip=127.0.0.1  \\\n",
        "  --server-port=6006 \\\n",
        "  sherpa-onnx-streaming-paraformer-bilingual-zh-en/test_wavs/0.wav"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95xJlS2NCLB2",
        "outputId": "09875641-db0a-4df1-c255-902d4604a03d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/project/sherpa-onnx/csrc/parse-options.cc:Read:361 sherpa-onnx-online-websocket-client --server-ip=127.0.0.1 --server-port=6006 sherpa-onnx-streaming-paraformer-bilingual-zh-en/test_wavs/0.wav \n",
            "\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:SendMessage:114 Starting to send audio\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:OnMessage:91 {\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"\",\"timestamps\":\"[]\",\"tokens\":[]}\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:OnMessage:91 {\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"昨\",\"timestamps\":\"[]\",\"tokens\":[\"昨\"]}\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:OnMessage:91 {\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"昨天是\",\"timestamps\":\"[]\",\"tokens\":[\"昨\",\"天\",\"是\"]}\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:OnMessage:91 {\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"昨天是 mon\",\"timestamps\":\"[]\",\"tokens\":[\"昨\",\"天\",\"是\",\"mon@@\"]}\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:OnMessage:91 {\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"昨天是 monday\",\"timestamps\":\"[]\",\"tokens\":[\"昨\",\"天\",\"是\",\"mon@@\",\"day\"]}\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:OnMessage:91 {\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"昨天是 monday\",\"timestamps\":\"[]\",\"tokens\":[\"昨\",\"天\",\"是\",\"mon@@\",\"day\"]}\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:OnMessage:91 {\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"昨天是 monday today\",\"timestamps\":\"[]\",\"tokens\":[\"昨\",\"天\",\"是\",\"mon@@\",\"day\",\"today\"]}\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:OnMessage:91 {\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"昨天是 monday today day\",\"timestamps\":\"[]\",\"tokens\":[\"昨\",\"天\",\"是\",\"mon@@\",\"day\",\"today\",\"day\"]}\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:OnMessage:91 {\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"昨天是 monday today day is\",\"timestamps\":\"[]\",\"tokens\":[\"昨\",\"天\",\"是\",\"mon@@\",\"day\",\"today\",\"day\",\"is\"]}\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:OnMessage:91 {\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"昨天是 monday today day is 零八\",\"timestamps\":\"[]\",\"tokens\":[\"昨\",\"天\",\"是\",\"mon@@\",\"day\",\"today\",\"day\",\"is\",\"零\",\"八\"]}\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:OnMessage:91 {\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"昨天是 monday today day is 零八二\",\"timestamps\":\"[]\",\"tokens\":[\"昨\",\"天\",\"是\",\"mon@@\",\"day\",\"today\",\"day\",\"is\",\"零\",\"八\",\"二\"]}\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:OnMessage:91 {\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"昨天是 monday today day is 零八二 the day\",\"timestamps\":\"[]\",\"tokens\":[\"昨\",\"天\",\"是\",\"mon@@\",\"day\",\"today\",\"day\",\"is\",\"零\",\"八\",\"二\",\"the\",\"day\"]}\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:OnMessage:91 {\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"昨天是 monday today day is 零八二 the day after\",\"timestamps\":\"[]\",\"tokens\":[\"昨\",\"天\",\"是\",\"mon@@\",\"day\",\"today\",\"day\",\"is\",\"零\",\"八\",\"二\",\"the\",\"day\",\"after\"]}\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:OnMessage:91 {\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"昨天是 monday today day is 零八二 the day after tomor\",\"timestamps\":\"[]\",\"tokens\":[\"昨\",\"天\",\"是\",\"mon@@\",\"day\",\"today\",\"day\",\"is\",\"零\",\"八\",\"二\",\"the\",\"day\",\"after\",\"tom@@\",\"or@@\"]}\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:SendMessage:151 Sent Done Signal\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:OnMessage:91 {\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"昨天是 monday today day is 零八二 the day after tomorrow\",\"timestamps\":\"[]\",\"tokens\":[\"昨\",\"天\",\"是\",\"mon@@\",\"day\",\"today\",\"day\",\"is\",\"零\",\"八\",\"二\",\"the\",\"day\",\"after\",\"tom@@\",\"or@@\",\"row\"]}\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:OnMessage:91 {\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"昨天是 monday today day is 零八二 the day after tomorrow 是星\",\"timestamps\":\"[]\",\"tokens\":[\"昨\",\"天\",\"是\",\"mon@@\",\"day\",\"today\",\"day\",\"is\",\"零\",\"八\",\"二\",\"the\",\"day\",\"after\",\"tom@@\",\"or@@\",\"row\",\"是\",\"星\"]}\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:OnMessage:91 {\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"昨天是 monday today day is 零八二 the day after tomorrow 是星期三\",\"timestamps\":\"[]\",\"tokens\":[\"昨\",\"天\",\"是\",\"mon@@\",\"day\",\"today\",\"day\",\"is\",\"零\",\"八\",\"二\",\"the\",\"day\",\"after\",\"tom@@\",\"or@@\",\"row\",\"是\",\"星\",\"期\",\"三\"]}\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:OnMessage:91 {\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"昨天是 monday today day is 零八二 the day after tomorrow 是星期三\",\"timestamps\":\"[]\",\"tokens\":[\"昨\",\"天\",\"是\",\"mon@@\",\"day\",\"today\",\"day\",\"is\",\"零\",\"八\",\"二\",\"the\",\"day\",\"after\",\"tom@@\",\"or@@\",\"row\",\"是\",\"星\",\"期\",\"三\"]}\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:operator():54 Disconnected\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:main:272 Done!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start a Python webscoket client to decode files"
      ],
      "metadata": {
        "id": "ikuhBdik_AXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Download the python client code first\n",
        "# https://github.com/k2-fsa/sherpa-onnx/blob/master/python-api-examples/online-websocket-client-decode-file.py\n",
        "\n",
        "wget https://raw.githubusercontent.com/k2-fsa/sherpa-onnx/master/python-api-examples/online-websocket-client-decode-file.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgfwwD_y-l41",
        "outputId": "e3ce0743-605c-4ff0-fe2c-141b46cbf6e6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-14 13:04:04--  https://raw.githubusercontent.com/k2-fsa/sherpa-onnx/master/python-api-examples/online-websocket-client-decode-file.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4797 (4.7K) [text/plain]\n",
            "Saving to: ‘online-websocket-client-decode-file.py’\n",
            "\n",
            "online-websocket-cl 100%[===================>]   4.68K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-08-14 13:04:05 (37.5 MB/s) - ‘online-websocket-client-decode-file.py’ saved [4797/4797]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install websockets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeaRtYsV_qVz",
        "outputId": "8a562385-6618-4f1c-a7bc-bf01e71979b4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting websockets\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m122.9/129.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: websockets\n",
            "Successfully installed websockets-11.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "python3 ./online-websocket-client-decode-file.py \\\n",
        "  --server-addr localhost \\\n",
        "  --server-port 6006 \\\n",
        "  sherpa-onnx-streaming-paraformer-bilingual-zh-en/test_wavs/0.wav"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ojwRcjt_WHp",
        "outputId": "1ae5e1d6-9cf8-4c42-d4e5-5e824667f97d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-14 13:04:12,288 INFO [online-websocket-client-decode-file.py:159] {'server_addr': 'localhost', 'server_port': 6006, 'samples_per_message': 8000, 'seconds_per_message': 0.1, 'sound_file': 'sherpa-onnx-streaming-paraformer-bilingual-zh-en/test_wavs/0.wav'}\n",
            "2023-08-14 13:04:12,295 INFO [online-websocket-client-decode-file.py:133] Sending sherpa-onnx-streaming-paraformer-bilingual-zh-en/test_wavs/0.wav\n",
            "2023-08-14 13:04:12,524 INFO [online-websocket-client-decode-file.py:115] {\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"\",\"timestamps\":\"[]\",\"tokens\":[]}\n",
            "2023-08-14 13:04:12,675 INFO [online-websocket-client-decode-file.py:115] {\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"昨\",\"timestamps\":\"[]\",\"tokens\":[\"昨\"]}\n",
            "2023-08-14 13:04:12,832 INFO [online-websocket-client-decode-file.py:115] {\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"昨天是\",\"timestamps\":\"[]\",\"tokens\":[\"昨\",\"天\",\"是\"]}\n",
            "2023-08-14 13:04:12,999 INFO [online-websocket-client-decode-file.py:115] {\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"昨天是 mon\",\"timestamps\":\"[]\",\"tokens\":[\"昨\",\"天\",\"是\",\"mon@@\"]}\n",
            "2023-08-14 13:04:13,162 INFO [online-websocket-client-decode-file.py:115] {\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"昨天是 monday\",\"timestamps\":\"[]\",\"tokens\":[\"昨\",\"天\",\"是\",\"mon@@\",\"day\"]}\n",
            "2023-08-14 13:04:13,295 INFO [online-websocket-client-decode-file.py:115] {\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"昨天是 monday\",\"timestamps\":\"[]\",\"tokens\":[\"昨\",\"天\",\"是\",\"mon@@\",\"day\"]}\n",
            "2023-08-14 13:04:13,448 INFO [online-websocket-client-decode-file.py:115] {\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"昨天是 monday today\",\"timestamps\":\"[]\",\"tokens\":[\"昨\",\"天\",\"是\",\"mon@@\",\"day\",\"today\"]}\n",
            "2023-08-14 13:04:13,607 INFO [online-websocket-client-decode-file.py:115] {\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"昨天是 monday today day\",\"timestamps\":\"[]\",\"tokens\":[\"昨\",\"天\",\"是\",\"mon@@\",\"day\",\"today\",\"day\"]}\n",
            "2023-08-14 13:04:13,766 INFO [online-websocket-client-decode-file.py:115] {\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"昨天是 monday today day is\",\"timestamps\":\"[]\",\"tokens\":[\"昨\",\"天\",\"是\",\"mon@@\",\"day\",\"today\",\"day\",\"is\"]}\n",
            "2023-08-14 13:04:13,923 INFO [online-websocket-client-decode-file.py:115] {\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"昨天是 monday today day is 零八\",\"timestamps\":\"[]\",\"tokens\":[\"昨\",\"天\",\"是\",\"mon@@\",\"day\",\"today\",\"day\",\"is\",\"零\",\"八\"]}\n",
            "2023-08-14 13:04:14,086 INFO [online-websocket-client-decode-file.py:115] {\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"昨天是 monday today day is 零八二\",\"timestamps\":\"[]\",\"tokens\":[\"昨\",\"天\",\"是\",\"mon@@\",\"day\",\"today\",\"day\",\"is\",\"零\",\"八\",\"二\"]}\n",
            "2023-08-14 13:04:14,370 INFO [online-websocket-client-decode-file.py:115] {\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"昨天是 monday today day is 零八二 the day\",\"timestamps\":\"[]\",\"tokens\":[\"昨\",\"天\",\"是\",\"mon@@\",\"day\",\"today\",\"day\",\"is\",\"零\",\"八\",\"二\",\"the\",\"day\"]}\n",
            "2023-08-14 13:04:14,578 INFO [online-websocket-client-decode-file.py:115] {\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"昨天是 monday today day is 零八二 the day after\",\"timestamps\":\"[]\",\"tokens\":[\"昨\",\"天\",\"是\",\"mon@@\",\"day\",\"today\",\"day\",\"is\",\"零\",\"八\",\"二\",\"the\",\"day\",\"after\"]}\n",
            "2023-08-14 13:04:14,738 INFO [online-websocket-client-decode-file.py:115] {\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"昨天是 monday today day is 零八二 the day after tomor\",\"timestamps\":\"[]\",\"tokens\":[\"昨\",\"天\",\"是\",\"mon@@\",\"day\",\"today\",\"day\",\"is\",\"零\",\"八\",\"二\",\"the\",\"day\",\"after\",\"tom@@\",\"or@@\"]}\n",
            "2023-08-14 13:04:14,897 INFO [online-websocket-client-decode-file.py:115] {\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"昨天是 monday today day is 零八二 the day after tomorrow\",\"timestamps\":\"[]\",\"tokens\":[\"昨\",\"天\",\"是\",\"mon@@\",\"day\",\"today\",\"day\",\"is\",\"零\",\"八\",\"二\",\"the\",\"day\",\"after\",\"tom@@\",\"or@@\",\"row\"]}\n",
            "2023-08-14 13:04:15,053 INFO [online-websocket-client-decode-file.py:115] {\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"昨天是 monday today day is 零八二 the day after tomorrow 是星\",\"timestamps\":\"[]\",\"tokens\":[\"昨\",\"天\",\"是\",\"mon@@\",\"day\",\"today\",\"day\",\"is\",\"零\",\"八\",\"二\",\"the\",\"day\",\"after\",\"tom@@\",\"or@@\",\"row\",\"是\",\"星\"]}\n",
            "2023-08-14 13:04:15,240 INFO [online-websocket-client-decode-file.py:115] {\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"昨天是 monday today day is 零八二 the day after tomorrow 是星期三\",\"timestamps\":\"[]\",\"tokens\":[\"昨\",\"天\",\"是\",\"mon@@\",\"day\",\"today\",\"day\",\"is\",\"零\",\"八\",\"二\",\"the\",\"day\",\"after\",\"tom@@\",\"or@@\",\"row\",\"是\",\"星\",\"期\",\"三\"]}\n",
            "2023-08-14 13:04:15,377 INFO [online-websocket-client-decode-file.py:115] {\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"昨天是 monday today day is 零八二 the day after tomorrow 是星期三\",\"timestamps\":\"[]\",\"tokens\":[\"昨\",\"天\",\"是\",\"mon@@\",\"day\",\"today\",\"day\",\"is\",\"零\",\"八\",\"二\",\"the\",\"day\",\"after\",\"tom@@\",\"or@@\",\"row\",\"是\",\"星\",\"期\",\"三\"]}\n",
            "2023-08-14 13:04:15,378 INFO [online-websocket-client-decode-file.py:154] \n",
            "Final result is:\n",
            "{\"is_final\":false,\"segment\":0,\"start_time\":0.0,\"text\":\"昨天是 monday today day is 零八二 the day after tomorrow 是星期三\",\"timestamps\":\"[]\",\"tokens\":[\"昨\",\"天\",\"是\",\"mon@@\",\"day\",\"today\",\"day\",\"is\",\"零\",\"八\",\"二\",\"the\",\"day\",\"after\",\"tom@@\",\"or@@\",\"row\",\"是\",\"星\",\"期\",\"三\"]}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the C++ streaming WebSocket is able to handle multiple client connecting at the same time."
      ],
      "metadata": {
        "id": "CAMe7cIl_zfV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start the Python C++ streaming websocket server"
      ],
      "metadata": {
        "id": "7G3THoIr_9qk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The server code is contained in the following folder:\n",
        "\n",
        "https://github.com/k2-fsa/sherpa-onnx/tree/master/python-api-examples\n",
        "\n",
        "It consits of:\n",
        " - [streaming_server.py](https://github.com/k2-fsa/sherpa-onnx/blob/master/python-api-examples/streaming_server.py); it contains a WebSocker server and a simple HTTP server. The two servers share the same port\n",
        "\n",
        " - [web](https://github.com/k2-fsa/sherpa-onnx/tree/master/python-api-examples/web); it contains HTML files so that users can use a browser to interact with the server."
      ],
      "metadata": {
        "id": "4gdzzvHNAA1t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of using `wget`, we use `git clone` below to download the code:"
      ],
      "metadata": {
        "id": "iYQCGFTyAjoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "git clone https://github.com/k2-fsa/sherpa-onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3twj5lj_kCQ",
        "outputId": "6e801b31-d167-4d9f-b709-756a6a8aca3e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'sherpa-onnx'...\n",
            "remote: Enumerating objects: 2855, done.\u001b[K\n",
            "remote: Counting objects: 100% (1332/1332), done.\u001b[K\n",
            "remote: Compressing objects: 100% (532/532), done.\u001b[K\n",
            "remote: Total 2855 (delta 915), reused 1006 (delta 780), pack-reused 1523\u001b[K\n",
            "Receiving objects: 100% (2855/2855), 2.02 MiB | 13.79 MiB/s, done.\n",
            "Resolving deltas: 100% (1647/1647), done.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can start the Python streaming websocket server:"
      ],
      "metadata": {
        "id": "6R7IXor4A6KL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "nohup python3 ./sherpa-onnx/python-api-examples/streaming_server.py \\\n",
        "  --tokens=./sherpa-onnx-streaming-paraformer-bilingual-zh-en/tokens.txt \\\n",
        "  --paraformer-encoder=./sherpa-onnx-streaming-paraformer-bilingual-zh-en/encoder.int8.onnx \\\n",
        "  --paraformer-decoder=./sherpa-onnx-streaming-paraformer-bilingual-zh-en/decoder.int8.onnx \\\n",
        "  --port=6008 \\\n",
        "  --doc-root=./sherpa-onnx/python-api-examples/web > python-server-log.txt &\n",
        "\n",
        "# sleep 10 seconds to wait for the server startup\n",
        "sleep 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xVSmCBHA5CH",
        "outputId": "696ebd40-eec7-407c-de88-02f546174ee1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: redirecting stderr to stdout\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "cat ./python-server-log.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-jhAjM8BM3p",
        "outputId": "319ff8a3-c940-4977-88bf-bb79eee167ea"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-14 13:04:16,659 INFO [streaming_server.py:719] {'encoder': None, 'decoder': None, 'joiner': None, 'paraformer_encoder': './sherpa-onnx-streaming-paraformer-bilingual-zh-en/encoder.int8.onnx', 'paraformer_decoder': './sherpa-onnx-streaming-paraformer-bilingual-zh-en/decoder.int8.onnx', 'tokens': './sherpa-onnx-streaming-paraformer-bilingual-zh-en/tokens.txt', 'sample_rate': 16000, 'feat_dim': 80, 'provider': 'cpu', 'decoding_method': 'greedy_search', 'num_active_paths': 4, 'use_endpoint': 1, 'rule1_min_trailing_silence': 2.4, 'rule2_min_trailing_silence': 1.2, 'rule3_min_utterance_length': 20, 'port': 6008, 'nn_pool_size': 1, 'max_batch_size': 50, 'max_wait_ms': 10, 'max_message_size': 1048576, 'max_queue_size': 32, 'max_active_connections': 500, 'num_threads': 2, 'certificate': None, 'doc_root': './sherpa-onnx/python-api-examples/web'}\n",
            "2023-08-14 13:04:21,357 INFO [streaming_server.py:547] No certificate provided\n",
            "2023-08-14 13:04:21,359 INFO [server.py:711] server listening on 0.0.0.0:6008\n",
            "2023-08-14 13:04:21,359 INFO [server.py:711] server listening on [::]:6008\n",
            "2023-08-14 13:04:21,359 INFO [streaming_server.py:573] Please visit one of the following addresses:\n",
            "\n",
            "  http://localhost:6008\n",
            "\n",
            "Since you are not providing a certificate, you cannot use your microphone from within the browser using public IP addresses. Only localhost can be used.You also cannot use 0.0.0.0 or 127.0.0.1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can visit the address\n",
        "  http://localhost:6008\n",
        "on your browser.\n",
        "\n",
        "Or you can use a websocket client to interact with the server:"
      ],
      "metadata": {
        "id": "sqDo14LgBpm0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start a C++ webscoket client to decode files"
      ],
      "metadata": {
        "id": "eNwspNYjCrdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "sherpa-onnx-online-websocket-client \\\n",
        "  --server-ip=127.0.0.1  \\\n",
        "  --server-port=6008 \\\n",
        "  sherpa-onnx-streaming-paraformer-bilingual-zh-en/test_wavs/0.wav"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JKb_ra5Cvo_",
        "outputId": "aeae8162-402a-47c7-c062-49b68d555b13"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/project/sherpa-onnx/csrc/parse-options.cc:Read:361 sherpa-onnx-online-websocket-client --server-ip=127.0.0.1 --server-port=6008 sherpa-onnx-streaming-paraformer-bilingual-zh-en/test_wavs/0.wav \n",
            "\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:SendMessage:114 Starting to send audio\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:OnMessage:91 {\"text\": \"\", \"segment\": 0}\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:OnMessage:91 {\"text\": \"\\u6628\", \"segment\": 0}\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:OnMessage:91 {\"text\": \"\\u6628\\u5929\\u662f\", \"segment\": 0}\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:OnMessage:91 {\"text\": \"\\u6628\\u5929\\u662f mon\", \"segment\": 0}\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:OnMessage:91 {\"text\": \"\\u6628\\u5929\\u662f monday\", \"segment\": 0}\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:OnMessage:91 {\"text\": \"\\u6628\\u5929\\u662f monday\", \"segment\": 0}\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:OnMessage:91 {\"text\": \"\\u6628\\u5929\\u662f monday today\", \"segment\": 0}\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:OnMessage:91 {\"text\": \"\\u6628\\u5929\\u662f monday today day\", \"segment\": 0}\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:OnMessage:91 {\"text\": \"\\u6628\\u5929\\u662f monday today day is\", \"segment\": 0}\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:OnMessage:91 {\"text\": \"\\u6628\\u5929\\u662f monday today day is \\u96f6\\u516b\", \"segment\": 0}\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:OnMessage:91 {\"text\": \"\\u6628\\u5929\\u662f monday today day is \\u96f6\\u516b\\u4e8c\", \"segment\": 0}\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:OnMessage:91 {\"text\": \"\\u6628\\u5929\\u662f monday today day is \\u96f6\\u516b\\u4e8c the day\", \"segment\": 0}\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:SendMessage:151 Sent Done Signal\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:OnMessage:91 {\"text\": \"\\u6628\\u5929\\u662f monday today day is \\u96f6\\u516b\\u4e8c the day after\", \"segment\": 0}\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:OnMessage:91 {\"text\": \"\\u6628\\u5929\\u662f monday today day is \\u96f6\\u516b\\u4e8c the day after tomor\", \"segment\": 0}\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:OnMessage:91 {\"text\": \"\\u6628\\u5929\\u662f monday today day is \\u96f6\\u516b\\u4e8c the day after tomorrow\", \"segment\": 0}\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:OnMessage:91 {\"text\": \"\\u6628\\u5929\\u662f monday today day is \\u96f6\\u516b\\u4e8c the day after tomorrow \\u662f\\u661f\", \"segment\": 0}\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:OnMessage:91 {\"text\": \"\\u6628\\u5929\\u662f monday today day is \\u96f6\\u516b\\u4e8c the day after tomorrow \\u662f\\u661f\\u671f\\u4e09\", \"segment\": 0}\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:operator():54 Disconnected\n",
            "/project/sherpa-onnx/csrc/online-websocket-client.cc:main:272 Done!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\u6628\\u5929\\u662f monday today day is \\u96f6\\u516b\\u4e8c the day after tomorrow \\u662f\\u661f\\u671f\\u4e09')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jSOHSe3C06I",
        "outputId": "ecb44906-909e-44e2-a83a-2db357d33da0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "昨天是 monday today day is 零八二 the day after tomorrow 是星期三\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start a Python webscoket client to decode files"
      ],
      "metadata": {
        "id": "aRM8q3h1Cr0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "# Note that the python server is listening on port 6008\n",
        "\n",
        "python3 ./online-websocket-client-decode-file.py \\\n",
        "  --server-addr localhost \\\n",
        "  --server-port 6008 \\\n",
        "  sherpa-onnx-streaming-paraformer-bilingual-zh-en/test_wavs/0.wav"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UffW89xBoao",
        "outputId": "29b767ba-7b5b-46a0-b558-6336d3e2339b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-14 13:04:31,329 INFO [online-websocket-client-decode-file.py:159] {'server_addr': 'localhost', 'server_port': 6008, 'samples_per_message': 8000, 'seconds_per_message': 0.1, 'sound_file': 'sherpa-onnx-streaming-paraformer-bilingual-zh-en/test_wavs/0.wav'}\n",
            "2023-08-14 13:04:31,339 INFO [online-websocket-client-decode-file.py:133] Sending sherpa-onnx-streaming-paraformer-bilingual-zh-en/test_wavs/0.wav\n",
            "2023-08-14 13:04:31,599 INFO [online-websocket-client-decode-file.py:115] {\"text\": \"\", \"segment\": 0}\n",
            "2023-08-14 13:04:31,808 INFO [online-websocket-client-decode-file.py:115] {\"text\": \"\\u6628\", \"segment\": 0}\n",
            "2023-08-14 13:04:32,027 INFO [online-websocket-client-decode-file.py:115] {\"text\": \"\\u6628\\u5929\\u662f\", \"segment\": 0}\n",
            "2023-08-14 13:04:32,278 INFO [online-websocket-client-decode-file.py:115] {\"text\": \"\\u6628\\u5929\\u662f mon\", \"segment\": 0}\n",
            "2023-08-14 13:04:32,879 INFO [online-websocket-client-decode-file.py:115] {\"text\": \"\\u6628\\u5929\\u662f monday\", \"segment\": 0}\n",
            "2023-08-14 13:04:33,104 INFO [online-websocket-client-decode-file.py:115] {\"text\": \"\\u6628\\u5929\\u662f monday\", \"segment\": 0}\n",
            "2023-08-14 13:04:33,319 INFO [online-websocket-client-decode-file.py:115] {\"text\": \"\\u6628\\u5929\\u662f monday today\", \"segment\": 0}\n",
            "2023-08-14 13:04:33,569 INFO [online-websocket-client-decode-file.py:115] {\"text\": \"\\u6628\\u5929\\u662f monday today day\", \"segment\": 0}\n",
            "2023-08-14 13:04:33,833 INFO [online-websocket-client-decode-file.py:115] {\"text\": \"\\u6628\\u5929\\u662f monday today day is\", \"segment\": 0}\n",
            "2023-08-14 13:04:34,257 INFO [online-websocket-client-decode-file.py:115] {\"text\": \"\\u6628\\u5929\\u662f monday today day is \\u96f6\\u516b\", \"segment\": 0}\n",
            "2023-08-14 13:04:34,778 INFO [online-websocket-client-decode-file.py:115] {\"text\": \"\\u6628\\u5929\\u662f monday today day is \\u96f6\\u516b\\u4e8c\", \"segment\": 0}\n",
            "2023-08-14 13:04:35,338 INFO [online-websocket-client-decode-file.py:115] {\"text\": \"\\u6628\\u5929\\u662f monday today day is \\u96f6\\u516b\\u4e8c the day\", \"segment\": 0}\n",
            "2023-08-14 13:04:35,930 INFO [online-websocket-client-decode-file.py:115] {\"text\": \"\\u6628\\u5929\\u662f monday today day is \\u96f6\\u516b\\u4e8c the day after\", \"segment\": 0}\n",
            "2023-08-14 13:04:36,419 INFO [online-websocket-client-decode-file.py:115] {\"text\": \"\\u6628\\u5929\\u662f monday today day is \\u96f6\\u516b\\u4e8c the day after tomor\", \"segment\": 0}\n",
            "2023-08-14 13:04:36,900 INFO [online-websocket-client-decode-file.py:115] {\"text\": \"\\u6628\\u5929\\u662f monday today day is \\u96f6\\u516b\\u4e8c the day after tomorrow\", \"segment\": 0}\n",
            "2023-08-14 13:04:37,109 INFO [online-websocket-client-decode-file.py:115] {\"text\": \"\\u6628\\u5929\\u662f monday today day is \\u96f6\\u516b\\u4e8c the day after tomorrow \\u662f\\u661f\", \"segment\": 0}\n",
            "2023-08-14 13:04:37,337 INFO [online-websocket-client-decode-file.py:115] {\"text\": \"\\u6628\\u5929\\u662f monday today day is \\u96f6\\u516b\\u4e8c the day after tomorrow \\u662f\\u661f\\u671f\\u4e09\", \"segment\": 0}\n",
            "2023-08-14 13:04:37,338 INFO [online-websocket-client-decode-file.py:154] \n",
            "Final result is:\n",
            "{\"text\": \"\\u6628\\u5929\\u662f monday today day is \\u96f6\\u516b\\u4e8c the day after tomorrow \\u662f\\u661f\\u671f\\u4e09\", \"segment\": 0}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\u6628\\u5929\\u662f monday today day is \\u96f6\\u516b\\u4e8c the day after tomorrow \\u662f\\u661f\\u671f\\u4e09')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suyR4ycdB4rK",
        "outputId": "f54f4b8f-8758-4050-d8aa-78b04155ef07"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "昨天是 monday today day is 零八二 the day after tomorrow 是星期三\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Speech recognition from microphones"
      ],
      "metadata": {
        "id": "6wdlcmTYD67p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Websocket client with a microphone"
      ],
      "metadata": {
        "id": "HM7sBKPzD9zE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "sudo apt-get install libasound-dev portaudio19-dev libportaudio2 libportaudiocpp0\n",
        "pip install sounddevice\n",
        "\n",
        "python3 ./sherpa-onnx/python-api-examples/online-websocket-client-microphone.py --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XGZvrhxECQ6",
        "outputId": "77c22369-32cc-4d47-cb11-84a2504ec38a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Note, selecting 'libasound2-dev' instead of 'libasound-dev'\n",
            "libasound2-dev is already the newest version (1.2.6.1-1ubuntu1).\n",
            "Suggested packages:\n",
            "  portaudio19-doc\n",
            "The following NEW packages will be installed:\n",
            "  libportaudio2 libportaudiocpp0 portaudio19-dev\n",
            "0 upgraded, 3 newly installed, 0 to remove and 16 not upgraded.\n",
            "Need to get 188 kB of archives.\n",
            "After this operation, 927 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudio2 amd64 19.6.0-1.1 [65.3 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudiocpp0 amd64 19.6.0-1.1 [16.1 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 portaudio19-dev amd64 19.6.0-1.1 [106 kB]\n",
            "Fetched 188 kB in 0s (470 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "(Reading database ... 120828 files and directories currently installed.)\n",
            "Preparing to unpack .../libportaudio2_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package libportaudiocpp0:amd64.\n",
            "Preparing to unpack .../libportaudiocpp0_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudiocpp0:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package portaudio19-dev:amd64.\n",
            "Preparing to unpack .../portaudio19-dev_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking portaudio19-dev:amd64 (19.6.0-1.1) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Setting up libportaudiocpp0:amd64 (19.6.0-1.1) ...\n",
            "Setting up portaudio19-dev:amd64 (19.6.0-1.1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "Collecting sounddevice\n",
            "  Downloading sounddevice-0.4.6-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice) (2.21)\n",
            "Installing collected packages: sounddevice\n",
            "Successfully installed sounddevice-0.4.6\n",
            "usage: online-websocket-client-microphone.py\n",
            "       [-h]\n",
            "       [--server-addr SERVER_ADDR]\n",
            "       [--server-port SERVER_PORT]\n",
            "\n",
            "options:\n",
            "  -h, --help\n",
            "    show this\n",
            "    help\n",
            "    message and\n",
            "    exit\n",
            "  --server-addr SERVER_ADDR\n",
            "    Address of\n",
            "    the server\n",
            "    (default:\n",
            "    localhost)\n",
            "  --server-port SERVER_PORT\n",
            "    Port of the\n",
            "    server\n",
            "    (default:\n",
            "    6006)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standalone python script with a microphone"
      ],
      "metadata": {
        "id": "3ERiVY4-Eh9y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Without endpoint detection"
      ],
      "metadata": {
        "id": "epmcZiupHIDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "python3 ./sherpa-onnx/python-api-examples/speech-recognition-from-microphone.py --help\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnDUfQfxEkJk",
        "outputId": "5f1b6b30-5664-4558-c53e-c967d2785a8b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: speech-recognition-from-microphone.py\n",
            "       [-h]\n",
            "       --tokens\n",
            "       TOKENS\n",
            "       --encoder\n",
            "       ENCODER\n",
            "       --decoder\n",
            "       DECODER\n",
            "       [--joiner JOINER]\n",
            "       [--decoding-method DECODING_METHOD]\n",
            "       [--max-active-paths MAX_ACTIVE_PATHS]\n",
            "       [--provider PROVIDER]\n",
            "       [--bpe-model BPE_MODEL]\n",
            "       [--modeling-unit MODELING_UNIT]\n",
            "       [--contexts CONTEXTS]\n",
            "       [--context-score CONTEXT_SCORE]\n",
            "\n",
            "options:\n",
            "  -h, --help\n",
            "    show this\n",
            "    help\n",
            "    message and\n",
            "    exit\n",
            "  --tokens TOKENS\n",
            "    Path to\n",
            "    tokens.txt\n",
            "    (default:\n",
            "    None)\n",
            "  --encoder ENCODER\n",
            "    Path to the\n",
            "    encoder\n",
            "    model\n",
            "    (default:\n",
            "    None)\n",
            "  --decoder DECODER\n",
            "    Path to the\n",
            "    decoder\n",
            "    model\n",
            "    (default:\n",
            "    None)\n",
            "  --joiner JOINER\n",
            "    Path to the\n",
            "    joiner\n",
            "    model\n",
            "    (default:\n",
            "    None)\n",
            "  --decoding-method DECODING_METHOD\n",
            "    Valid\n",
            "    values are \n",
            "    greedy_sear\n",
            "    ch and modi\n",
            "    fied_beam_s\n",
            "    earch\n",
            "    (default: g\n",
            "    reedy_searc\n",
            "    h)\n",
            "  --max-active-paths MAX_ACTIVE_PATHS\n",
            "    Used only\n",
            "    when\n",
            "    --decoding-\n",
            "    method is m\n",
            "    odified_bea\n",
            "    m_search.\n",
            "    It\n",
            "    specifies\n",
            "    number of\n",
            "    active\n",
            "    paths to\n",
            "    keep during\n",
            "    decoding.\n",
            "    (default:\n",
            "    4)\n",
            "  --provider PROVIDER\n",
            "    Valid\n",
            "    values:\n",
            "    cpu, cuda,\n",
            "    coreml\n",
            "    (default:\n",
            "    cpu)\n",
            "  --bpe-model BPE_MODEL\n",
            "    Path to\n",
            "    bpe.model,\n",
            "    it will be\n",
            "    used to\n",
            "    tokenize\n",
            "    contexts\n",
            "    biasing\n",
            "    phrases.\n",
            "    Used only\n",
            "    when\n",
            "    --decoding-\n",
            "    method=modi\n",
            "    fied_beam_s\n",
            "    earch\n",
            "    (default: )\n",
            "  --modeling-unit MODELING_UNIT\n",
            "    The type of\n",
            "    modeling\n",
            "    unit, it\n",
            "    will be\n",
            "    used to\n",
            "    tokenize\n",
            "    contexts\n",
            "    biasing\n",
            "    phrases.\n",
            "    Valid\n",
            "    values are\n",
            "    bpe,\n",
            "    bpe+char,\n",
            "    char. Note:\n",
            "    the char\n",
            "    here means\n",
            "    characters\n",
            "    in CJK\n",
            "    languages.\n",
            "    Used only\n",
            "    when\n",
            "    --decoding-\n",
            "    method=modi\n",
            "    fied_beam_s\n",
            "    earch\n",
            "    (default:\n",
            "    char)\n",
            "  --contexts CONTEXTS\n",
            "    The context\n",
            "    list, it is\n",
            "    a string\n",
            "    containing\n",
            "    some words/\n",
            "    phrases\n",
            "    separated\n",
            "    with /, for\n",
            "    example,\n",
            "    'HELLO\n",
            "    WORLD/I\n",
            "    LOVE YOU/GO\n",
            "    AWAY\". Used\n",
            "    only when\n",
            "    --decoding-\n",
            "    method=modi\n",
            "    fied_beam_s\n",
            "    earch\n",
            "    (default: )\n",
            "  --context-score CONTEXT_SCORE\n",
            "    The context\n",
            "    score of\n",
            "    each token\n",
            "    for biasing\n",
            "    word/phrase\n",
            "    . Used only\n",
            "    if\n",
            "    --contexts\n",
            "    is given.\n",
            "    Used only\n",
            "    when\n",
            "    --decoding-\n",
            "    method=modi\n",
            "    fied_beam_s\n",
            "    earch\n",
            "    (default:\n",
            "    1.5)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### With endpoint detection"
      ],
      "metadata": {
        "id": "1fLdLm2UHMub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "python3 ./sherpa-onnx/python-api-examples/speech-recognition-from-microphone-with-endpoint-detection.py --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mZ0etI3HMIQ",
        "outputId": "8cf238ab-c493-488a-89f1-90833aa3bb61"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: speech-recognition-from-microphone-with-endpoint-detection.py\n",
            "       [-h]\n",
            "       --tokens\n",
            "       TOKENS\n",
            "       --encoder\n",
            "       ENCODER\n",
            "       --decoder\n",
            "       DECODER\n",
            "       --joiner\n",
            "       JOINER\n",
            "       [--decoding-method DECODING_METHOD]\n",
            "       [--provider PROVIDER]\n",
            "\n",
            "options:\n",
            "  -h, --help\n",
            "    show this\n",
            "    help\n",
            "    message and\n",
            "    exit\n",
            "  --tokens TOKENS\n",
            "    Path to\n",
            "    tokens.txt\n",
            "    (default:\n",
            "    None)\n",
            "  --encoder ENCODER\n",
            "    Path to the\n",
            "    encoder\n",
            "    model\n",
            "    (default:\n",
            "    None)\n",
            "  --decoder DECODER\n",
            "    Path to the\n",
            "    decoder\n",
            "    model\n",
            "    (default:\n",
            "    None)\n",
            "  --joiner JOINER\n",
            "    Path to the\n",
            "    joiner\n",
            "    model\n",
            "    (default:\n",
            "    None)\n",
            "  --decoding-method DECODING_METHOD\n",
            "    Valid\n",
            "    values are \n",
            "    greedy_sear\n",
            "    ch and modi\n",
            "    fied_beam_s\n",
            "    earch\n",
            "    (default: g\n",
            "    reedy_searc\n",
            "    h)\n",
            "  --provider PROVIDER\n",
            "    Valid\n",
            "    values:\n",
            "    cpu, cuda,\n",
            "    coreml\n",
            "    (default:\n",
            "    cpu)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decode files with a standalone Python script"
      ],
      "metadata": {
        "id": "6PuUjAmRDd0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "python3 ./sherpa-onnx/python-api-examples/online-decode-files.py \\\n",
        "  --tokens=./sherpa-onnx-streaming-paraformer-bilingual-zh-en/tokens.txt \\\n",
        "  --paraformer-encoder=./sherpa-onnx-streaming-paraformer-bilingual-zh-en/encoder.int8.onnx \\\n",
        "  --paraformer-decoder=./sherpa-onnx-streaming-paraformer-bilingual-zh-en/decoder.int8.onnx \\\n",
        "  ./sherpa-onnx-streaming-paraformer-bilingual-zh-en/test_wavs/0.wav"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQlZJjnFDgzN",
        "outputId": "96102c29-4ae7-4ed8-9a40-0a707b656c57"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started!\n",
            "Done!\n",
            "./sherpa-onnx-streaming-paraformer-bilingual-zh-en/test_wavs/0.wav\n",
            "昨天是 monday today day is 零八二 the day after tomorrow 是星期三\n",
            "----------\n",
            "num_threads: 1\n",
            "decoding_method: greedy_search\n",
            "Wave duration: 10.053 s\n",
            "Elapsed time: 4.420 s\n",
            "Real time factor (RTF): 4.420/10.053 = 0.440\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What's Next"
      ],
      "metadata": {
        "id": "wWTHblx-CAeg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try streaming Paraformer with Android\n",
        "\n",
        "Please refer to\n",
        "https://k2-fsa.github.io/sherpa/onnx/android/index.html\n"
      ],
      "metadata": {
        "id": "aDfBN_WlHUZh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try streaming Paraformer with iOS\n",
        "\n",
        "Please refer to\n",
        "https://k2-fsa.github.io/sherpa/onnx/ios/index.html"
      ],
      "metadata": {
        "id": "qW7ZFh2mHfx_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try streaming Paraformer with 64-bit ARM\n",
        "\n",
        "For instance, you can run streaming paraformer with\n",
        "Rapsberry Pi.\n",
        "\n",
        "Please see\n",
        "- Cross compiling: https://k2-fsa.github.io/sherpa/onnx/install/aarch64-embedded-linux.html\n",
        "- Direct compiling: https://k2-fsa.github.io/sherpa/onnx/install/linux.html\n"
      ],
      "metadata": {
        "id": "zUonK1JNHli2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try streaming Paraformer with 32-bit ARM\n",
        "\n",
        "Please see\n",
        "\n",
        "- Cross compiling: https://k2-fsa.github.io/sherpa/onnx/install/arm-embedded-linux.html\n",
        "- Direct compiling: https://k2-fsa.github.io/sherpa/onnx/install/linux.html\n"
      ],
      "metadata": {
        "id": "LJLep9LVH0oM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try streaming Paraformer on Windows with MFC\n",
        "\n",
        "- Please download the `exe` from https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.7.5/sherpa-onnx-streaming-v1.7.5.exe"
      ],
      "metadata": {
        "id": "B_X-7PXwIC84"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try streaming Paraformer with Go API\n",
        "\n",
        "Please see\n",
        "- https://k2-fsa.github.io/sherpa/onnx/go-api/index.html\n",
        "- https://github.com/k2-fsa/sherpa-onnx/tree/master/go-api-examples"
      ],
      "metadata": {
        "id": "KwvtvIlrIPkP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try streaming Paraformer with `C#` API\n",
        "\n",
        "Please see\n",
        "- https://k2-fsa.github.io/sherpa/onnx/csharp-api/index.html\n",
        "- https://github.com/k2-fsa/sherpa-onnx/tree/master/dotnet-examples"
      ],
      "metadata": {
        "id": "LJe_3gULIYac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try streaming Paraformer with C API\n",
        "\n",
        "Please see\n",
        "- https://k2-fsa.github.io/sherpa/onnx/c-api/index.html\n",
        "- https://github.com/k2-fsa/sherpa-onnx/tree/master/c-api-examples"
      ],
      "metadata": {
        "id": "ok47uCS3IjLa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try streaming Paraformer on GPU\n",
        "\n",
        "Please see\n",
        "\n",
        "https://github.com/k2-fsa/colab/blob/master/sherpa-onnx/sherpa_onnx_streaming_paraformer_gpu.ipynb\n"
      ],
      "metadata": {
        "id": "d4eXq4dXIsmO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M3xa-7i5B9hr"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}
