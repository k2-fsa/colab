{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaS0hjKeCRxyE2QEhIBDve",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/k2-fsa/colab/blob/master/sherpa-onnx/Run_distil_whisper_with_sherpa_onnx_on_CPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook show how to run models from\n",
        "https://github.com/huggingface/distil-whisper\n",
        "with [sherpa-onnx](https://github.com/k2-fsa/sherpa-onnx)."
      ],
      "metadata": {
        "id": "a_zc7ABqAmA4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install sherpa-onnx"
      ],
      "metadata": {
        "id": "n_-oof4dAzM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "# All you need is\n",
        "\n",
        "pip install sherpa-onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jz0rGh1mAyZY",
        "outputId": "8046c2de-df6b-4a1b-ce71-bcfa57143432"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sherpa-onnx\n",
            "  Downloading sherpa_onnx-1.8.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sherpa-onnx) (1.23.5)\n",
            "Requirement already satisfied: click>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from sherpa-onnx) (8.1.7)\n",
            "Collecting sentencepiece==0.1.96 (from sherpa-onnx)\n",
            "  Downloading sentencepiece-0.1.96-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece, sherpa-onnx\n",
            "Successfully installed sentencepiece-0.1.96 sherpa-onnx-1.8.7\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download model files\n",
        "\n",
        "We have exported the model to onnx and saved the model files in\n",
        "the repo\n",
        "https://huggingface.co/csukuangfj/sherpa-onnx-whisper-distil-medium.en/tree/main"
      ],
      "metadata": {
        "id": "X3yEU29QA8Q8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_6RcPCUAdN0",
        "outputId": "b2b8be44-eb7f-41c0-efbd-8d2bf55b851d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 548M\n",
            "-rw-r--r-- 1 root root 208K Nov  6 14:23 0.wav\n",
            "-rw-r--r-- 1 root root 523K Nov  6 14:23 1.wav\n",
            "-rw-r--r-- 1 root root  76K Nov  6 14:23 8k.wav\n",
            "-rw-r--r-- 1 root root 234M Nov  6 12:16 distil-medium.en-decoder.int8.onnx\n",
            "-rw-r--r-- 1 root root 313M Nov  6 12:32 distil-medium.en-encoder.int8.onnx\n",
            "-rw-r--r-- 1 root root 816K Nov  6 14:23 distil-medium.en-tokens.txt\n",
            "drwxr-xr-x 1 root root 4.0K Nov  3 18:00 sample_data\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "%%shell\n",
        "wget -q https://huggingface.co/csukuangfj/sherpa-onnx-whisper-distil-medium.en/resolve/main/distil-medium.en-decoder.int8.onnx\n",
        "wget -q https://huggingface.co/csukuangfj/sherpa-onnx-whisper-distil-medium.en/resolve/main/distil-medium.en-encoder.int8.onnx\n",
        "wget -q https://huggingface.co/csukuangfj/sherpa-onnx-whisper-distil-medium.en/resolve/main/distil-medium.en-tokens.txt\n",
        "wget -q https://huggingface.co/csukuangfj/sherpa-onnx-whisper-distil-medium.en/resolve/main/test_wavs/0.wav\n",
        "wget -q https://huggingface.co/csukuangfj/sherpa-onnx-whisper-distil-medium.en/resolve/main/test_wavs/1.wav\n",
        "wget -q https://huggingface.co/csukuangfj/sherpa-onnx-whisper-distil-medium.en/resolve/main/test_wavs/8k.wav\n",
        "ls -lh"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run it"
      ],
      "metadata": {
        "id": "y_GspI_bBLZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "sherpa-onnx-offline \\\n",
        "  --whisper-encoder=./distil-medium.en-encoder.int8.onnx \\\n",
        "  --whisper-decoder=./distil-medium.en-decoder.int8.onnx \\\n",
        "  --tokens=./distil-medium.en-tokens.txt \\\n",
        "  ./0.wav \\\n",
        "  ./1.wav \\\n",
        "  ./8k.wav"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dK1LLgBFBJhV",
        "outputId": "3ea39e3a-cbc8-4ea6-ac02-378f8b1a7a63"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/project/sherpa-onnx/csrc/parse-options.cc:Read:361 sherpa-onnx-offline --whisper-encoder=./distil-medium.en-encoder.int8.onnx --whisper-decoder=./distil-medium.en-decoder.int8.onnx --tokens=./distil-medium.en-tokens.txt ./0.wav ./1.wav ./8k.wav \n",
            "\n",
            "OfflineRecognizerConfig(feat_config=OfflineFeatureExtractorConfig(sampling_rate=16000, feature_dim=80), model_config=OfflineModelConfig(transducer=OfflineTransducerModelConfig(encoder_filename=\"\", decoder_filename=\"\", joiner_filename=\"\"), paraformer=OfflineParaformerModelConfig(model=\"\"), nemo_ctc=OfflineNemoEncDecCtcModelConfig(model=\"\"), whisper=OfflineWhisperModelConfig(encoder=\"./distil-medium.en-encoder.int8.onnx\", decoder=\"./distil-medium.en-decoder.int8.onnx\", language=\"\", task=\"transcribe\"), tdnn=OfflineTdnnModelConfig(model=\"\"), zipformer_ctc=OfflineZipformerCtcModelConfig(model=\"\"), tokens=\"./distil-medium.en-tokens.txt\", num_threads=2, debug=False, provider=\"cpu\", model_type=\"\"), lm_config=OfflineLMConfig(model=\"\", scale=0.5), ctc_fst_decoder_config=OfflineCtcFstDecoderConfig(graph=\"\", max_active=3000), decoding_method=\"greedy_search\", max_active_paths=4, hotwords_file=\"\", hotwords_score=1.5)\n",
            "Creating recognizer ...\n",
            "Started\n",
            "/project/sherpa-onnx/csrc/offline-stream.cc:AcceptWaveformImpl:113 Creating a resampler:\n",
            "   in_sample_rate: 8000\n",
            "   output_sample_rate: 16000\n",
            "\n",
            "Done!\n",
            "\n",
            "./0.wav\n",
            "{\"text\": \" After early nightfall, the yellow lamps would light up here and there the squalid quarter of the brothels.\", \"timestamps\": [], \"tokens\":[\" After\", \" early\", \" night\", \"fall\", \",\", \" the\", \" yellow\", \" lamps\", \" would\", \" light\", \" up\", \" here\", \" and\", \" there\", \" the\", \" squ\", \"alid\", \" quarter\", \" of\", \" the\", \" bro\", \"the\", \"ls\", \".\"]}\n",
            "----\n",
            "./1.wav\n",
            "{\"text\": \" God, as a direct consequence of the sin which man thus punished, had given her a lovely child whose place was on that same dishonored bosom to connect her parent forever with the race and descent of mortals, and to be finally a blessed soul in heaven.\", \"timestamps\": [], \"tokens\":[\" God\", \",\", \" as\", \" a\", \" direct\", \" consequence\", \" of\", \" the\", \" sin\", \" which\", \" man\", \" thus\", \" punished\", \",\", \" had\", \" given\", \" her\", \" a\", \" lovely\", \" child\", \" whose\", \" place\", \" was\", \" on\", \" that\", \" same\", \" dishon\", \"ored\", \" bos\", \"om\", \" to\", \" connect\", \" her\", \" parent\", \" forever\", \" with\", \" the\", \" race\", \" and\", \" descent\", \" of\", \" mortals\", \",\", \" and\", \" to\", \" be\", \" finally\", \" a\", \" blessed\", \" soul\", \" in\", \" heaven\", \".\"]}\n",
            "----\n",
            "./8k.wav\n",
            "{\"text\": \" Yet these thoughts affected Hester Pren less with hope than apprehension.\", \"timestamps\": [], \"tokens\":[\" Yet\", \" these\", \" thoughts\", \" affected\", \" H\", \"ester\", \" P\", \"ren\", \" less\", \" with\", \" hope\", \" than\", \" apprehension\", \".\"]}\n",
            "----\n",
            "num threads: 2\n",
            "decoding method: greedy_search\n",
            "Elapsed seconds: 49.245 s\n",
            "Real time factor (RTF): 49.245 / 28.165 = 1.748\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What's more\n",
        "\n",
        "You can find detailed documentation at\n",
        "https://k2-fsa.github.io/sherpa/onnx/index.html"
      ],
      "metadata": {
        "id": "PruDELKZBRH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "which sherpa-onnx-offline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-0Dszw0BPXW",
        "outputId": "5c564c5a-bee0-4aea-882b-f10fac790710"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/bin/sherpa-onnx-offline\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "file /usr/local/bin/sherpa-onnx-offline\n",
        "ldd /usr/local/bin/sherpa-onnx-offline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGZNtg8CBsEX",
        "outputId": "66f97e9a-9abc-405c-f688-c1077d2a4a50"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/bin/sherpa-onnx-offline: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, for GNU/Linux 2.6.32, BuildID[sha1]=04a1c996936a13ead6b1c76de8f275eaf495d538, stripped\n",
            "\tlinux-vdso.so.1 (0x00007ffe17388000)\n",
            "\tlibsherpa-onnx-core.so => /usr/local/bin/../lib/python3.10/dist-packages/sherpa_onnx/lib/libsherpa-onnx-core.so (0x00007a0258872000)\n",
            "\tlibkaldi-native-fbank-core.so => /usr/local/bin/../lib/python3.10/dist-packages/sherpa_onnx/lib/libkaldi-native-fbank-core.so (0x00007a025883d000)\n",
            "\tlibkaldi-decoder-core.so => /usr/local/bin/../lib/python3.10/dist-packages/sherpa_onnx/lib/libkaldi-decoder-core.so (0x00007a0258823000)\n",
            "\tlibsherpa-onnx-kaldifst-core.so => /usr/local/bin/../lib/python3.10/dist-packages/sherpa_onnx/lib/libsherpa-onnx-kaldifst-core.so (0x00007a02587ca000)\n",
            "\tlibsherpa-onnx-fst.so.6 => /usr/local/bin/../lib/python3.10/dist-packages/sherpa_onnx/lib/libsherpa-onnx-fst.so.6 (0x00007a0258640000)\n",
            "\tlibonnxruntime.so.1.16.0 => /usr/local/bin/../lib/python3.10/dist-packages/sherpa_onnx/lib/libonnxruntime.so.1.16.0 (0x00007a02575c5000)\n",
            "\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007a0257383000)\n",
            "\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007a025729c000)\n",
            "\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007a025727c000)\n",
            "\tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007a0257275000)\n",
            "\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007a025704d000)\n",
            "\tlibdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007a0257048000)\n",
            "\tlibrt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007a0257043000)\n",
            "\t/lib64/ld-linux-x86-64.so.2 (0x00007a02589af000)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "sherpa-onnx-offline --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5JfYZyPBbt_",
        "outputId": "e15dd188-01a8-4ec0-c7b1-ce0553dc6927"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/project/sherpa-onnx/csrc/parse-options.cc:PrintUsage:402 \n",
            "\n",
            "Speech recognition using non-streaming models with sherpa-onnx.\n",
            "\n",
            "Usage:\n",
            "\n",
            "(1) Transducer from icefall\n",
            "\n",
            "See https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-transducer/index.html\n",
            "\n",
            "  ./bin/sherpa-onnx-offline \\\n",
            "    --tokens=/path/to/tokens.txt \\\n",
            "    --encoder=/path/to/encoder.onnx \\\n",
            "    --decoder=/path/to/decoder.onnx \\\n",
            "    --joiner=/path/to/joiner.onnx \\\n",
            "    --num-threads=1 \\\n",
            "    --decoding-method=greedy_search \\\n",
            "    /path/to/foo.wav [bar.wav foobar.wav ...]\n",
            "\n",
            "\n",
            "(2) Paraformer from FunASR\n",
            "\n",
            "See https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-paraformer/index.html\n",
            "\n",
            "  ./bin/sherpa-onnx-offline \\\n",
            "    --tokens=/path/to/tokens.txt \\\n",
            "    --paraformer=/path/to/model.onnx \\\n",
            "    --num-threads=1 \\\n",
            "    --decoding-method=greedy_search \\\n",
            "    /path/to/foo.wav [bar.wav foobar.wav ...]\n",
            "\n",
            "(3) Whisper models\n",
            "\n",
            "See https://k2-fsa.github.io/sherpa/onnx/pretrained_models/whisper/tiny.en.html\n",
            "\n",
            "  ./bin/sherpa-onnx-offline \\\n",
            "    --whisper-encoder=./sherpa-onnx-whisper-base.en/base.en-encoder.int8.onnx \\\n",
            "    --whisper-decoder=./sherpa-onnx-whisper-base.en/base.en-decoder.int8.onnx \\\n",
            "    --tokens=./sherpa-onnx-whisper-base.en/base.en-tokens.txt \\\n",
            "    --num-threads=1 \\\n",
            "    /path/to/foo.wav [bar.wav foobar.wav ...]\n",
            "\n",
            "(4) NeMo CTC models\n",
            "\n",
            "See https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-ctc/index.html\n",
            "\n",
            "  ./bin/sherpa-onnx-offline \\\n",
            "    --tokens=./sherpa-onnx-nemo-ctc-en-conformer-medium/tokens.txt \\\n",
            "    --nemo-ctc-model=./sherpa-onnx-nemo-ctc-en-conformer-medium/model.onnx \\\n",
            "    --num-threads=2 \\\n",
            "    --decoding-method=greedy_search \\\n",
            "    --debug=false \\\n",
            "    ./sherpa-onnx-nemo-ctc-en-conformer-medium/test_wavs/0.wav \\\n",
            "    ./sherpa-onnx-nemo-ctc-en-conformer-medium/test_wavs/1.wav \\\n",
            "    ./sherpa-onnx-nemo-ctc-en-conformer-medium/test_wavs/8k.wav\n",
            "\n",
            "(5) TDNN CTC model for the yesno recipe from icefall\n",
            "\n",
            "See https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-ctc/yesno/index.html\n",
            "      //\n",
            "  ./build/bin/sherpa-onnx-offline \\\n",
            "    --sample-rate=8000 \\\n",
            "    --feat-dim=23 \\\n",
            "    --tokens=./sherpa-onnx-tdnn-yesno/tokens.txt \\\n",
            "    --tdnn-model=./sherpa-onnx-tdnn-yesno/model-epoch-14-avg-2.onnx \\\n",
            "    ./sherpa-onnx-tdnn-yesno/test_wavs/0_0_0_1_0_0_0_1.wav \\\n",
            "    ./sherpa-onnx-tdnn-yesno/test_wavs/0_0_1_0_0_0_1_0.wav\n",
            "\n",
            "Note: It supports decoding multiple files in batches\n",
            "\n",
            "foo.wav should be of single channel, 16-bit PCM encoded wave file; its\n",
            "sampling rate can be arbitrary and does not need to be 16kHz.\n",
            "\n",
            "Please refer to\n",
            "https://k2-fsa.github.io/sherpa/onnx/pretrained_models/index.html\n",
            "for a list of pre-trained models to download.\n",
            "\n",
            "Options:\n",
            "  --hotwords-file             : The file containing hotwords, one words/phrases per line, and for eachphrase the bpe/cjkchar are separated by a space. For example: ▁HE LL O ▁WORLD你 好 世 界 (string, default = \"\")\n",
            "  --whisper-language          : The spoke language in the input audio file. Example values: en, de, fr, zh, jp. If it is not given for a multilingual model, we will infer the language from the input audio file. Please refer to https://github.com/openai/whisper/blob/main/whisper/tokenizer.py#L10 for valid values. Note that for non-multilingual models, it supports only 'en' (string, default = \"\")\n",
            "  --whisper-decoder           : Path to onnx decoder of whisper, e.g., tiny-decoder.onnx, medium.en-decoder.onnx. (string, default = \"\")\n",
            "  --paraformer                : Path to model.onnx of paraformer. (string, default = \"\")\n",
            "  --num-threads               : Number of threads to run the neural network (int, default = 2)\n",
            "  --encoder                   : Path to encoder.onnx (string, default = \"\")\n",
            "  --sample-rate               : Sampling rate of the input waveform. Note: You can have a different sample rate for the input waveform. We will do resampling inside the feature extractor (int, default = 16000)\n",
            "  --joiner                    : Path to joiner.onnx (string, default = \"\")\n",
            "  --provider                  : Specify a provider to use: cpu, cuda, coreml (string, default = \"cpu\")\n",
            "  --ctc.graph                 : Path to H.fst, HL.fst, or HLG.fst (string, default = \"\")\n",
            "  --model-type                : Specify it to reduce model initialization time. Valid values are: transducer, paraformer, nemo_ctc, whisper, tdnn, zipformer2_ctcAll other values lead to loading the model twice. (string, default = \"\")\n",
            "  --whisper-encoder           : Path to onnx encoder of whisper, e.g., tiny-encoder.onnx, medium.en-encoder.onnx. (string, default = \"\")\n",
            "  --nemo-ctc-model            : Path to model.onnx of Nemo EncDecCtcModel. (string, default = \"\")\n",
            "  --hotwords-score            : The bonus score for each token in context word/phrase. Used only when decoding_method is modified_beam_search (float, default = 1.5)\n",
            "  --feat-dim                  : Feature dimension. Must match the one expected by the model. (int, default = 80)\n",
            "  --decoder                   : Path to decoder.onnx (string, default = \"\")\n",
            "  --debug                     : true to print model information while loading it. (bool, default = false)\n",
            "  --lm-num-threads            : Number of threads to run the neural network of LM model (int, default = 1)\n",
            "  --ctc.max-active            : Decoder max active states.  Larger->slower; more accurate (int, default = 3000)\n",
            "  --whisper-task              : Valid values: transcribe, translate. Note that for non-multilingual models, it supports only 'transcribe' (string, default = \"transcribe\")\n",
            "  --tdnn-model                : Path to onnx model (string, default = \"\")\n",
            "  --lm-provider               : Specify a provider to LM model use: cpu, cuda, coreml (string, default = \"cpu\")\n",
            "  --zipformer-ctc-model       : Path to zipformer CTC model (string, default = \"\")\n",
            "  --decoding-method           : decoding method,Valid values: greedy_search, modified_beam_search. modified_beam_search is applicable only for transducer models. (string, default = \"greedy_search\")\n",
            "  --tokens                    : Path to tokens.txt (string, default = \"\")\n",
            "  --lm                        : Path to LM model. (string, default = \"\")\n",
            "  --lm-scale                  : LM scale. (float, default = 0.5)\n",
            "  --max-active-paths          : Used only when decoding_method is modified_beam_search (int, default = 4)\n",
            "\n",
            "Standard options:\n",
            "  --print-args                : Print the command line arguments (to stderr) (bool, default = true)\n",
            "  --help                      : Print out usage message (bool, default = false)\n",
            "  --config                    : Configuration file to read (this option may be repeated) (string, default = \"\")\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "ls -lh /usr/local/bin/sherpa-onnx*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uI5-SHkBhvL",
        "outputId": "fd880af2-39ba-48f9-bfd4-62b8fd2a0f13"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rwxr-xr-x 1 root root  67K Nov  6 14:22 /usr/local/bin/sherpa-onnx\n",
            "-rwxr-xr-x 1 root root  212 Nov  6 14:22 /usr/local/bin/sherpa-onnx-cli\n",
            "-rwxr-xr-x 1 root root  39K Nov  6 14:22 /usr/local/bin/sherpa-onnx-microphone\n",
            "-rwxr-xr-x 1 root root  47K Nov  6 14:22 /usr/local/bin/sherpa-onnx-microphone-offline\n",
            "-rwxr-xr-x 1 root root  43K Nov  6 14:22 /usr/local/bin/sherpa-onnx-offline\n",
            "-rwxr-xr-x 1 root root  31K Nov  6 14:22 /usr/local/bin/sherpa-onnx-offline-tts\n",
            "-rwxr-xr-x 1 root root 497K Nov  6 14:22 /usr/local/bin/sherpa-onnx-offline-websocket-server\n",
            "-rwxr-xr-x 1 root root 485K Nov  6 14:22 /usr/local/bin/sherpa-onnx-online-websocket-client\n",
            "-rwxr-xr-x 1 root root 505K Nov  6 14:22 /usr/local/bin/sherpa-onnx-online-websocket-server\n",
            "-rwxr-xr-x 1 root root  35K Nov  6 14:22 /usr/local/bin/sherpa-onnx-vad-microphone\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "sherpa-onnx-offline-tts --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6mjQCTBBmlQ",
        "outputId": "2fdeb598-4aa6-4f99-e393-096d94215000"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/project/sherpa-onnx/csrc/parse-options.cc:PrintUsage:402 \n",
            "\n",
            "Offline text-to-speech with sherpa-onnx\n",
            "\n",
            "./bin/sherpa-onnx-offline-tts \\\n",
            " --vits-model=/path/to/model.onnx \\\n",
            " --vits-lexicon=/path/to/lexicon.txt \\\n",
            " --vits-tokens=/path/to/tokens.txt \\\n",
            " --sid=0 \\\n",
            " --output-filename=./generated.wav \\\n",
            " 'some text within single quotes on linux/macos or use double quotes on windows'\n",
            "\n",
            "It will generate a file ./generated.wav as specified by --output-filename.\n",
            "\n",
            "You can download a test model from\n",
            "https://huggingface.co/csukuangfj/vits-ljs\n",
            "\n",
            "For instance, you can use:\n",
            "wget https://huggingface.co/csukuangfj/vits-ljs/resolve/main/vits-ljs.onnx\n",
            "wget https://huggingface.co/csukuangfj/vits-ljs/resolve/main/lexicon.txt\n",
            "wget https://huggingface.co/csukuangfj/vits-ljs/resolve/main/tokens.txt\n",
            "\n",
            "./bin/sherpa-onnx-offline-tts \\\n",
            "  --vits-model=./vits-ljs.onnx \\\n",
            "  --vits-lexicon=./lexicon.txt \\\n",
            "  --vits-tokens=./tokens.txt \\\n",
            "  --sid=0 \\\n",
            "  --output-filename=./generated.wav \\\n",
            "  'liliana, the most beautiful and lovely assistant of our team!'\n",
            "\n",
            "Please see\n",
            "https://k2-fsa.github.io/sherpa/onnx/tts/index.html\n",
            "or details.\n",
            "\n",
            "Options:\n",
            "  --vits-lexicon              : Path to lexicon.txt for VITS models (string, default = \"\")\n",
            "  --num-threads               : Number of threads to run the neural network (int, default = 1)\n",
            "  --output-filename           : Path to save the generated audio (string, default = \"./generated.wav\")\n",
            "  --provider                  : Specify a provider to use: cpu, cuda, coreml (string, default = \"cpu\")\n",
            "  --sid                       : Speaker ID. Used only for multi-speaker models, e.g., models trained using the VCTK dataset. Not used for single-speaker models, e.g., models trained using the LJSpeech dataset (int, default = 0)\n",
            "  --vits-model                : Path to VITS model (string, default = \"\")\n",
            "  --vits-tokens               : Path to tokens.txt for VITS models (string, default = \"\")\n",
            "  --vits-noise-scale          : noise_scale for VITS models (float, default = 0.667)\n",
            "  --debug                     : true to print model information while loading it. (bool, default = false)\n",
            "  --vits-noise-scale-w        : noise_scale_w for VITS models (float, default = 0.8)\n",
            "  --vits-length-scale         : Speech speed. Larger->Slower; Smaller->faster. (float, default = 1)\n",
            "\n",
            "Standard options:\n",
            "  --config                    : Configuration file to read (this option may be repeated) (string, default = \"\")\n",
            "  --print-args                : Print the command line arguments (to stderr) (bool, default = true)\n",
            "  --help                      : Print out usage message (bool, default = false)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sTg7xhZDBpDL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
